[
    {
        "title": "联想集团董事长兼CEO杨元庆：出海是联想做过的最正确的战略抉择之一 | 最前线",
        "link": "https://36kr.com/p/3016125754238469?f=rss",
        "description": "<p>“回顾过往，我觉得出海是联想做过的最正确的战略抉择之一。”</p>\n  <p>10月30日，北京，在联想集团主办的“领航者征途：2024中国企业高质量出海论坛”上，联想集团董事长兼CEO杨元庆谈到，“如果没有改革开放，如果没有全球化，就没有今天的联想。”他说。</p>\n  <p>2004年12月，一场被喻为“蛇吞象”的并购案中，联想以三十亿美元收购了一百亿美元规模的IBM PC业务，这一举动令全球瞩目，至今仍被广泛提及。</p>\n  <p>杨元庆回顾了联想全球化的起点，“2000年，我带领团队前往美国，拜访了包括微软、英特尔、惠普、思科在内的很多家高科技企业。在加州圆石滩，我们下定决心，要在接下来的十年内，将联想打造成为一家名副其实的全球化企业。为了实现这个理想，联想尝试过不同路径，比如自建式发展，几经探索，最终选择在2004年通过并购IBM PC业务的方式正式扬帆出海”。</p>\n  <p>但要想实现真正的全球化，并购只是起点，整合才是关键。</p>\n  <p>在杨元庆看来，整合不仅是财务报表的合并，更涉及从产品、品牌到供应链、组织文化等各方面的系统性整合，复杂度极高。</p>\n  <p>以产品与品牌为例，为了在并购后避免员工流失和客户流失风险，联想选择在过渡期内联想和IBM的产品、品牌各自独立运行的模式，业务稳定之后，才开始深入整合，并且根据不同品类的不同特点，采取不同程度的整合路径。</p>\n  <p>组织文化整合就更难了。老联想的流程和经营偏重垂直管理，IBM则偏重矩阵管理；老联想有自己长期形成的中文工作语言和工作术语体系，而IBM则是完全不同的英文体系，光特定缩写和术语就有十几页；老联想基于单一国家业务的管理方式是事业部直接触达区域市场的具体管理，而IBM在全球市场上则是通过各国家/地区的业务团队来间接管理；文化上，老联想引以为豪的是“主人翁精神”和“说到做到”，他们则是经理人文化等。</p>\n  <p>经历了从磨合到融合的方方面面，最终让联想有更强的竞争力，也沉淀下更多的经验与方法论。演讲中，杨元庆首度对联想全球化20年的实战方法论进行了提炼，总结了全球供应链、全球研发体系及全球市场营销体系三大关键支柱，以及数字化与ESG两大基座，这也是每一家出海的中国企业需要锻造的关键能力。</p>\n  <p>“中国企业在全球市场上发展壮大的基础，一定是扎根中国。回看过去这20年，联想不仅是对外的拓展，更是一种对内的赋能”，联想集团董事长兼CEO杨元庆表示，联想约80%的生产制造、70%的研发人员、60%的员工都布局在中国大本营。通过深耕海外市场，联想能够更有效地整合全球资源，实现对国内市场的有力回馈，进而为国家经济的繁荣发展注入新的活力。这样的出海之路，正是中国扩大高水平对外开放的应有之义。</p>\n  <p>然而，中国企业“出海”的外在环境也在不断变化。此时此刻的世界，正处于一个全球贸易格局、产业链价值链重构与技术变革浪潮交汇的关键时刻。</p>\n  <p>联想集团董事长兼CEO杨元庆表示，作为中国企业，“走出去”是开拓海外市场、提升企业争力的有效路径。但光“走出去”是不够的，中国企业还需要加速“走进去”和“走上去”。</p>\n  <p>如何达成这样的目标？杨元庆进一步提出了三点建议：一是重视通过本地化产品创新构建全球知名品牌；二是重视通过贴近本地市场构建韧性供应链；三是重视通过合作共赢塑造当地市场企业公民形象。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_c9b5805a1bc048539434d257cd9cc723@405798_oswg565465oswg1080oswg720_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">联想集团董事长兼CEO杨元庆</p>\n  <h3>以下是联想集团董事长兼CEO杨元庆的演讲全文，经编辑后发布：</h3>\n  <p>杨元庆： 尊敬的各位来宾、朋友们，大家好！很高兴参加“领航者征途”中国企业高质量出海论坛。距离这里不远的地方就是上世纪80年代有名的“中关村电子一条街”，那里不仅见证着中国科技创新的蝶变，也是联想“计算梦”与“出海梦”的起点。40年前，我们从一间传达室创业出发，一步步从代理别人的品牌到发展自主品牌，从国际化到多元化，从摘取全球个人电脑市场冠军，再到今天引领新一代人工智能发展浪潮，我们用20年的时间成长为中国全球化程度最高的企业之一。</p>\n  <p>如果说20年前联想国际化是一次冒险的下水试航，如今，越来越多的中国企业正在以更加自信的姿态走出国门，勇敢地去探索更广阔的世界，出海品类日益丰富，从服装、家具、家电的“老三样”，到口袋至云端的信息产品，从新能源汽车、锂电池、光伏产品的“新三样”，再到新餐饮出海、游戏出海、文化出海，为全球经济注入了一股来自中国的全新活力。</p>\n  <p>刚才白院长从宏观趋势层面分享了他的真知灼见，下面我想基于企业实践，谈谈出海及全球化如何塑造了今天的联想，并对中国企业在新一轮出海浪潮中如何加速“走上去”，谈谈我的思考和建议。</p>\n  <p>作为在中国IT产业摸爬滚打了整整40年的企业，联想是借助改革开放的东风成长起来的最早一批民营企业之一，也是第一批树立国际化愿景并成功实现的中国企业。</p>\n  <p>时间倒回到新世纪初年的2000年，当时我带领我们的团队前往美国，拜访了包括微软、英特尔、惠普、思科在内的很多家高科技企业。在加州Pebble Beach圆石滩，我们下定决心，要在接下来的十年内，将联想打造成为一家名副其实的全球化企业。为了实现这个理想，我们尝试过不同路径，比如自建式发展，但很快就发现，作为面向消费者的出海品牌先行者，不但不能像今天得到中国国家品牌这样的强力背书，还要背负长期以来人家给我们打上的“价廉质次”这样的先入为主的标签，所以别说打动客户，连招个员工都难，没有长时间的资源投入和经验积累是不可能做成的。后面的故事，大家都知道，2004年，我们选择了通过并购IBM个人电脑业务的方式正式扬帆出海。</p>\n  <p>三十亿美元的公司并购一百亿美元的业务，这桩收购被喻为“蛇吞象”，大家在为我们叫好的同时，也都为我们捏了把汗。当时没有中国企业成功并购海外品牌的先例，我们只能怀着不成功便成仁的心态“摸着石头过河”。</p>\n  <p>并购成功之所以难，最大的难度还不在于谈判的拉扯难，而是在于整合的过程难。它不仅仅是简单的财务报表的合并，更是从前端到后端、从业务到职能的系统性的整合，不可能停下高速运行的企业运营，等整合完毕再前行，更不能一刀切，而需要根据不同领域的不同情况做细致的、针对性的策略设计，更要在执行过程中保持灵活，在妥协和坚持之间找到最佳平衡。</p>\n  <p>以产品与品牌为例，为了在并购后避免员工流失和客户流失的风险，我们选择了在过渡期内联想和IBM的产品、品牌各自独立运行的模式，业务稳定之后，才开始深入整合，并且根据不同品类的不同特点，采取不同程度的整合路径。像台式电脑，已经是非常成熟、大宗化的产品，效率和成本的优先级更高，我们就做了快速、彻底的整合，把原来IBM产品从研发到生产制造的整个链条从美国完整地搬到了中国，从而让它从完成彻底整合之初就一直成为我们的现金牛；而对于笔记本电脑，则采用了循序渐进、分工协作的方式，我们珍视ThinkPad这个在商用客户中拥有最高美誉度的品牌，完整地保留了基于美国和日本的研发团队，把它的创新优势和精湛工艺发扬光大，同时充分利用联想多年打造的卓越运营的优势，把这个在IBM时代年亏损两到三亿美元的产品线，扭转到盈利水平行业领先。我们还基于整合后的研发和供应链优势，推出了面向全球的消费品牌YOGA，与Think形成互补，把联想电脑的市场地位不断提升，直到作为一个整体在2013年登上了全球个人电脑市场的冠军宝座。</p>\n  <p>不仅产品和品牌，销售、服务、供应链、生产制造、研发；乃至人力资源、组织、流程、文化等方方面面都有这样的从磨合到融合，再到竞争力更强的过程。对于中国企业来说，开辟海外市场从来就是一场危与机并存的探险，既需要无所畏惧的勇气与魄力，更需要长期主义的韧性与毅力。可以这么说：一直到联想成为全球个人电脑冠军，且能保持整体营收和利润的持续增长，各个大区均衡发展时，我们才敢说，联想完成了最大的一次冒险，实现了“走出去”的阶段性胜利。</p>\n  <p>回顾过往，我觉得出海是联想做过的最正确的战略抉择之一，甚至可以说，如果没有改革开放，如果没有全球化，就没有今天的联想。主动全球化20年不但为我们带来了年营收18倍以上的增长，更让联想稳步打造了植根中国、致胜全球的均衡布局和韧性竞争力。今天我们在全球拥有18个研发基地和30多家制造工厂，在全球180个市场开展业务，75%以上营收来自于海外，是全球市场上最受信赖与赞赏的中国品牌之一。</p>\n  <p>如果说并购整合是联想国际化的路径的话，那么在登上全球化舞台之后要锻造怎样的关键能力，我们也总结出几个要点，希望对有志于出海的中国企业有参考借鉴的意义，那就是——三大关键支柱和两大基座。</p>\n  <p>首先是三大关键支柱，分别是全球供应链、全球研发体系以及全球市场营销体系。这也构成了联想独有的“全球资源、本地交付”运营模式中最核心的内容。</p>\n  <p>一方面，我们充分调用全球优质资源，打造全球供应链、全球市场销售系统、全球创新研发体系，实现全球资源的高效整合；另一方面，我们又充分调动各区域市场的主人翁意识与灵活性，通过本地特色的产品和服务创新，高效灵活的本地交付，无限贴近本地市场。</p>\n  <p>不同国家和市场各自具有日积月累形成的比较优势，中国有产业链集群优势，日本有工程化优势，美国有创新设计优势，印度有软件服务优势，东欧国家有多语种商务支持优势等等，我们充分利用各地的优势资源，博采众长凝结成更具创新性的技术、产品和方案，以及更具竞争力的价值链，所以我们不仅形成了以中国-日本-美国为支点的研发三角，全球统一配置和管理的采购、制造、物流网络，最后一公里的生产交付服务中心更是遍布各地，辐射全球：墨西哥供应整个北美，匈牙利供应欧洲，巴西、阿根廷、日本、印度供应客户偏好独特的本土市场，正在规划建设的沙特生产基地则供应数字化进程不断提速的中东-非洲。</p>\n  <p>其次是两大基座，指的是支撑三大关键支柱的数字化基座与ESG基座。</p>\n  <p>自完成IBM个人电脑业务并购后，我们耗时8年，整合部署了一套贯穿联想“研产供销服”全价值链的数字化基石，不仅支撑着联想全球体系实现规模、效率与协同，更贯穿助力我们实现从硬件产品到解决方案和服务提供商的转型。今天，我们用自身的数字化、智能化实践经验和能力为各行各业的转型赋能，为中国制造、中国品牌走向全球产业链中高端赋能。</p>\n  <p>而ESG企业社会价值则构成了联想的软实力，帮助联想在复杂多变的环境中赢得更多尊重。作为上世纪90年代就在香港上市的企业，我们本身就具有与国际接轨的治理基础。在全球化过程中，联想又组建了真正国际化、多元化的董事会及高管团队，采用高度规范、透明的公司治理和管理架构。不管走到哪里，联想都将合规视为企业的生命线，以诚信正直的方式开展业务，尊重当地市场的法律法规和文化习俗。而可持续发展的理念，又确保联想在全球每一个市场提供创新优质的绿色产品，通过公益实践、尊重支持弱势群体来贡献社会价值，并以海纳百川的胸怀包罗天下人才。</p>\n  <p>可以说，数字化让联想走得快，ESG则让联想行得稳，两大基座提供扎实的支撑，让我们规避了许多水面之下的风险和挑战。</p>\n  <p>云路鹏程九万里，雪窗萤火二十年。20年出海征程的砥砺前行，不仅由内而外重塑了联想，更锻造了联想引以为豪的核心竞争力；而这些核心竞争力又助力联想穿越经济周期，加速迈向更高层次、更高质量的全球化。</p>\n  <p>在这里，我还要特别强调的是，中国企业在全球市场上发展壮大的基础，一定是扎根中国。回看过去这20年，联想不仅是对外的拓展，更是一种对内的赋能。我们约80%的生产制造、70%的研发人员、60%的员工都布局在中国大本营。通过深耕海外市场，我们能够更有效地整合全球资源，实现对国内市场的有力回馈，进而为国家经济的繁荣发展注入新的活力。这样的出海之路，正是中国扩大高水平对外开放的应有之义。</p>\n  <p>当我们为这一波中国企业出海所取得的成绩感到振奋时，我们也要清楚地看到，此时此刻的世界，正处于一个全球贸易格局、产业链价值链重构与技术变革浪潮交汇的关键时刻，内外部环境可以说是充满挑战。</p>\n  <p>作为中国企业，“走出去”是我们开拓海外市场、提升企业竞争力的有效路径。但仅仅走出去是不够的，还需要加速“走进去”和“走上去”。那么，具体要如何做才能达到这样的目标呢？从联想20年全球化经验出发，我有三方面的建议。</p>\n  <p>第一个建议，要重视通过本地化产品创新构建全球知名品牌。如今，全球消费者不再局限于购买传统品牌，而是更愿意尝试、探索新品牌，为更好的产品和服务买单，这意味着中国品牌有望凭借更具竞争力的产品与服务在海外市场迎来更蓬勃的发展空间。而构建品牌力的核心是产品创新，而产品创新则一定要基于对当地消费者需求的挖掘，不仅仅要物美价优，更要质量过硬。只有这样，才能得到全球不同市场消费者的信任和认可。</p>\n  <p>第二个建议，要重视通过贴近本地市场构建韧性供应链。目前全球供应链开始呈现出脆弱多变、复杂交织的新态势。对于中国企业来说，只有从出口导向跨越到深度融合的本地化，串联起更多产业链环节，搭建起更加高效、抗风险性更强的供应链，才能更加贴近本地市场，服务好当地的消费者与客户。这也是联想在日本、印度、阿根廷、巴西等多个“出海困难模式”的市场逆袭成功的关键。例如日本消费者偏爱Made in Japan的产品，我们就通过与NEC和富士通的合资，利用他们在米泽和岛根的工厂生产电脑提供给日本用户，又学习他们精良的工艺标准，以及严苛的质量把控，反哺我们国内制造能力的提升。</p>\n  <p>最后，要重视通过合作共赢塑造当地市场企业公民形象。我们选择出海，不是为了征服某个市场，而是为了融入他们。我们销售的产品与服务，要给当地消费者带来更好的体验，而并非只为了销售业绩；我们雇佣当地的员工，是为了让他们和联想实现共同发展。只有在出海时思考如何为当地创造价值，共享发展红利，成为负责任的企业公民，才能真正融入海外市场。</p>\n  <p>我相信，未来会有更多的中国企业走出国门，通过成功的跨国运营将中国智造、中国品牌的技术和产品带到更广阔的市场，增强国内国际两个市场、两种资源的联动效应，加速构建“双循环”新格局，培育国际经济合作和竞争的新优势，以高水平对外开放促进世界合作共赢。</p>\n  <p>回顾人类文明史，无论是中国古代的“丝绸之路”，还是欧洲近代的大航海时代，只有秉持探索、开放、勤奋的精神才能不断登上新大陆，桥接起自己跟外面的世界。《天工开物》说得好：“梯航万国，能使帝京元气充然”，只有融入世界的中国才是有朝气的中国。</p>\n  <p>正所谓“一花独放不是春,百花齐放春满园”。联想一直是全球化的受惠者与推动者，我们也将持续发挥自身优势，助力中国企业走向世界，推动中国与全球市场的互联互通、价值共享、共同繁荣。不仅让世界的市场，成为中国的机遇；更让中国的发展，成为世界的动力。谢谢大家！</p>",
        "published": "2024-10-31 08:02:08",
        "id": "8f1508ef-8344-4c2c-9310-29e30b33d5ee",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "联想集团董事长杨元庆在论坛上表示出海是联想最正确的战略抉择之一，回顾联想20年全球化历程中的并购整合、关键能力锻造，以及面对当下环境提出中国企业出海要加速‘走进去’和‘走上去’的三点建议"
        },
        "tokens": 4678
    },
    {
        "title": "智能宠物监控革命，灵予科技要做毛孩子的AI保姆｜早期项目",
        "link": "https://36kr.com/p/3015741200835848?f=rss",
        "description": "<p>作者｜叶丹璇</p>\n  <p>编辑｜袁斯来</p>\n  <p>随着AI越来越广泛地应用于人类的日常生活，许多传统硬件行业都正在积极拥抱算法的改造，宠物监控也在此列。</p>\n  <p>对于当代人而言，宠物早已超出豢养动物的范畴，而更多地成为互相陪伴的“毛孩子 ”家人。宠物主也更愿意在宠物的照护和日常监测上投入精力和金钱：上班族在家 里安装监控，在离家的间隙通过摄像头查看宠物的状况，远程与之对话，正在变成一种常见现象。</p>\n  <p>硬氪近期接触的灵予科技，团队通过接入自研的AI视觉算法体系，推出一款可以自主记录宠物精彩瞬间与分析异常行为的宠物摄像头——SiiPet。</p>\n  <p>灵予科技创始人付星昱和万纬韬具备丰富的智能硬件和算法开发经验。基于对宠物家庭和市场的观察，发现目前市面上传统的宠物监控在诸如高速追踪、行为理解等垂直场景中非常重要的特性上，效果都不如人意，依然存在较为显著的技术痛点，需要更深入的AI算法和数据积累才能突破。</p>\n  <p>一个属于上班族宠物主的常见烦恼是，每天真正陪伴宠物的时间只有下班后的几 小时，时常加班或出差的宠物主的陪伴时间则更少。此外的时间里 ，“毛孩子 ”只能在家里独自生活，大量的宠物生活内容无法被发现和记录。更重要的是，即使宠物出现了健康相关的异常行为，宠物主也很难通过传统监控和短暂的相处发现。</p>\n  <p>基于对此问题的思考，灵予科技将自主研发的一系列AI视觉算法嵌入SiiPet宠物监控摄像头中，通过核心端侧算法及云端大模型工具，结合宠物真实行为数据、临床数据与美学评估数据等底层大数据集，实现宠物高光画面自动捕捉、异常行为提醒与多宠识别。</p>\n  <p>在具体的使用过程中，SiiPet通过搭载了AI视觉系统的4k摄像头，能够对高速移动的宠物进行精准识别和超高速追踪。同时，摄像头会对高光画面进行自动捕捉与构图，确保画面尽可能符合人类的拍照审美，区别于传统的监控画面。</p>\n  <p>不仅如此，SiiPet目前已经能够准确进行多宠物识别，并在后台将高光片段按不<span style=\"letter-spacing: 0px;\">同宠物归档推送，宠物主可以实时切换查看不同宠物的状态，同时一键分享每日的高光图片和短视频在喜爱的社交媒体上。</span></p>\n  <p>值得注意的是，SiiPet特有的异常行为捕捉功能，填补了目前宠物摄像头领域对宠物行为和健康监测的空白。</p>\n  <p>呕吐、蹭屁股、抓挠、甩头、跛行或抽搐等行为及其出现频率，都是判断宠物异常健康状况的重要指标。但大量宠物主即使安装了现有的宠物监控，也无法准确监测到上述行为以及对应的的出现频率，大量早期健康问题因此被忽视，宠物也容易错过最佳治疗窗口。</p>\n  <p>目前，SiiPet的异常行为捕捉功能，可以通过其智能化系统自动监测、捕捉，根据内置的兽医动物行为学知识和后台用户数据不断学习、准确判定，并推送给宠物主。</p>\n  <p>付星昱对硬氪表示，由于目前宠物用品市场的硬件供应链已经非常成熟，行业正在进入一个在硬件指标和成本上“ 内卷”的阶段，亟需通过技术上的革新，对一直存在的难题进行攻克和解决。</p>\n  <p>“摄像头的追踪、识别与分析等能力本质上是由算法带来的智能化差异。我坚定地认为，宠物用品从联网与自动化，向真正的智能化发展是一个明确的趋势。 ”付星昱说。他留意到，近期部分宠物公司在寻求算法支持，引入更多AI技术，灵予科技正在与他们建立合作。</p>\n  <p>团队方面，灵予科技的核心技术团队均来自智能硬件大厂，CEO付星昱和系清华大学电子工程系硕士，曾担任tp-link与小米高级产品经理，负责多个过亿收入的智能家居产品；CTO万纬韬系清华大学电子工程系博士，曾担任微信通用物体检测和小程序AI视觉能力负责人，发表多篇AI顶会论文。</p>\n  <p><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_ad94226257c8469097ba51f1c786e99b@6129731_oswg90987oswg1080oswg336_img_jpg?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\" contenteditable=\"false\">36氪供图</p>\n  <p><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_3654ad1b75564d03bd8bf088b9a31fc2@6129731_oswg83785oswg1080oswg336_img_jpg?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\" contenteditable=\"false\">36氪供图</p>\n  <p class=\"img-desc\"><br /></p>",
        "published": "2024-10-31 01:32:54",
        "id": "457100f6-1853-4a69-aee7-1135dade5e5c",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "灵予科技由具备丰富智能硬件和算法开发经验的团队创立，其推出的SiiPet宠物摄像头接入自研AI视觉算法体系，具备多种功能，能满足宠物主对宠物监控的需求，团队正在与其他宠物公司建立合作。"
        },
        "tokens": 1919
    },
    {
        "title": "「临科智华」完成2300万元种子轮融资，明年预计营收1.5亿元 | 36氪首发",
        "link": "https://36kr.com/p/3013441972348416?f=rss",
        "description": "<p>文 | 田哲</p>\n  <p>编辑 | 苏建勋</p>\n  <p>36氪获悉，临科智华近日宣布完成2300万元种子轮融资，投资方为谦益资本。本轮融资资金，临科智华将用于团队建设、技术研发，以及人工智能核心技术基础建设。</p>\n  <p>临科智华成立于2024年，以人工智能最重要的部分“高质量数据”为核心，致力于为各行业公司提供数据智能解决方案。据悉，临科智华核心团队拥有十年以上人工智能数据服务经验，其CEO王旭辉为美国Scale AI核心科学家、美国计算机学会会员、国际人工智能学会会员。</p>\n  <p>随着AI和大数据技术的逐渐成熟，越来越多的传统行业开启智能化转型。然而，企业在智能化转型过程中，可能会出现异构数据统一难、研发成本高、安全风险高等难题。此外，构建智能化系统的过程中，需要大量算力、存储、云服务等基础设施，这对企业来讲成本较高。智能化过程需要专业的技术人才、运维和产品人员，这是传统企业所不具备的。</p>\n  <p>临科智华的解决方案正是解决这一痛点，其采用数据处理引擎和硬件系统的结合模式，为企业客户提供数据智能解决方案。这将是一个潜力巨大的市场，数据公司IDC指出，2024-2028年，中国企业开发和运营数字化业务相关的ICT支出总额将达到2.99万亿美元。</p>\n  <p>临科智华CEO王旭辉告诉36氪，针对日益庞大的市场需求，临科智华推出了训推一体机「曦华智驱AW2000」，其搭载多款自研人工智能大模型和垂直领域系统， 可以适配各行业的定制化需求，快速适配各行业需求。</p>\n  <p>据悉，AW2000采用气液两相散热技术，在保证AI算力的情况下，可降低对IDC机房的要求，可以部署在企业内部，保证数据私密与安全性。自研模型通过智能调度算力，从而大幅降低算力消耗，提升GPU利用率。&nbsp;</p>\n  <p>此外，临科智华的模型训练架构支持多模态架构，使用各种训练、推理加速技术，高效支持多模态需求。并能够根据企业业务需求进行定制，满足不同场景下的智能化需求。&nbsp;</p>\n  <p>系统方面，临科有四大平台：异构算力平台、大模型基础平台、低代码智能Agent平台，临科多模态数据标注处理平台。临科智华通过这四大平台为各行业不同客户需求提供智能系统全链路解决方案</p>\n  <p>目前，临科智华的服务客户涵盖自动驾驶、智能客服、智能制造、教育、智能医疗等场景。</p>\n  <p>据悉，临科智华正在构建千卡超算中心，以满足大规模AI计算需求。该超算中心将集成业界尖端的计算资源，拥有处理海量数据集与复杂计算任务的能力，以支持地方科研机构及企业用户。</p>\n  <p>王旭辉表示，临科智华计划在未来两年内拓展其在制造、金融、教育等领域的市场份额，进一步赋能各行各业，让AI真正在各行各业落地，发挥更大的作用。预计明年，临科智华将实现营收1.5亿元。</p>",
        "published": "2024-10-31 01:00:00",
        "id": "3802d1dd-087b-470c-a977-1009f6e6e330",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "临科智华2024年成立，以人工智能数据为核心提供数据智能解决方案，完成2300万元种子轮融资，其推出训推一体机，有四大平台，服务多场景客户，正在构建超算中心，预计明年营收1.5亿元。"
        },
        "tokens": 1393
    },
    {
        "title": "科氪 |荣耀Magic7系列发布：开创AI智能体新纪元，重塑智能手机未来",
        "link": "https://36kr.com/p/3016182774048001?f=rss",
        "description": "<p>2024年10月30日，荣耀于深圳正式发布了年度AI旗舰手机——荣耀Magic7系列，这不仅标志着智能手机行业正式迈入AI智能体时代，更是一次对智能手机未来发展方向的深刻探索与重塑。凭借独树一帜的创新理念与卓越技术实力，以及领先行业的AI能力，荣耀Magic7系列重新定义了智能手机的想象力边界。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_731ae43b2ef74b1cb1a0a383e6d3f740@517825446_oswg1017596oswg1280oswg853_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀终端有限公司CEO赵明表示：“AI毫无疑问是现在最引人注目的科技魔法，但荣耀很早就看到了AI让手机进化的魔力，我们坚信AI是智能手机的未来。从拍照、屏幕、续航到通信，我们用AI魔法使能和重构⼀切，包括操作系统。荣耀Magic7系列将会是引领未来的AI手机。”</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_0f5598f35ddc4104804ec9e56abcb6cf@517825446_oswg362996oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>作为集荣耀AI技术创新大成之作的旗舰手机，荣耀Magic7系列在系统、影像、屏幕、通信、续航、性能等多个维度，均实现了AI全面赋能的革新。尤其在搭载荣耀自主研发的YOYO智能体之后，可实现自动执行、一语到位的高阶智慧能力，为用户带来了颠覆性体验。</p>\n  <p>据了解，荣耀Magic7系列首发搭载的MagicOS 9.0的AI大模型能力，获得中国信通院权威行业认证，获颁泰尔测评证书卓越级；在中国信通院颁发的终端智能化分级能力证书中，首发搭载MagicOS 9.0的荣耀Magic7系列通过《终端智能化分级测试方案》评估，终端智能化水平达到行业目前最高等级L3。并且荣耀也是业内唯一一家达到该智能化水平的终端厂商。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1d84e57d590e48449eb357de27a04f15@517825446_oswg560886oswg1269oswg423_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>在智能手机市场面临创新瓶颈的当下，荣耀Magic7系列的发布，不仅是对智能手机未来的深刻探索，更为行业注入了新的活力与灵感。以荣耀为代表的中国企业，终于站在了AI浪潮的前列，率先进入了智能手机的”自动驾驶”时代。</p>\n  <p><strong>用真AI开启智能手机的“自动驾驶” &nbsp;</strong></p>\n  <p>在AI手机元年拉开序幕的2024年，几乎所有的旗舰新机和操作系统都纷纷打出了“AI化”的旗号，市场上充斥着各种声称具备AI功能的手机和操作系统，各种新功能也都纷纷冠以“AI”之名。然而，荣耀Magic7系列却凭借其深入底层的AI技术创新，真正树立了AI手机的里程碑之作，开启了AI手机“自动驾驶”的新时代。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_22a6d95ea1dd46d8afa58c040651554f@517825446_oswg314348oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>目前，多数手机厂商仍然以嫁接式的生成式AI服务作为其可以被称作“AI手机”的理由，但此类AI能力更多是通过开放接口，将大模型技术集成到手机的具体应用中，如照片编辑、语言翻译、记笔记、发短信、搜索等。然而，真正的AI手机所追求的，绝非仅限于这种“应用层AI”的浅尝辄止，而是要用AI从操作系统的底层开始，彻底重构服务逻辑、业务流程和资源分配，超越现有以应用程序为基础的操作系统框架。</p>\n  <p>在荣耀看来，真正的AI手机需要用AI技术全面重塑底层的硬件和操作系统，从用户体验到业务逻辑的每一个细节，都需经历深刻的变革。荣耀Magic7系列搭载业界首个实现商业化落地的AI智能操作系统MagicOS 9.0正是平台级AI能力的真正体现。</p>\n  <p>荣耀Magic7系列通过全新的YOYO智能体，实现了纯AI视觉、无需生态适配的任务自主执行新突破。无论是单一指令的系统级任务，还是第三方应用任务，甚至是多应用的协同执行，YOYO智能体都能游刃有余地处理。从“一句话点咖啡”到“一句话关闭应用权限”，YOYO智能体以精准的理解和自动执行，为用户带来了前所未有的便捷体验。这一创新不仅改变了“人理解手机”和“人找服务”的传统模式，更让手机拥有了自主行动力，率先引领智能手机从“手动驾驶”迈入“自动驾驶”的新时代。</p>\n  <p>此外，YOYO智能体还能根据当前屏幕内容，主动提供智慧服务，如英文翻译、文章摘要、日程创建等，实现多轮、多意图、全屏意图的主动理解与响应。这种智能化的服务方式，不仅提升了人机交互的便捷性和效率，更让用户感受到了仿佛“自动驾驶”般的智能体验。</p>\n  <p>除了前沿的AI体验外，荣耀Magic7系列在数据安全与隐私保护方面也展现出了卓越的能力。通过创新的端侧AI换脸检测技术和系统默认守护模式，荣耀Magic7系列能够实时分析视频通话，准确识别并预警AI换脸风险，且所有检测均在本地完成，确保用户信息安全无虞。</p>\n  <p><strong>里程碑式的AI旗舰，全面赋能硬件</strong></p>\n  <p>AI作为核心基石，深度融入手机的每一个层面，方能称之为真正的AI手机。在这一新兴领域中，AI手机需通过智能技术无缝链接软硬件，确保每台设备都能根据用户的独特需求，转化为专属的、高度个性化的超级智能伙伴。依托其强大的平台级AI能力，荣耀Magic7系列手机不仅在操作系统层面实现了质的飞跃，更在摄影、显示、通信、电池续航、性能优化及音频体验等多个维度，带来了颠覆性的提升与智能化变革，重新界定了智能手机的极限。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_a5a9cb903123428c972ad3dc156d75b3@517825446_oswg721141oswg2286oswg810_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>在影像技术领域，荣耀Magic7系列搭载了开创性的荣耀AI驭光引擎，将拍摄与后期处理巧妙融合。同时在AI技术的助力下，荣耀鹰眼相机与舞台模式实现了更为精准、高效的拍摄表现。这种前所未有的软硬协同能力，不仅突破了当前移动影像技术的瓶颈，更为整个行业开辟了一片由AI引领的移动影像新天地。</p>\n  <p>在屏幕护眼方面，荣耀Magic7系列开创性地搭载了业界唯一的全局全天候荣耀AI自然光绿洲护眼屏，该技术通过平台级AI算法，精妙地模拟自然光，从波动、亮度、节律、色彩、频闪和光谱六大维度进行全面优化，集成了圆偏振光护眼、4320Hz超高频PWM调光、类自然光护眼、自然色彩显示、硬件级低蓝光、AI离焦护眼技术、AI干眼友好技术、AI助眠显示功能的八大护眼技术，这标志着手机屏幕护眼技术迈入了一个崭新的时代。</p>\n  <p>在通信技术领域，荣耀Magic7系列的荣耀优速通功能凭借AI智能识别网络拥堵场景，实现专线加速，显著提升网络流畅度。荣耀Magic7 Pro首次搭载荣耀通信芯片 HONOR C2，全面覆盖36种用户通信场景，实现更佳信号表现，行业首发双Wi-Fi芯片，可实现双Wi-Fi聚合下载，通过调用主Wi-Fi芯片2.4GHz和5GHz以及HONOR C2备份Wi-Fi芯片的5GHz频段，在咖啡、酒店、商场、校园网、公司等限速场景下， 应用市场、王者荣耀更新下载等可以带来更快的下载体验，缩短用户等待时间。同时，荣耀鸿燕通信也借助AI技术对卫星通信功能进行了优化，确保实机使用的稳定性和易用性。</p>\n  <p>在电池续航方面，荣耀Magic7系列搭载的全新升级的第三代青海湖电池技术和自研能效增强芯片HONOR E2，重塑续航边界，让每一份电量都充满智慧与力量。通过尖端硬件与智能AI算法的深度融合，进行全方位、多层次的能效优化，实现了荣耀Magic7系列的超长续航能力。值得一提的是荣耀Magic7 Pro推出了行业首创的第三代三极耳技术，这一创新设计提供了最佳的10%硅碳负极+100W有线+80W无线快充综合解决方案，实现了高能量密度和快充能力的极致均衡设计。在性能方面，骁龙®️8至尊版移动平台与荣耀AI技术的深度融合，充分激发了端侧AI的潜能，为产品创作和生产力提升注入了澎湃动力。同时，散热系统也在AI技术的优化下，达到了业界领先的散热效果。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_46d887825c224360bc7de59702669be1@517825446_oswg509000oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀Magic7系列的问世，标志着荣耀正带领行业向更智能、更人性化的未来挺进，触发了一场由荣耀主导的“荣耀效应”变革，凭借AI与硬科技的双重引擎，荣耀正加速推动整个行业向新高度进发。</p>\n  <p><strong>独立四周年&nbsp;荣耀艰难而正确的路径迎来开花结果</strong></p>\n  <p>在发布会现场，一张意味深长的图文：“用自己的名字，去自己的远方”，被外界视为荣耀独立四周年的真实写照。自从独立以来，荣耀选择的是面向未来的“创新引领”路线，站在明天看今天，以此不断牵引带动国内产业价值链条攀升，相比行业盛行的拿来主义、看重短期利益的“创新变现”做法，这注定是一条艰难但是正确的道路。时至今日，荣耀的坚持已经逐渐开花结果。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_64ea939b634247e8bd6cabc591789f99@517825446_oswg1592706oswg2386oswg792_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>第三方机构预测，荣耀即将迎来的是一次井喷式的历史机遇：面对生成式AI手机市场预计的爆发式增长——IDC预测2024年出货量将激增364%至2.342亿部，2028年更将达到9.12亿部的广阔蓝海。荣耀凭借深厚的技术底蕴与不懈的创新追求，蓄势待发，准备继续引领智能手机行业的未来走向。</p>\n  <p>荣耀Magic 7系列堪称是荣耀技术创新的集大成者，不仅在AI能力上实现了断档式领先，也巩固了荣耀在高端旗舰市场的领先地位。这一旗舰系列的问世，背后是荣耀在端侧AI领域多年的深耕细作与不断突破技术界限的坚持。</p>\n  <p>早在2016年，荣耀便以前瞻性的布局，从系统底层开始构建全面的AI技术优势。从第一代荣耀Magic系列智能手机搭载荣耀Magic Live智慧引擎，到2018年荣耀Magic第二代启用自进化、自学习的智慧生命体YOYO，再到2022年底发布AI使能的个人化全场景操作系统MagicOS 7.0，荣耀在AI技术的探索上从未停歇。2023年，荣耀更是首次提出将AI大模型引入端侧，进一步推动了AI技术在智能手机领域的应用。2024年，荣耀首次提出AI的四层架构，为智能终端的发展提供了清晰的路径。如今，随着荣耀Magic7系列的发布，荣耀YOYO智能体再次实现了端侧AI的创新叠加效应和用户体验增值，标志着智能手机行业正式迈入智能体时代。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_243adb2857b7492782fd3872e6d9d47d@517825446_oswg216695oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀AI的成功并非偶然，背后是其在技术研发上的坚定投入。2023年，荣耀整体研发投入占到总营收的11.5%，AI研发费用累计已达100亿，AI专利成果达2100篇，其中AI意图识别相关专利就有600类。同时，荣耀持续加大AI高精尖人才的招聘力度，近两年校招博士浓度高达36.2%，且对顶级AI人才薪酬不设上限，为公司的持续创新提供了坚实的人才保障。</p>\n  <p>&nbsp;</p>\n  <p>并且，荣耀的技术创新并不仅限于AI领域。在通信、续航、屏幕、玻璃、影像以及折叠屏等多个领域，荣耀同样��得了显著的领先优势。从鸿燕通信技术的突破，到青海湖电池技术的革新，从绿洲护眼屏的推出，到巨犀玻璃的应用，再到鹰眼相机的问世，荣耀不断为智能手机市场带来新的亮点与惊喜，为行业提供了极具价值的解决方案，推动了智能手机技术的全面发展与进步。</p>",
        "published": "2024-10-31 09:02:07",
        "id": "e852fdea-8891-4d8c-9de6-b67e6a584e42",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 5
            },
            "keyFacts": "2024年10月30日荣耀于深圳发布年度AI旗舰手机Magic7系列，该系列在多维度实现AI全面赋能革新，其多项技术创新突破，标志着智能手机行业迈入AI智能体时代，也体现出荣耀独立四年坚持创新迎来成果。"
        },
        "tokens": 4445
    },
    {
        "title": "一年服务千万亩次，觉物科技让农业机械化更精细｜早期项目",
        "link": "https://36kr.com/p/3015729080198405?f=rss",
        "description": "<p>作者｜叶丹璇</p>\n  <p>编辑｜袁斯来</p>\n  <p>农业机械化第一次进入中国时，第一台拖拉机把“耕地不用牛”的理念拉进了中国的田间地头。</p>\n  <p>农业机械化和智能化时代的到来，使得越来越多的农机新设备开始进入农业生产的环节。伴随着这一农业新浪潮的，是另一个尖锐的事实：农业生产人口正在下降。根据第三次全国农业普查信息，当前农业劳动力基本是1990年之前出生，且以1960年之前出生的老人为主。经学者测算，2030年，我国的农业劳动力将会减少20%甚至更多。</p>\n  <p>与农业老龄化问题相伴相生的是，目前我国的农业生产过程中专业人才的缺乏。传统农业生产中，耕、种、管、收四个环节，“管”是持续时间最长，也是农机设备介入程度较低的一个环节。田间管理环节涉及的环节多，操作复杂，作物的收成和地块的可持续发展都和农民在这一环节的专业操作分不开。</p>\n  <p>在田间管理过程中，农业机械的精细化作业能力至关重要。传统农机设备在参与田间管理时，需要人工在机耕道上进行换行、转弯等操作，容易碾轧植被，造成对植株的机械损伤。同时，传统农机设备在普洒农药的过程中，由于缺乏精准作业的能力，肥料滥用和农资浪费问题突出，土壤板结化严重。</p>\n  <p>能够精准作业的农业机器人，成为留在耕地上的“新农人”最迫切的需求。</p>\n  <p>硬氪注意到，觉物科技推出的“鹤出”农业机器人（型号Function Robot，FR）正在新疆百万亩的辽阔棉田中作业，并在抖音、视频号等平台获得了许多在地农户的好评。</p>\n  <p>觉物科技2020年成立于广东深圳，是全球最早通过mRaaS(modularized Robot as a Service)模式直接提供农业种植服务的机器人公司之一。其核心产品“鹤出”农业机器人，采取觉物科技全球首创的模块化变形机器人系统，结合人工智能、农田自动驾驶、大数据技术，为全球农业客户提供零碳纯电、数字化、无人化的“耕种管收”全流程农业服务，目前已在新疆农田实现规模化应用。</p>\n  <p>与市面上常见的农机企业整机售卖模式不同，觉物科技选择以社会化服务作业切入市场，致力于解决农业作业季集中爆发的需求高峰。农户只要在农服小程序上自助下单，即可预约专业植保服务。</p>\n  <p>觉物团队在新疆调研的时候发现，农业的时效性非常强，但目前农户采购单一农机只能在单一种植环节专用，导致农机设备利用率很低。“比如除草、采棉这些关键环节，留给农民的时间窗口非常短，你可能要在7-10天内把草全部除掉，采棉机每年只工作40天。”觉物科技创始人宋佳音对硬氪表示，觉物提供的社会化服务作业能够解决这一行业痛点，由更专业、更精细的系统和机器人来为农户提供农业服务，更高效地作用于农业生产。</p>\n  <p>除了农业服务模式优势之外，觉物科技的“鹤出”机器人在实际的农业操作中，也解决了现有植保方式作业质量不佳的问题，并通过在作物生长期提高病虫害防治质量，达到增产10%-15%的效果。</p>\n  <p>目前，市面上的植保机器人尺寸大小基本固定，但在实际的农业生产中，植株间距、高度无序，现有植保方式中，地面式农业机械存在人工驾驶难度大、机械损伤严重等问题；新型的空中设备存在水量不足、药害漂散严重等问题。值得注意的是，现有的所有植保设备都无法实现精准打药。</p>\n  <p>在研发过程中，觉物团队按照深度模块化设计的原则，优化不同功能不同层级的组件，灵活组合出自动变形的多种机器人模态，实现了像乐高一样快速拼装的机器人系统。如“鹤出”变形机器⼈(Code Name: Function Robot)可以在大田场景并通过不同的模态变形自适应不同的作物和种植模式。机器人采用自动驾驶，全向灵活的轮边驱动独立线控技术和自适应变形技术，可以实现360°运动横移倒退换行，几乎实现零机械损伤。</p>\n  <p>同时“鹤出”机器人采取模块化风送系统，在不同的植株发育阶段实现靶向变量施药，精准打透需要施药的部位。在“鹤出”机器人精准作业模式下，施药量低至0.015克/亩，相比传统模式节约90%以上的药量，减少滥用农药的土地破坏，也为农民节省了农资投入。</p>\n  <p>值得注意的是，新疆棉花在采收前，需要对植株喷洒脱叶剂。在以往的农业生产中，利用其它设备喷洒脱叶剂时，由于棉株处于生长最茂盛、最高大的时期，药剂往往难以覆盖中低部的叶片，导致棉花青叶残留在植株上，成为成品棉的杂质。同时，传统普洒设备穿透力不足、脱叶效果不佳，往往需要对棉花植株进行重复施药，造成棉花纤维受损，影响成品棉的品质等级。</p>\n  <p>宋佳音透露，“鹤出”机器人在新疆合作基地的应用，精准解决了二十年来当地棉花脱叶剂滥用的问题，极大地提高了当地棉花采收品质。不仅在新疆农户中有口皆碑，复购率接近100%，还吸引了巴西等地的外国客户。</p>\n  <p>“目前全球的农业都在面临老龄化的问题，农机产品的用户也正在变成新一代的农民、农场主。”想要抓住这一代新农人，精准作业和高效专业的社会化服务将是两个最重要的核心竞争力。“我们认为这是农业机器人的好时代，也是觉物科技的机会。”宋佳音告诉硬氪。</p>\n  <p>日前，觉物科技（深圳）有限公司宣布已完成Pre-A轮融资，本轮融资由浙江机器人产业集团领投，同时获得深圳市投控东海投资有限公司跟投。本轮融资将主要用于推动觉物科技模块化机器人系统的研发进程，加速产品迭代，并进一步推动mRaaS（机器人即服务）模式在农业社会化服务领域的推广。</p>\n  <p><span style=\"color: #999999; font-size: 12px; letter-spacing: 0px; text-align: center;\"></span></p>\n  <p><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_b80b281dc9de436e9badfdbc4e562210@6129731_oswg90987oswg1080oswg336_img_jpg?x-oss-process=image/quality,q_100/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\" contenteditable=\"false\">36氪供图</p>\n  <p><span style=\"color: #999999; font-size: 12px; letter-spacing: 0px; text-align: center;\">&nbsp;</span><br /></p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_36b47fb2df4a46c781acf09994b27304@6129731_img_000?x-oss-process=image/format,jpg/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">36氪供图</p>",
        "published": "2024-10-31 01:30:00",
        "id": "cea64fe8-e085-457d-a781-d65f3e3e5d0a",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "觉物科技推出的'鹤出'农业机器人采用多种先进技术提供农业服务，以社会化服务作业切入市场解决农机设备利用率低等痛点，已在新疆农田规模化应用并获好评，该公司完成Pre - A轮融资用于研发和推广。"
        },
        "tokens": 2423
    },
    {
        "title": "「私董会」的奇幻漂流：蹲在视频号门口的老板们｜深氪",
        "link": "https://36kr.com/p/3015867578426633?f=rss",
        "description": "<p><strong>文｜</strong>王毓婵</p>\n  <p><strong>编辑｜</strong>杨轩 乔芊</p>\n  <p><strong>来源｜</strong>36氪未来消费（ID：lslb168)</p>\n  <p><strong>封面来源｜</strong>作者拍摄</p>\n  <p>周末早上九点，2000多个小企业老板已经占满了一间会议厅的所有椅子，很多来迟的人不得不坐在地上。他们来学习怎么在视频号上做生意。</p>\n  <p>每张椅子上都放着已经打印好的演讲稿和一支笔。这些平均年入百万以上的老板们，像小学生一样把纸放在膝盖上，又圈又写。</p>\n  <p>去年，同样的活动只来了700人，今年翻了一倍还多。</p>\n  <p>“今年谁还搞抖音大会？全TM搞视频号了。为啥？因为做抖音没钱赚了，很简单。”刘思毅站在台上说。</p>\n  <p>刘思毅是流量操盘手社群“群响”的主理人，一年花费2399元入会，你就可以像这两千人一样，一起学着站上风口。前几年，群响的大会主题还是抖音电商，今年则变成了视频号、小红书和个人IP。</p>\n  <p>不过，在两千人的大会场，当视频号讲师展示三张女主播照片，问台下谁是“视频号一姐”时，出声作答的人寥寥无几。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_22fc8159436446c488fc08ae74456f54@10269314_oswg748222oswg960oswg720_img_000?x-oss-process=image/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">36氪摄影</p>\n  <p>这群过去主做抖音的操盘手们对视频号是陌生的。“今天在场的两千个操盘手，还有百分之七八十的人根本没有入局视频号，但是愿意用两天来学。”刘思毅说。私董会将带领大家逐帧学习头部主播，随后，他把“抄”字大大地打在会场PPT上。</p>\n  <p>大会的一个重头环节，是刘思毅在台上带着两千人逐字逐句分析张小龙2021年的演讲。因为张小龙已经久不在公开场合露面发言，所以这份演讲稿几乎成为了《红楼梦》甲戌抄本一般的传世经典，被“龙学家”们翻过来倒过去地解读。</p>\n  <p>“我叫张小龙爷爷，因为你们是我爸爸，而张小龙也是你们的爸爸，所以说张小龙是我爷爷。”刘思毅以他高于一般人声量的嗓门举着麦克风大声说：“每一个私域操盘手，请都叫张小龙爷爷！因为张小龙爷爷的价值观和一举一动，会决定各位能不能赚钱！”</p>\n  <p>台下没有人笑。</p>\n  <p>私董会、企业家社群、商业俱乐部等等一切与之大同小异的组织，其实都是观察商业社会的显微镜。今天讲师展示一个爆品，明天许多直播间里就会出现同款；今天讲师展示一种投流技巧，明天电商老板们就能快速共享一波红利。</p>\n  <p>流量在哪里，这群老板就在哪里。</p>\n  <p>但字节跳动和腾讯是两家截然不同的公司。狠狠花钱，再拼命挣钱，是字节跳动从上到下的共识，这催生了抖音生态内巨大、快速、确定性的赚钱机会。但在腾讯内部，用户思维和商业化欲望共同左右着视频号的走向，甚至前者更占上风，这让赚钱的机会没那么确定。</p>\n  <p>而这种“不确定”，恰恰成了私董会的机会。</p>\n  <h3><strong>视频号的不足，私董会的商机</strong></h3>\n  <p>“我这个门，没有几万块钱进不来。”李柔指着自己的新办公室门说。然后她一指周围人说：“他们都是交了钱的人。”李柔的保时捷就停在楼下。</p>\n  <p>李柔是一位身处中国西南部某城市的私董会老板，为想做视频号的老板们传授经验、答疑解惑。</p>\n  <p>她也做直播电商，在抖音上赚了几年“轻松钱”，办公室也从之前又暗又旧又闷的老楼，摇身一变为如今的窗明几净。只是她的员工们看起来却不像白领，闹闹哄哄，总有人跑来跑去，搬设备、念口播、录视频，氛围热火朝天。</p>\n  <p>几个付了几万才进来的学员，苦于无法在这种气氛中捉住李柔说几句话，只得坐在一旁从白天等到黑夜。</p>\n  <p>李柔的会员多是在电商圈里打滚的老江湖，在微信视频号上，他们最关心，也最弄不清的，是如何解决限流、封号、投流效果差等等问题。简单来说：就是“什么不能做”。</p>\n  <p>伴随着微信视频号2023年交易额的3倍增长，它的管理也越加严格。从今年4月开始，“微信视频创作安全中心”每月发布违规处理公告，封禁视频号的数量从每月数千量级，很快增长到过万。</p>\n  <p>李柔的商家群里哀鸿遍野，她自己也未能幸免。</p>\n  <p>“我的公司昨天一天被封了好些账号，一整层楼没有一个账号是正常的。”李柔说，“经常是一个账号违规，导致一条网线甚至一个基站下所有的账号‘连坐’。</p>\n  <p>找到视频号的人不容易。之前，花了广告费的商家还能绕道去找腾讯广告，让他们帮忙答疑。不过，在7月腾讯广告优化了从事带货运营和商业治理工作的团队后，这项工作彻底被移交给微信事业群。很快，就有不少商家向36氪反映，海量问题堆积给视频号的官方人员后，对方无暇人工处理。</p>\n  <p>“我们账号莫名其妙出了问题，在群里找小二，只有机器人回复。”一位商家说。“要想走人工通道，还要排队。最久的一次我们等了7天。”</p>\n  <p>“腾讯的人是很死板的，”李柔用略带夸张的语气说。“你只能问他我这个账号什么情况，对方就只会回答你两个字，‘实锤’或‘误判’，你问他哪个地方实锤？他不告诉你，没时间理你。”</p>\n  <p>这个时候，商家们就需要私董会出现，扮演一个导师，和一个“替自己说话”的话事人。</p>\n  <p>不少私董会都会重点介绍，自己有“腾讯系离职”的讲师，或者是私董会主理人本人有腾讯内部的人脉。</p>\n  <p>“你们不能再刻意制造矛盾了，吵架的内容是不会给流量的。”一位腾讯广告出身的私董会讲师说。商家恍然大悟，说“完了，我跟他（指着身边的合作伙伴）最近正在直播间扮演一对小夫妻，通过吵架来带货。”</p>\n  <p>这一点拨，就节省了商家的时间和金钱。相比之下，商家为获得点拨而付出的几万块学费就不算什么了。</p>\n  <p>如李柔所说，一个平台的电商业务从小到大，通常都有一个规矩从松到严的过程。先把闸门打开，让第一波商家成长起来，把用户的兴趣标签打起来，随着广告越来越精准，用户也养成了消费习惯，平台再强调合规，给商家立规矩。</p>\n  <p>但是视频号不是这样。从视频号开始电商业务的第一天起，就呈现“一夫当关，万夫莫开”的局面。视频号官方一直保持着很小的团队，主持着很严格的规则。</p>\n  <p>所以才有这个差别: 在抖音小红书主题的私董会大课上，大家更关心“怎么做”、“如何发财”，但在视频号主题的课上，大家最关心“不能做什么”。</p>\n  <p>抖音电商业务在发力伊始的2020年，定下“3年内在上海增员2万人”的目标，并花数十亿一举在上海杨浦区买下19.5万平方米的办公项目，用来安置庞大的抖音电商团队。</p>\n  <p>从2020年的“视频号电商元年”到“微信小店”终于上线，大家等了整整4年。四年后，视频号仍然只用一支数百人的小团队，做抖音用一整栋楼在做的事情。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_b8eb862ca25e4f6f97c079acf18c1269@10269314_oswg168228oswg1080oswg844_img_000?x-oss-process=image/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">字节跳动电商业务所在大楼</p>\n  <h3><strong>腾讯不是字节，视频号不是抖音</strong></h3>\n  <p>“微信有这么多的用户，为什么不去把电商生态做得更健全一点？可能是他怕影响他的用户。”李柔说。“电商生态和用户体验，是两条背离的路线。如果要做电商，也许会伤害一些用户的体验。问题就是，腾讯不愿意去做这个事情。”</p>\n  <p>一位商家对36氪说，在与视频号官方人员接触的过程中，对他们的印象就是8个字：不求有功，但求无过。与其说小二们的目标是让商家赚更多钱，不如说他们更在乎“不要出乱子”。</p>\n  <p>有腾讯内部人士透露说，当腾讯广告部门每周与视频号团队开会时，常常会提出“做个金融专题”或“做个大健康专题”之类的议案，以吸引相关行业的金主投放广告，而视频号团队通常会拒绝这类需求。</p>\n  <p>而且，视频号一直都保持着极低的广告加载率，年轻用户通常在停留40分钟之后才会收到广告，这是远低于其他同体量平台的。今年5月，腾讯首席战略官詹姆斯·米歇尔称，视频号广告加载率只有其他主流短视频产品的1/4左右。</p>\n  <p>一种巨大的撕裂产生了——当你去研读腾讯财报，会发现每一季度的电话会议上，腾讯高管们都强调视频号的成长空间；但当商家真正去跟视频号团队发生业务交集时，又会发现他们“过于佛系”。</p>\n  <p>虽然商家们会抱怨今天视频号的爆发力不如早期的抖音，但他们确实已经不需要第二个抖音，把从盛到卷再到衰的流程再走一遍——腾讯也清楚这一点。</p>\n  <p>今年Q2的财报电话会议上，腾讯公司总裁刘炽平说，传统的直播电商存在一个自然的增长上限，而腾讯对抗这一“自然衰退”的方式，就是“构建一个生态系统，使其区别于单纯的直播电商。”</p>\n  <p>刘炽平在电话会议上说：腾讯“重新定位了直播电商业务，使其更趋近于微信电商。”也就是说，腾讯要做的不是第二个抖音电商，而是一个前无古人的产品。</p>\n  <p>但是，每过一阵，都会有一些受不了缓慢增长和严格规则的商家离开；但同时，留在生态内的商家，又乐于享受“内卷不起来”的环境，盛赞视频号的良好氛围。</p>\n  <p>在这样复杂的局势中，作为一个普通商家，想要在视频号获得安全感太难了。那么提供安全感，就成了这些私董会、商业社群、企业家俱乐部的共同责任。</p>\n  <p>首先，私董会的主理人们，承担了像心理医生一样“作出解释”的责任。</p>\n  <p>“今天的腾讯，如果没有游戏业务、广告业务、金融业务，肯定会拼了命地把电商业务做起来。”李柔对她的学员们说，“但今天腾讯的选择太多了，这家公司更担心的不是增长慢，而是出乱子。”</p>\n  <p>以及，像刘思毅、铁头梁以及其他非常多的私董会主理人一样，李柔也会向她的学员们解读张小龙。</p>\n  <p>“张小龙是产品经理出身，他的思维不是商人思维，而是产品思维。”李柔说。“只要你不伤害他的用户，留在生态里，等他判断时机OK的时候，自然就会给你机会的。”</p>\n  <p>这类的话，在36氪调查期间，听过数不清多少次。研读张小龙，就是寻求确定性和安全感的一种表现。几乎没有一个视频号的创业者，在跟你谈论视频号的未来时，会不谈张小龙、不引用张小龙。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_dbfb0804b6bc425f994dbfa8f237624d@10269314_oswg782484oswg959oswg720_img_000?x-oss-process=image/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">36氪摄影</p>\n  <p>但是张小龙的演讲稿还是太少了，一个合格的私董会主理人，要有自己“制造信心”的能力。</p>\n  <p>“你不下牌桌，就一定有机会！”李柔说。“而且我们经历了这么多年腾讯的起伏，知道腾讯肯定是要做电商的，这只是一个时间问题。”</p>\n  <p>铁头梁也在用他自己的方式，给员工和学员们制造安全感。在那场2000人的大会上，他在大屏幕上亮出了一张照片，引发全场人心照不宣的笑声。</p>\n  <p>那张照片是他直播间的一面墙，墙上挂了一幅腾讯集团CEO马化腾的照片，左联“私域承接不封号”，右联“公域引流天天爆”，横批“化马腾飞”。</p>\n  <p>现在，每天开播之前，他都会带着全体员工对着照片拜一拜，并且把这一招也教给了他的学员们。</p>\n  <p>“有人学着我拜了，当天的销量真的就上去了。”铁头梁说。“如果你也实在想不通问题在那儿，就试试玄学吧。”</p>\n  <h3><strong>要在视频号上岸，做个好人吧</strong></h3>\n  <p>去年，余白花了将近200万加各种私董会。他对胡鼎说：“3万块以下的群你随便帮我拉，5万块以上的群我要稍微斟酌一下。”很快，他就靠对这些精准客群销售一款全球知名的减重产品而回本。</p>\n  <p>胡鼎能帮余白做这件事，因为他是私董会云林学社的创始人之一，而且他本人也花真金白银买了非常多私董会、商学院、企业家俱乐部等等的入场券。他跟他的合伙人光在交学费上，就花了600多万。</p>\n  <p>不过，许多会员他买过之后，一次活动也没去过，因为真的很忙。在工作稍微有些间隙的时候，胡鼎就有一大堆“欠下的课程”要处理。</p>\n  <p>胡鼎并不是富二代，但他盖着浙江人的思想烙印。“我从来就没有一天想过我要去给别人打工。”胡鼎大一就辍学创业，他说，“从小我父母对我的教育就是，你可以读不好书，但是不能不会做生意。”</p>\n  <p>过去几年，胡鼎在各家私董会见证了太多人踩在时代红利上突然起飞，然后又销声匿迹。这群人有更强的不安全感，更希望找到跨越周期的答案。</p>\n  <p>李柔就是这样。她本人吃到了好几波投机红利，从公众号，到广点通广告，到朋友圈，再到视频号初期，过了几年“简直是捡钱”的日子。</p>\n  <p>“经历了几轮电商平台的迭代之后，你会发现其实每个电商平台都一样，刚开始我们进场时间早，就快速地挣了钱，但随着电商平台越来越规规范化，利润就会变得非常少。”李柔说。</p>\n  <p>她的旧办公室看起来黄、旧且闷热，人在电梯里一会儿立着不动，就会被蚊子叮两个大包。“我们刚开始起家的时候就在老办公室做，那边就土一点。其实做电商就是这样，装修的太豪华，容易死掉。”李柔说。</p>\n  <p>在电商平台监管不严的早期时代，许多商家惯于使用浮夸的广告+巨额投流+杂牌货的组合拳。但是那样的日子已经结束了——投抖音ROI达不到1:1的惨案发生得也越来越频繁。</p>\n  <p>投机的机会没有了，投机者去哪呢？</p>\n  <p>“我们已经见证了抖音红利的从有到无。大家有没有想过，如果以后视频号的流量红利没了，我们该怎么办？我经常在深夜问自己这样一个问题。”铁头梁在刘思毅的大课上压轴演讲，他这样问在场的观众。</p>\n  <p>今年，直播电商GMV增长失速，许多商家都尝到了从风口上掉下来的失重感。公域的流量总会从贱到贵，从有到无。如果继续把视频号的流量耗尽，还能去哪呢？</p>\n  <p>“如果有一天视频号的流量红利也没了，下一步就是私域。”铁头梁说。那个他经常在晚上思考的问题，可能只有一个答案。那就是趁着视频号还在成长，把公域的流量抓到自己的私域来，与这些用户建立牢固的信任关系，从而不断重复下单。</p>\n  <p>铁头梁团队伙伴曾经也“走过捷径”。最开始测试视频号的时候，被封了多达100个账号。搞过无人直播测试，做过搬运混剪测试，也测试过质量不那么好的货。但是最后都放弃了。</p>\n  <p>现在，在他的私董会“梁山会”的大课上，他劝大家“在视频号，做个好人”。</p>\n  <p>“我的建议是，大家如果在视频号里想做正向循环，就老老实实，安安心心地做私域，做流量，做好产品，做高客单价，因为这种人群才是最有价值的。”铁头梁说。</p>\n  <p>劝大家这么做，有两个原因：一是因为微信的私域属性，视频号受众更依赖信任关系，也更愿意为高品质多付费，确实不适合再搞以前那一套；二是许多投机者真的把视频号，把向来更宽容、更让利的微信视作人生“对抗周期”的机会。</p>\n  <p>他们不甘心再打一枪换一个地方。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_51a63fbbbf6841e8a67b2085e9ffa357@10269314_oswg1143081oswg959oswg720_img_000?x-oss-process=image/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">36氪摄影</p>\n  <h3><strong>天派地派，沟壑纵横的中国</strong></h3>\n  <p>一个私董会会员曾跟会里的其他200个老板去某个城市旅行。他发现，起初，200个人都在1个群里没有分组，但他发现，大家很快就形成了自然分化——到聚餐的时候，十亿跟十亿的人坐到了一桌，一亿跟一亿的人坐到了一桌，千万跟千万的人坐到了一桌。</p>\n  <p>不同圈层的人，关注的问题是不一样的。在视频号这个赛道，分化也更加显著。</p>\n  <p>一个私董会往往吸引的是同类人，有的面向拧螺丝出身一夜暴富的操盘手，有的圈子是名校毕业经商世家的老钱或富二代。本来这两类私董会井水不犯河水，但视频号这个“卷入所有人”的战场把双方赶到了一起，有主理人在活动上公开批评其他私董会，说对方粗野、不尊重客户、价值观有问题，引发舌战。</p>\n  <p>“有些私董会的主理人本身就是做小生意出身的，他们拿到流量之后，再去吸引跟他一样的小老板。”王博轩把这些草莽电商人群定义为“地派”。“地派”们的草莽电商打法虽然在视频号也能延续，但视频号上特有一些属于“天派”的空间。</p>\n  <p>王博轩定义自己的社群Newmoney则为“天派”，因为他的人脉来自于在一线财经媒体接触的公司老板，他们不会教这些老板们“如何钻平台的空子”，也不需要帮他们去找人解决封号问题。他要做的，是帮他们了解如何用视频号把他们原本已经成功的事业变得更成功。</p>\n  <p>比如，一个做桥梁生意的老板，在竞标政府的桥梁订单时频频获胜。他的小秘诀就是——每逢公务员上班时间，就在视频号上推送自己的广告，让筹备竞标的工作人员总能看到自己的品牌，然后竞标时就不知不觉地比同行优势大一些。</p>\n  <p>“这是视频号上才有的熟人或者半熟人生意，是抖音或者小红书做不到的。”王博轩说。“我这儿的老板们不是要当网红。他们是在向过去的积累要结果。”</p>\n  <p>一个人，一张桌，对着镜头说三分钟。偶尔“出外景”，就是拍拍工厂和流水线。很多企业老板的个人视频号，场景都千篇一律，播放量几百。但他们并不在乎播放量，只想撬动微信里成百上千个上下游的合作伙伴，收获精准的生意线索。</p>\n  <p>王博轩觉得乐观，这样打破认知的案例还有很多，每天都有新消息。李柔的“地派”会员们，却为新环境迟迟不进化而感到焦虑。</p>\n  <p>“中文互联网，是一个信息鸿沟巨大的世界”。铁头梁在群响的活动上，对着台下的2000个人说。“在座的各位，其实你们已经领先了。今天的中国还有很多人根本不知道视频号是什么东西，甚至从来没体验过电商。你的认知，代表不了中国人的认知。”</p>\n  <p>他见过有人削尖脑袋钻平台的漏洞，研究怎么才能绕过监管快速卖出一批垃圾货；见过有人靠在直播间里跳舞、聊天，招来打赏千万；也见过有人利用舆论情绪，批量生产内容偏颇的短视频，然后转眼开播带货。</p>\n  <p>如果不能正确地认识中国的复杂性，就理解不了视频号的谨慎和缓慢，也自然看不到其中的机会。“对那些认知已经遥遥领先的老板们来说，在今天唱衰视频号没有意义。”铁头梁说。</p>\n  <p>“不要急！张小龙说：慢慢走，比较快。”刘思毅在大会上再次引用张小龙。试图安抚那些在微信慢慢走的这三年里，积压了越来越多的期望和失望的老板们。</p>\n  <p>焦虑之余，在微信慢慢走的节奏里，老板们只好进行自我心理疏导。</p>\n  <p>研读张小龙讲话当然是一种显学。不过，36氪发现，在客户资产规模较高的“天派”私董会中，普遍流行研读《毛选》——有的是主理人与好友小规模“共读”，有的是干脆组织大课开讲。</p>\n  <p>余白把这阵《毛选》热想得很明白。“这其实是一种‘爽’课。因为你再苦苦不过毛主席，你遇到困难再大也大不过毛主席。我们中的大多数人其实挣的钱都不知道怎么挣的，说白了就是赶上了。”余白说。</p>\n  <p>年轻的朋友们则求助于命理。</p>\n  <p>蓝茵所在的私董会的会员，大部分是靠短视频红利成长起来的年轻创业者。在一次大课结束后的晚饭上，蓝茵问在场学员，是否相信紫微斗数。在场的所有人都举手表示相信，其中八成人是95后。蓝茵见状，笑笑分享了自己的“命盘”。</p>\n  <p>“如果算出来命好，那就信命。如果算出来不好，那就去他妈的。”蓝茵说。在场的年轻人们一起开怀大笑。</p>\n  <p>“这一代人经历了太多轮红利，但是，属于下一代创业者的红利在哪里呢？”26岁的胡鼎发问。</p>\n  <p>有人说，是AI。但AI的门槛听起来太高了，现在的AI离成为一个“给所有人机会”的暴富时机也太远了。</p>\n  <p>相比之下，“劳动密集型产业”的视频号看起来才更像“能给所有人一杯羹”。</p>\n  <p>所以，视频号的成功，不会只是一个巨头公司的成功，不会只是腾讯股价拉升的希望，它身上还寄托了太多人的财富期待。</p>\n  <p>去年3月，铁头梁对36氪说：“我就喜欢看到别人骂视频号。他们越说这里没机会，我越高兴。这代表他们不会来跟我抢。我想当沙漠动物。不能在江河湖海里称王，能在沙漠里称王也很好啊。”</p>\n  <p>今年9月，我再次向他提起这句话，他说，他已经感受到了雨水，现在沙漠里已经有了绿洲。</p>\n  <p>在沙漠中盼甘霖的不止铁头梁。李柔决定干私董会的一个主要原因，就是为了在社群里物色合适的视频号项目，然后，投资他们，让打湿别人的雨水也打湿自己。</p>\n  <p>“风口来的时候，就像下暴雨。你只拿一个锅，根本就接不过来。所以你要做的，就是想办法把锅给弄多，让别人的锅也能为你接雨。”李柔说。“现在嘛，是雷阵雨，一会儿下雨一会儿烈日。但我相信，后面肯定还有更大的雨。”</p>\n  <p>（本文李柔、蓝茵、余白为化名）</p>",
        "published": "2024-10-31 04:02:37",
        "id": "ddca046b-f08f-4583-888e-d330badba0fc",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "文章讲述了视频号相关的私董会情况，包括私董会商机源于视频号的不足、腾讯与字节的差异导致视频号与抖音的不同、私董会为商家提供安全感、商家应在视频号做私域流量以及不同类型私董会的分化等内容。"
        },
        "tokens": 7072
    },
    {
        "title": "8点1氪｜“三只羊”辟谣小杨哥复播传言；于东来回应山姆进驻河南；中兴通讯回应联想专利诉讼",
        "link": "https://36kr.com/p/3015654297708034?f=rss",
        "description": "<h2>TOP3大新闻</h2>\n  <p><strong>“三只羊”辟谣小杨哥复播传言</strong></p>\n  <p>近日，部分三只羊切片账号更新了一些小杨哥直播的短视频，这类视频下方出现了“小杨哥27号直播回放”的搜索词条。同时，抖音多个账号称，小杨哥于10月27日再次复播，当天小杨哥和大杨哥以及三只羊旗下主播集体回归，直播刚开始时人数较少，但不到一个小时，在线人数突破10万。对于该传言，三只羊相关负责人向北京商报记者回应称不属实，没有在10月27日时直播。同时，疯狂小杨哥和三只羊网络客服也表示，没有接到主播排班情况的具体通知。</p>\n  <p>据了解，在9月26日，据合肥市监局通报，依据《行政处罚法》《反不当竞争法》等相关规定，合肥市联合调查组拟决定对三只羊公司没收违法所得、罚款共计6894.91万元。针对三只羊公司直播带货中存在虚假商业宣传行为相关问题，责令暂停经营限期整改，承担相关法律责任。三只羊当天发布声明称，就近期公司在直播中虚假宣传误导消费者的问题致歉。公司将全面接受联合调查组的调查处理意见和处罚结果，并愿承担相关法律责任。随即，抖音宣布对“三只羊”旗下账号进行停播处理。（北京商报）</p>\n  <p><strong>于东来回应山姆进驻河南</strong></p>\n  <p>近日，自河南首家山姆会员店官宣落地郑州后，网上关于山姆与胖东来的讨论不断，不少人开始比较两家的模式与产品。10月30日，胖东来创始人于东来发文回应山姆进河南，称胖东来只会为有这样优秀的企业感到喜悦，永远不会跟好品质的、对顾客好、对员工好、对社会好的任何企业有正面竞争。于东来曾多次在演讲中称，胖东来以山姆、开市客等世界优秀企业为榜样，目标是在商品、性价比和质量方面超越他们。（蓝鲸新闻）</p>\n  <p><strong>中兴通讯回应联想专利诉讼</strong></p>\n  <p>中兴通讯10月30日早间针对联想在英国起诉其专利侵权作出回应。中兴通讯称，近日得知联想在英国高等法院提起知识产权诉讼，对此感到遗憾。双方已就专利许可问题协商数年，中兴通讯始终秉持善意，追求合理回报，且基于对联想作为中国公司的信任，一直对采取协商以外的维权措施保持审慎、克制。联想远赴英国诉讼虽难以理解但表示尊重，同时强调这不会改变中兴通讯维护合法权益的决心。</p>\n  <p>作为全球领先的综合信息与通信技术解决方案提供商，中兴通讯拥有大量专利，5G标准必要专利声明量占比稳居全球前五。中兴通讯尊重知识产权价值，坚持推动合理、公平、公允的专利许可制度，通过合理回报实现技术创新的正向循环。（凤凰网科技）</p>\n  <h2>大公司/大事件</h2>\n  <p><strong>小鹏汇天“陆地航母”分体式飞行汽车将于11月12日全球公开首飞</strong></p>\n  <p>36氪获悉，据小鹏汇天消息，小鹏汇天“陆地航母”分体式飞行汽车即将亮相2024年中国航展。11月12日将在中国航展第二展区（斗门莲洲）进行全球首次公开飞行；同时，“陆地航母”也将在珠海国际航展中心8号馆进行静态展示。</p>\n  <p><strong>京东App已开通支付宝付款</strong></p>\n  <p>36氪获悉，京东App已开通支付宝付款。目前，部分用户已经能在结账时看到支付宝选项，还可使用余额宝等工具。</p>\n  <p><strong>上海将迎1981年来11月最大降雨</strong></p>\n  <p>受台风“康妮”外围环流和冷空气共同影响，10月31日傍晚到11月1日，上海将有大暴雨，过程雨量120～180毫米，东南部可达220～280毫米，最大小时雨强40～70毫米，降水具有极端性，有可能成为1981年以来11月最大降水过程。台风影响期间，申城10月31日、11月1日气温下降，最高温仅20℃，11月3日天气将恢复。（021视频）</p>\n  <p><strong>哪吒汽车正式启动公司全员股权激励计划</strong></p>\n  <p>36氪获悉，10月29日，哪吒汽车正式启动公司全员股权激励计划，将拿出5%的股份（估值约20亿元），作为股权激励分配给全体员工，同时内部宣布了工资及绩效考核的新方案。本次激励和调薪计划是为尽早实现公司经营现金流转正目标的一部分，接下来，哪吒汽车还将实施机构精简、裁汰冗员、业务聚焦、扁平管理等一系列降本增效的举措。</p>\n  <p><strong>理想汽车成立出海一级部门</strong></p>\n  <p>36氪独家获悉，理想汽车成立出海一级部门，理想出海新负责人为王进，向理想汽车销售与服务高级副总裁邹良军汇报。除了中东，拉丁美洲国家也成为理想出海的新选项。理想曾表示在海外坚持直营模式，但进度不顺，目前理想的出海策略是在当地招募经销商。</p>\n  <p><strong>10月30日足金饰品零售价突破820元/克</strong></p>\n  <p>从周大福、六福珠宝、潮宏基、谢瑞麟等黄金珠宝品牌获悉，10月30日多家品牌公布的境内足金饰品零售价已突破820元/克关口，其中较高的达826元/克，再创新高。（上海证券报）</p>\n  <p><strong>小米汽车卖到80万，雷军：不要迷信BBA，国产品牌正全面崛起</strong></p>\n  <p>36氪获悉，10月30日，有网友制作了中国市场新势力豪华车型售价排行榜，109.8万的仰望U8、81.49万的小米SU7 Ultra、80万的蔚来ET9等豪华车型悉数上榜，并表示“再不努力就只能开BBA”。对此，雷军转发相关消息表示，“BBA确实有很多值得我们国内厂商学习，但也不要迷信BBA。国产品牌正在全面崛起。有人问我，小米产品卖到80多万了？我回答是，我真正关心的是，我们产品够不够好、值不值、是否超预期。”</p>\n  <p><strong>充电宝厂商安克进军具身机器人领域</strong></p>\n  <p>一位接近安克创新的知情人士证实，安克创新已经组建了具身机器人团队，由前小米智驾量产负责人刘方负责该团队，刘方直接向安克创新副总裁、智新科技总裁祝芳浩汇报。安克创新正在招聘平台放出机器人方向的本体工程师、Unity工程师、具身智能算法工程师、机器人产品经理等相关岗位。这意味着，这家靠充电宝、数据线等业务起家的公司，已经在为具身机器人业务储备人才。（界面新闻）</p>\n  <p><strong>铃木和丰田将深化在电动汽车领域的合作</strong></p>\n  <p>铃木汽车和丰田汽车10月30日宣布，决定进一步加强合作，铃木将向丰田供应铃木开发的纯电动SUV汽车，这款新车型计划从2025年春季开始在印度古吉拉特邦的铃木汽车工厂生产。（界面新闻）</p>\n  <p><strong>美股三大指数集体收跌，热门中概股多数走低</strong></p>\n  <p>36氪获悉，10月30日收盘，美股三大指数集体下跌，纳指跌0.56%，标普500指数跌0.33%，道指跌0.22%。大型科技股多数下跌，英特尔跌超2%，苹果、英伟达跌超1%，特斯拉、奈飞、Meta小幅下挫；谷歌涨超2%，亚马逊涨1%，微软小幅走强。热门中概股多数走低，蔚来跌超4%，拼多多跌超3%，京东、小鹏汽车跌超2%，腾讯音乐、微博、阿里巴巴跌超1%；名创优品涨超4%，理想汽车、B站涨超1%。</p>\n  <p><strong>苹果正使用印度工厂进行iPhone 17的开发</strong></p>\n  <p>据两名直接了解情况的人士透露，苹果将首次使用一家印度工厂来完成明年iPhone 17基本款的前期制造工作。（新浪财经）</p>\n  <p><strong>The North Face北面入选《时代周刊》“年度最佳户外品牌”</strong></p>\n  <p>36氪获悉，近日《时代周刊》与综合数据平台Statista联合发布“2024全球最佳品牌”，The North Face北面入选为 “年度最佳户外品牌”。据了解，北面的亚太区首家UE（Urban Exploration）精品店已于广州太古汇开启。同时，北面不同层级的店铺在全国范围内逐步优化，后续将会有不同层级、多元主题的店铺陆续更新。</p>\n  <p><strong>消息称三星将于2025年初从ASML引进High-NA EUV光刻机</strong></p>\n  <p>据ET News，消息人士称，三星电子将从ASML引进首台High-NA EUV光刻机EXE:5000，预计2025年初到货。半导体设备安装通常需要较长测试时间，该光刻机预计最快2025年中旬开始运行。High-NA EUV为2纳米以下先进制程所需设备，韩国业界预期，三星也将正式启动1纳米芯片的商用化进程。（财联社）</p>\n  <p><strong>韩国新世界集团宣布拆分旗下超市和百货业务</strong></p>\n  <p>韩国新世界集团10月30日宣布，旗下大型超市易买得和新世界百货业务将全面拆分，两大业务“各立门户”。在集团当天进行的人事调动中，新世界控股公司总括社长郑有庆晋升为会长，并将执掌百货业务。这是郑有庆于2015年12月出任总括社长后，时隔9年晋升为会长。（界面新闻）</p>\n  <p><strong>美国支付巨头Visa据悉计划裁员约1400名正式及合同工</strong></p>\n  <p>据消息，知情人士透露，作为精简国际业务计划的一部分，Visa计划在今年年底前裁员约1400名雇员和合同工。他们说，大约1000个技术岗位将被裁员，其余大部分岗位将集中在销售和全球数字合作伙伴关系岗位上。Visa在全球拥有3万多名员工，部分裁员将集中在工程团队的合同工，目前还不清楚受影响的员工中有多少是非雇员。Visa发言人说，公司不断发展，以更好地服务客户和支持增长，“这可能会导致一些职位被取消”。他说，在可预见的未来，公司预计每年将雇用更多员工。（界面新闻）</p>\n  <p><strong>美国三州州长联名促波音结束罢工</strong></p>\n  <p>美国犹他、密苏里和蒙大拿三州共和党籍州长29日联名致信波音公司与组织波音员工罢工的工会组织，称3.3万名波音机械工人持续近7周的罢工已冲击多家供应商，殃及三州经济和就业，呼吁双方尽快达成协议结束罢工。（财联社）</p>\n  <p><strong>奈飞公共政策高管和首席传播官将离职</strong></p>\n  <p>奈飞10月29日表示，全球公共政策副总裁Dean Garfield和首席传播官Rachel Whetstone将离职。一位熟悉内情的消息人士称，目前尚未确定新职位的人选。另外，该流媒体平台的联席首席执行官Ted Sarandos正在为一个新设立的职位——首席全球事务官寻找候选人，以监督公共政策和沟通。（新浪财经）</p>\n  <h2>AI最前沿</h2>\n  <p><strong>OpenAI据称已计划联手博通和台积电共同打造自研芯片</strong></p>\n  <p>据消息，消息人士称，OpenAI正在与博通和台积电合作，制造其首款内部芯片，用于支持其人工智能系统，同时在英伟达芯片的基础上增加AMD芯片，以满足其激增的基础设施需求。OpenAI已经研究了一系列方案，以实现芯片供应多样化并降低成本。OpenAI考虑过在公司内部制造，也考虑过为一项昂贵的计划筹集资金，即建立一个被称为“代工厂”的芯片制造工厂网络。由于建立网络需要成本和时间，公司暂时放弃了雄心勃勃的代工厂计划，转而计划专注于内部芯片设计工作。（界面新闻）</p>\n  <p><strong>NVIDIA以太网加速xAI构建的全球最大AI超级计算机</strong></p>\n  <p>36氪获悉，NVIDIA宣布，xAI位于田纳西州孟菲斯市的Colossus超级计算机集群达到了10万颗 NVIDIA® Hopper GPU的巨大规模。该集群使用了NVIDIA Spectrum-X™以太网网络平台，该平台是专为多租户、超大规模的AI工厂提供卓越性能而设计的RDMA（Remote Direct Memory Access）网络。Colossus是世界上最大的AI超级计算机，目前正被用于训练xAI的Grok系列大语言模型，以及作为X Premium用户功能之一的聊天机器人（Chatbot）。</p>\n  <h2>大公司财报</h2>\n  <p><strong>比亚迪季度收入首超特斯拉</strong></p>\n  <p>10月30日晚，比亚迪发布了2024年三季度业绩，前三季度公司实现营业收入5022.5亿元，同比增长18.9%，实现净利润252.4亿元，同比增长18.1%。</p>\n  <p>单看第三季度业绩的话，比亚迪已经在营收上反超了特斯拉。2024年第三季度，比亚迪实现营收2011.25亿元，而特斯拉三季度营收换算后接近1800亿元。这意味着，比亚迪达成了电动车发展历程中的又一个“里程碑”。（证券时报）</p>\n  <p><strong>瑞幸咖啡：第三季度总净营收101.8亿元，同比增长41.4%</strong></p>\n  <p>36氪获悉，瑞幸咖啡发布第三季度财报。财报显示，第三季度总净营收为101.8亿元，较上年同期增长41.4%；净利润13.0亿元，同比增长32%；经调整每股ADS净收益 4.40元。</p>\n  <p><strong>京沪高铁：前三季度净利润100.15亿元，同比增长12.35%</strong></p>\n  <p>36氪获悉，京沪高铁发布第三季度财报。财报显示，前三季度实现营业收入323.55亿元，同比增长4.77%；归属于上市公司股东的净利润100.15亿元，同比增长12.35%。第三季度实现营业收入114.93亿元，同比下降0.94%；归属于上市公司股东的净利润36.58亿元，同比下降3.17%。</p>\n  <p><strong>联发科第三季度营业利润238.6亿元台币</strong></p>\n  <p>联发科第三季度营业利润238.6亿元台币，预估230.2亿元台币；第三季度净利润255.9亿元台币，预估229.5亿元台币。（财联社）</p>\n  <p><strong>中国银行：第三季度净利润571.6亿元人民币，同比增长4.38%</strong></p>\n  <p>36氪获悉，中国银行发布第三季度财报。财报显示，中国银行第三季度营业收入1611.76亿元人民币，同比增长6.58%；第三季度净利润571.6亿元人民币，同比增长4.38%。前三季度，集团实现税後利润1,874.89亿 元，实现本行股东应享税後利润1757.63亿元，同比分别增长0.53%和0.52%。平均总资产回报率（ROA）0.75%，净资产收益率（ROE）9.55%。核心一级资本充足率2为12.23%，一级资本充足率2为14.36%，资本充足率2为19.01%。</p>\n  <h2>投融资</h2>\n  <p><strong>“视旅科技”获亿元A轮融资</strong></p>\n  <p>36氪获悉，“视旅科技”近日宣布完成亿元A轮融资，投资方包括深报一本基金、广州城投集团下属粤港澳大湾区文化产业投资基金、高榕创投、优山投资、亚康股份投资平台中互亚康等投资机构，总规模超亿元人民币，华兴资本担任本轮融资独家财务顾问。本轮资金将用于加速VtripGPT旅游大模型的研发升级，并持续推动其在国内外优质旅游目的地的市场化应用。</p>\n  <p><strong>“向量方程”获近千万天使轮融资</strong></p>\n  <p>36氪获悉，智能数字人平台开发商“向量方程”此前完成近千万元天使轮融资，由真成资本（已投资得到APP、印象笔记）领投，北京极信管理咨询和上海天使汇跟投。资金将用于丰富数字人产品技术的研发。</p>\n  <p><strong>“GMI Cloud”获8200万美元A轮资金</strong></p>\n  <p>36氪获悉，AI云计算服务商“GMI Cloud”宣布完成A轮融资，金额总计8200万美元。此轮融资由Headline Asia领投，同时获得亚太区智能能源解决方案提供商Banpu以及纬创资通的战略投资。这笔资金将用于科罗拉多州数据中心的建设，以强化GMI Cloud在全球AI算力服务方面的布局。</p>\n  <p><strong>“智源深澜”完成数千万元种子轮融资</strong></p>\n  <p>36氪获悉，“智源深澜”已完成数千万元种子轮融资。本轮融资由英诺天使基金领投，水木清华校友种子基金、零以创投跟投，募集资金将主要用于以功能为导向的生物分子生成式AI平台开发、自动化蛋白质和多肽功能进化平台建设、以及商业市场的开拓。智源深澜创办于2024年，专注从事数据驱动的生物分子设计和制造，由镁伽科技孵化成立。</p>\n  <h2>酷产品</h2>\n  <p><strong>苹果发布新款MacBook Pro，搭载M4系列芯片</strong></p>\n  <p>10月30日，苹果发布新款MacBookPro，搭载M4系列（M4、M4 Pro和M4 Max）芯片。新款MacBook Pro专为Apple Intelligence设计。14英寸机型配备M4芯片和3个雷雳4端口，标配16GB内存，起售价12999元。搭载M4 Pro和M4 Max芯片的14与16英寸机型支持雷雳5，传输速度更快。新款MacBook Pro将于11月1日上午9点起接受预购，11月8日起正式发售。苹果表示，Apple Intelligence推出时间依监管部门审批情况而定。（界面新闻）</p>\n  <p><strong>马斯克：2040年人形机器人数量将超过人类，售价2-2.5万美元</strong></p>\n  <p>10月29日，埃隆·马斯克在沙特阿拉伯“未来投资倡议”大会上谈到人形机器人时表示，他认为到2040年，人形机器人的数量可能会超过人类，届时至少会有100亿个人形机器人，每个价格在2万-2.5万美元之间。马斯克还表示，这将推动人类进入一个富足的时代。（智通财经）</p>\n  <p><strong>整理｜</strong> 郝禹琦&nbsp;</p>",
        "published": "2024-10-31 00:03:49",
        "id": "9d1ef71f-45eb-47ba-bd7b-cd9157ee3d6d",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "8点1氪资讯涵盖商业、科技等多方面内容，包括三只羊辟谣小杨哥复播、于东来回应山姆进驻河南、中兴通讯回应联想专利诉讼等事件，还有小鹏汇天飞行汽车首飞、京东开通支付宝付款等各类公司动态、大公司财报、投融资消息、酷产品发布等。"
        },
        "tokens": 5309
    },
    {
        "title": "三星暗示有望近期开始向英伟达供应HBM芯片",
        "link": "https://36kr.com/newsflashes/3016141164586505?f=rss",
        "description": "韩国三星电子公司周四暗示，有可能在近期向美国人工智能巨头英伟达提供先进的高带宽存储器（HBM）。这家韩国科技巨头一直在努力让其HBM3E芯片通过英伟达的质量测试，而其本土竞争对手SK海力士公司最近已开始量产业界领先的12层HBM3E芯片。三星电子内存业务副总裁Kim Jae-june在第三季度财报公布后召开的电话会议上表示：“目前，我们正在量产8层和12层HBM3E产品。”（新浪财经）",
        "published": "2024-10-31 08:44:21",
        "id": "fc608522-a1fc-4d90-9860-b39419bfbdd7",
        "source": "36氪",
        "section": "综合资讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "三星暗示近期可能向英伟达供应HBM芯片，三星正在量产8层和12层HBM3E产品，其本土竞争对手SK海力士已开始量产业界领先的12层HBM3E芯片。"
        },
        "tokens": 715
    },
    {
        "title": "科氪 |荣耀Magic7系列发布：开创AI智能体新纪元，重塑智能手机未来",
        "link": "https://36kr.com/p/3016182774048001?f=rss",
        "description": "<p>2024年10月30日，荣耀于深圳正式发布了年度AI旗舰手机——荣耀Magic7系列，这不仅标志着智能手机行业正式迈入AI智能体时代，更是一次对智能手机未来发展方向的深刻探索与重塑。凭借独树一帜的创新理念与卓越技术实力，以及领先行业的AI能力，荣耀Magic7系列重新定义了智能手机的想象力边界。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_731ae43b2ef74b1cb1a0a383e6d3f740@517825446_oswg1017596oswg1280oswg853_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀终端有限公司CEO赵明表示：“AI毫无疑问是现在最引人注目的科技魔法，但荣耀很早就看到了AI让手机进化的魔力，我们坚信AI是智能手机的未来。从拍照、屏幕、续航到通信，我们用AI魔法使能和重构⼀切，包括操作系统。荣耀Magic7系列将会是引领未来的AI手机。”</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_0f5598f35ddc4104804ec9e56abcb6cf@517825446_oswg362996oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>作为集荣耀AI技术创新大成之作的旗舰手机，荣耀Magic7系列在系统、影像、屏幕、通信、续航、性能等多个维度，均实现了AI全面赋能的革新。尤其在搭载荣耀自主研发的YOYO智能体之后，可实现自动执行、一语到位的高阶智慧能力，为用户带来了颠覆性体验。</p>\n  <p>据了解，荣耀Magic7系列首发搭载的MagicOS 9.0的AI大模型能力，获得中国信通院权威行业认证，获颁泰尔测评证书卓越级；在中国信通院颁发的终端智能化分级能力证书中，首发搭载MagicOS 9.0的荣耀Magic7系列通过《终端智能化分级测试方案》评估，终端智能化水平达到行业目前最高等级L3。并且荣耀也是业内唯一一家达到该智能化水平的终端厂商。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1d84e57d590e48449eb357de27a04f15@517825446_oswg560886oswg1269oswg423_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>在智能手机市场面临创新瓶颈的当下，荣耀Magic7系列的发布，不仅是对智能手机未来的深刻探索，更为行业注入了新的活力与灵感。以荣耀为代表的中国企业，终于站在了AI浪潮的前列，率先进入了智能手机的”自动驾驶”时代。</p>\n  <p><strong>用真AI开启智能手机的“自动驾驶” &nbsp;</strong></p>\n  <p>在AI手机元年拉开序幕的2024年，几乎所有的旗舰新机和操作系统都纷纷打出了“AI化”的旗号，市场上充斥着各种声称具备AI功能的手机和操作系统，各种新功能也都纷纷冠以“AI”之名。然而，荣耀Magic7系列却凭借其深入底层的AI技术创新，真正树立了AI手机的里程碑之作，开启了AI手机“自动驾驶”的新时代。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_22a6d95ea1dd46d8afa58c040651554f@517825446_oswg314348oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>目前，多数手机厂商仍然以嫁接式的生成式AI服务作为其可以被称作“AI手机”的理由，但此类AI能力更多是通过开放接口，将大模型技术集成到手机的具体应用中，如照片编辑、语言翻译、记笔记、发短信、搜索等。然而，真正的AI手机所追求的，绝非仅限于这种“应用层AI”的浅尝辄止，而是要用AI从操作系统的底层开始，彻底重构服务逻辑、业务流程和资源分配，超越现有以应用程序为基础的操作系统框架。</p>\n  <p>在荣耀看来，真正的AI手机需要用AI技术全面重塑底层的硬件和操作系统，从用户体验到业务逻辑的每一个细节，都需经历深刻的变革。荣耀Magic7系列搭载业界首个实现商业化落地的AI智能操作系统MagicOS 9.0正是平台级AI能力的真正体现。</p>\n  <p>荣耀Magic7系列通过全新的YOYO智能体，实现了纯AI视觉、无需生态适配的任务自主执行新突破。无论是单一指令的系统级任务，还是第三方应用任务，甚至是多应用的协同执行，YOYO智能体都能游刃有余地处理。从“一句话点咖啡”到“一句话关闭应用权限”，YOYO智能体以精准的理解和自动执行，为用户带来了前所未有的便捷体验。这一创新不仅改变了“人理解手机”和“人找服务”的传统模式，更让手机拥有了自主行动力，率先引领智能手机从“手动驾驶”迈入“自动驾驶”的新时代。</p>\n  <p>此外，YOYO智能体还能根据当前屏幕内容，主动提供智慧服务，如英文翻译、文章摘要、日程创建等，实现多轮、多意图、全屏意图的主动理解与响应。这种智能化的服务方式，不仅提升了人机交互的便捷性和效率，更让用户感受到了仿佛“自动驾驶”般的智能体验。</p>\n  <p>除了前沿的AI体验外，荣耀Magic7系列在数据安全与隐私保护方面也展现出了卓越的能力。通过创新的端侧AI换脸检测技术和系统默认守护模式，荣耀Magic7系列能够实时分析视频通话，准确识别并预警AI换脸风险，且所有检测均在本地完成，确保用户信息安全无虞。</p>\n  <p><strong>里程碑式的AI旗舰，全面赋能硬件</strong></p>\n  <p>AI作为核心基石，深度融入手机的每一个层面，方能称之为真正的AI手机。在这一新兴领域中，AI手机需通过智能技术无缝链接软硬件，确保每台设备都能根据用户的独特需求，转化为专属的、高度个性化的超级智能伙伴。依托其强大的平台级AI能力，荣耀Magic7系列手机不仅在操作系统层面实现了质的飞跃，更在摄影、显示、通信、电池续航、性能优化及音频体验等多个维度，带来了颠覆性的提升与智能化变革，重新界定了智能手机的极限。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_a5a9cb903123428c972ad3dc156d75b3@517825446_oswg721141oswg2286oswg810_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>在影像技术领域，荣耀Magic7系列搭载了开创性的荣耀AI驭光引擎，将拍摄与后期处理巧妙融合。同时在AI技术的助力下，荣耀鹰眼相机与舞台模式实现了更为精准、高效的拍摄表现。这种前所未有的软硬协同能力，不仅突破了当前移动影像技术的瓶颈，更为整个行业开辟了一片由AI引领的移动影像新天地。</p>\n  <p>在屏幕护眼方面，荣耀Magic7系列开创性地搭载了业界唯一的全局全天候荣耀AI自然光绿洲护眼屏，该技术通过平台级AI算法，精妙地模拟自然光，从波动、亮度、节律、色彩、频闪和光谱六大维度进行全面优化，集成了圆偏振光护眼、4320Hz超高频PWM调光、类自然光护眼、自然色彩显示、硬件级低蓝光、AI离焦护眼技术、AI干眼友好技术、AI助眠显示功能的八大护眼技术，这标志着手机屏幕护眼技术迈入了一个崭新的时代。</p>\n  <p>在通信技术领域，荣耀Magic7系列的荣耀优速通功能凭借AI智能识别网络拥堵场景，实现专线加速，显著提升网络流畅度。荣耀Magic7 Pro首次搭载荣耀通信芯片 HONOR C2，全面覆盖36种用户通信场景，实现更佳信号表现，行业首发双Wi-Fi芯片，可实现双Wi-Fi聚合下载，通过调用主Wi-Fi芯片2.4GHz和5GHz以及HONOR C2备份Wi-Fi芯片的5GHz频段，在咖啡、酒店、商场、校园网、公司等限速场景下， 应用市场、王者荣耀更新下载等可以带来更快的下载体验，缩短用户等待时间。同时，荣耀鸿燕通信也借助AI技术对卫星通信功能进行了优化，确保实机使用的稳定性和易用性。</p>\n  <p>在电池续航方面，荣耀Magic7系列搭载的全新升级的第三代青海湖电池技术和自研能效增强芯片HONOR E2，重塑续航边界，让每一份电量都充满智慧与力量。通过尖端硬件与智能AI算法的深度融合，进行全方位、多层次的能效优化，实现了荣耀Magic7系列的超长续航能力。值得一提的是荣耀Magic7 Pro推出了行业首创的第三代三极耳技术，这一创新设计提供了最佳的10%硅碳负极+100W有线+80W无线快充综合解决方案，实现了高能量密度和快充能力的极致均衡设计。在性能方面，骁龙®️8至尊版移动平台与荣耀AI技术的深度融合，充分激发了端侧AI的潜能，为产品创作和生产力提升注入了澎湃动力。同时，散热系统也在AI技术的优化下，达到了业界领先的散热效果。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_46d887825c224360bc7de59702669be1@517825446_oswg509000oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀Magic7系列的问世，标志着荣耀正带领行业向更智能、更人性化的未来挺进，触发了一场由荣耀主导的“荣耀效应”变革，凭借AI与硬科技的双重引擎，荣耀正加速推动整个行业向新高度进发。</p>\n  <p><strong>独立四周年&nbsp;荣耀艰难而正确的路径迎来开花结果</strong></p>\n  <p>在发布会现场，一张意味深长的图文：“用自己的名字，去自己的远方”，被外界视为荣耀独立四周年的真实写照。自从独立以来，荣耀选择的是面向未来的“创新引领”路线，站在明天看今天，以此不断牵引带动国内产业价值链条攀升，相比行业盛行的拿来主义、看重短期利益的“创新变现”做法，这注定是一条艰难但是正确的道路。时至今日，荣耀的坚持已经逐渐开花结果。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_64ea939b634247e8bd6cabc591789f99@517825446_oswg1592706oswg2386oswg792_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>第三方机构预测，荣耀即将迎来的是一次井喷式的历史机遇：面对生成式AI手机市场预计的爆发式增长——IDC预测2024年出货量将激增364%至2.342亿部，2028年更将达到9.12亿部的广阔蓝海。荣耀凭借深厚的技术底蕴与不懈的创新追求，蓄势待发，准备继续引领智能手机行业的未来走向。</p>\n  <p>荣耀Magic 7系列堪称是荣耀技术创新的集大成者，不仅在AI能力上实现了断档式领先，也巩固了荣耀在高端旗舰市场的领先地位。这一旗舰系列的问世，背后是荣耀在端侧AI领域多年的深耕细作与不断突破技术界限的坚持。</p>\n  <p>早在2016年，荣耀便以前瞻性的布局，从系统底层开始构建全面的AI技术优势。从第一代荣耀Magic系列智能手机搭载荣耀Magic Live智慧引擎，到2018年荣耀Magic第二代启用自进化、自学习的智慧生命体YOYO，再到2022年底发布AI使能的个人化全场景操作系统MagicOS 7.0，荣耀在AI技术的探索上从未停歇。2023年，荣耀更是首次提出将AI大模型引入端侧，进一步推动了AI技术在智能手机领域的应用。2024年，荣耀首次提出AI的四层架构，为智能终端的发展提供了清晰的路径。如今，随着荣耀Magic7系列的发布，荣耀YOYO智能体再次实现了端侧AI的创新叠加效应和用户体验增值，标志着智能手机行业正式迈入智能体时代。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_243adb2857b7492782fd3872e6d9d47d@517825446_oswg216695oswg1269oswg423_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">官方图片</p>\n  <p>荣耀AI的成功并非偶然，背后是其在技术研发上的坚定投入。2023年，荣耀整体研发投入占到总营收的11.5%，AI研发费用累计已达100亿，AI专利成果达2100篇，其中AI意图识别相关专利就有600类。同时，荣耀持续加大AI高精尖人才的招聘力度，近两年校招博士浓度高达36.2%，且对顶级AI人才薪酬不设上限，为公司的持续创新提供了坚实的人才保障。</p>\n  <p>&nbsp;</p>\n  <p>并且，荣耀的技术创新并不仅限于AI领域。在通信、续航、屏幕、玻璃、影像以及折叠屏等多个领域，荣耀同样取得了显著的领先优势。从鸿燕通信技术的突破，到青海湖电池技术的革新，从绿洲护眼屏的推出，到巨犀玻璃的应用，再到鹰眼相机的问世，荣耀不断为智能手机市场带来新的亮点与惊喜，为行业提供了极具价值的解决方案，推动了智能手机技术的全面发展与进步。</p>",
        "published": "2024-10-31 09:02:07",
        "id": "5cef7749-e9c3-4423-bec8-a3d7df515462",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 5
            },
            "keyFacts": "2024年10月30日荣耀发布Magic7系列手机，该手机凭借诸多AI技术实现多维度革新，开启智能手机AI智能体时代，荣耀多年在AI等多领域的技术创新和研发投入使其取得如今成果并巩固高端旗舰市场地位。"
        },
        "tokens": 4441
    },
    {
        "title": "当一辆智能汽车有了「直觉」，会发生什么？",
        "link": "https://36kr.com/p/3011983733961600?f=rss",
        "description": "<p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241029/v2_0c7938bb83a043328fe7b8e3d9cf5fcc@6129457_oswg1803888oswg4032oswg3024_img_jpg?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">头图来源：智己官方</p>\n  <p>作为大自然赋予人类的生存密码，「直觉」往往可以在某些微妙瞬间帮助我们提前感知危险，甚至是化险为夷。&nbsp;</p>\n  <p>这种能力的背后是，人类在亿万年来经历了无穷无尽的危急场景锤炼，再基于对过往经验的强化记忆以及持续思考，进化出来的无意识的信息处理过程。&nbsp;</p>\n  <p>眼下，智能汽车的发展正在模仿人类的驾驶行为以及思考方式。&nbsp;</p>\n  <p>譬如在应对黄金800ms处理加塞、大车智慧躲避等典型场景；或是进一步应对插空变道、加减速变道等复杂场景上。&nbsp;</p>\n  <p>智己汽车已经率先进入Next Level：&nbsp;</p>\n  <p>今天，智己汽车举办「直觉·新时代」智能驾驶技术发布日，宣布IM AD 3.0已经具备了基于人工智能的直觉能力。因此，即便面对未知障碍，IM AD 3.0也可以实现以本能反应为主导、省时果断的快思考。&nbsp;</p>\n  <p>实际上，IM AD 3.0的直觉能力得益于智己汽车研发的量产一段式端到端智驾大模型。&nbsp;</p>\n  <p>在端到端大模型的支持下，智己汽车已进化出无图城市NOA全国全系可开的能力，IM AD在德、法等欧洲国家也能驾驶流畅、安全。&nbsp;</p>\n  <p>当市场上绝大部分主流汽车产品还在「模仿人类」的时候，当端到端大模型的一段式/两段式路线之争不绝于耳的时候，智己汽车的智驾技术已经实现了断代式进化。&nbsp;</p>\n  <h2><strong>在正确的赛道狂奔、领先</strong></h2>\n  <p>人类大脑的直觉，犹如一套精密算法，基于经验持续进化而来。&nbsp;</p>\n  <p>实际上，智能汽车的大脑也一样。&nbsp;</p>\n  <p>对于早期的智能汽车而言，受限于智能化技术程度以及软硬件性能，汽车行业玩家的普遍做法是，将车辆可能遇到的道路场景预先定义，然后将对应的代码写进智驾系统里。&nbsp;</p>\n  <p>例如，研发人员人为汽车输入「水坑」场景的定义，并使用大量数据训练感知模型来识别水坑、再训练规划模型绕开水坑。&nbsp;</p>\n  <p>然而，真实世界的场景是无穷无尽的。如果车辆遇到新出现的场景，则需要研发人员持续更新算法规则。这样不仅效率低，还容易出错。&nbsp;</p>\n  <p>端到端大模型技术的兴起，改变了这样的局面。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241029/v2_47bf8e4de67e424883f2ef80f55e78cd@6129457_oswg34526oswg1080oswg359_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">来源：智己官方&nbsp;</p>\n  <p>基于过往积累的海量行车场景信息以及仿真测试数据，智己汽车的IM AD智驾大模型已经能够如同人类驾驶员一般，凭直觉预判并处理突发状况。&nbsp;</p>\n  <p>而且，通过模拟人类的直觉与思考，高阶智驾系统不仅可以直接「看路开车」，在面对人车混行、高峰期拥堵等特殊场景时也能处理得当。&nbsp;</p>\n  <p>作为行业内为屈指可数的「全国无图NOA」品牌之一，智己汽车在高阶智驾方面真正实现了全场景可用。&nbsp;</p>\n  <p>地区方面，IM AD 3.0能够摆脱对高精地图的依赖，从容应对各种长尾场景，比如无保护左转博弈、礼让盲区横穿。无论是在国内，还是在德、法等海外国家，都能纵享丝滑。&nbsp;</p>\n  <p>智己汽车之所以能做到如此成绩，很大程度上离不开其对一段式端到端大模型技术路线的坚定选择。&nbsp;</p>\n  <p>现阶段，智能汽车玩家对端到端大模型的研发主要分为两派，即一段式和两段式。&nbsp;</p>\n  <p>我们可以这么来理解：&nbsp;</p>\n  <p>l 一段式方案将传统智驾系统的感知、规划、决策等多个环节融合到一个大模型之中，当外部信息通过传感器输入至系统，大模型可以快速地给输出决策，帮助车辆进行下一步动作；&nbsp;</p>\n  <p>l 两段式方案通常由感知模型和规控模型两个部分组成，外部信息的处理步骤是，经过感知模型过滤，再传输到规控模型进行计算和决策输出。&nbsp;</p>\n  <p>相比起两段式方案，智己汽车所拥趸的一段式方案决策链路更短、效率更高、对场景的理解更全面。即便遇到未定义的物体时，智驾大模型也可以学习和应对这一场景，成功绕开障碍物。&nbsp;</p>\n  <p>这一技术突破了感知模型的局限性，实现了更加智能的路径规划和驾驶决策，为用户带来更优异的使用体验。&nbsp;</p>\n  <h2><strong>技术创新，断代进化的基石</strong></h2>\n  <p>如前所言，一段式端到端大模型技术路线虽好，但想要将它的效果做到极致也并非易事。这项技术对数据规模以及数据质量的要求非常高。&nbsp;</p>\n  <p>对于智驾大模型而言，数据量越大，能力提升就越快；学习场景越多元化，路径规划就越准确。&nbsp;</p>\n  <p>但是基于多年的发展，当下智能驾驶车辆已经能够处理绝大部分常见的道路场景，高价值的Corner Case数据可遇不可求。&nbsp;</p>\n  <p>另外，大模型还会学习人类驾驶员的驾驶行为，一旦某一种驾驶风格的样本量过大，就可能影响整个智驾系统的效果。&nbsp;</p>\n  <p>因此，怎么在保证数据数量的同时控制数据质量，就成为了一段式端到端技术路线玩家必须解决的问题。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241029/v2_60c8a1a9c3cb4a77807d35ab78581f43@6129457_oswg52468oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">来源：智己官方&nbsp;</p>\n  <p>跻身智能驾驶第一梯队的智己汽车给出的创新思考是——行业首创「长短期记忆」结合模式，学习人脑「直觉推理+逻辑分析」的问题处理方式。&nbsp;</p>\n  <p>简单来说就是，将智驾大模型的数据处理分为短期记忆和长期记忆两条支路。&nbsp;</p>\n  <p>短期记忆的目的在于快速验证算法有效性以及评估数据质量，训练周期以天计算；&nbsp;</p>\n  <p>长期记忆则是将筛选过后的优质数据以人脑类似的记忆原理进行积累，应用于端到端大模型。&nbsp;</p>\n  <p>长短期记忆的支路同时也对应着「快」和「慢」两个系统。&nbsp;</p>\n  <p>快系统即直觉推理，善于直觉与经验快速处理问题，形成决策。慢系统即逻辑分析，通过端到端大模型中的安全逻辑网，增强智驾系统的决策安全性。&nbsp;</p>\n  <p>直觉与安全逻辑模型共同赋能智己IM AD，使其达到最为理想的平衡状态。&nbsp;</p>\n  <p>这样一来，不但数据质量有了保证，也减少了不必要的数据处理成本，训练成本常规方式小10到100倍，而智驾系统的安全性则超人类驾驶员10倍以上。&nbsp;</p>\n  <p>至于数据规模，智己汽车已经积累了亿级别的事件优质数据。通过海量数据的积累和自动化闭环数据链路的驱动，IM AD数据飞轮不断迭代，反哺用户体验。&nbsp;</p>\n  <p>除了软件能力，硬件在IM AD架构中也起到至关重要的作用。&nbsp;</p>\n  <p>据介绍，智己汽车是当下汽车行业唯二完成Xavier/Orin高低算力双平台的自研玩家，最大化激发硬件的潜能。比如，通过不断的模型优化，智驾大模型的运行效率能够提升500%，算力需求降低90%。&nbsp;</p>\n  <p>因此，软硬结合之下，IM AD才能做到仅需单激光雷达和单OrinX，实现「全球都能开」的无图NOA。&nbsp;</p>\n  <p>智己汽车也由此成为领跑行业的全球首批量产一段式端到端大模型的品牌。&nbsp;</p>\n  <h2><strong>十年磨一剑，出鞘必惊人</strong></h2>\n  <p>根据智己汽车的调研数据，IM AD智驾安全水平已达到人类驾驶的6.7倍；常用IM AD智驾功能的用户，安全行驶里程比常规用户提升了3.1倍。&nbsp;</p>\n  <p>与此同时，智己汽车的智驾合规道路测试里程超10万公里、仿真道路测试里程超150万公里、算法开发测试里程超250万公里、安全场景仿真近9.5万例。&nbsp;</p>\n  <p>从结果来看的话，智己汽车正在朝着其完全自动驾驶的终极愿景大步迈进，IM AD也是为此而生的。&nbsp;</p>\n  <p>2014年，上汽前瞻团队开始对前沿的智驾领域展开调研，并在三年内决定设立上汽人工智能实验室，对深度学习进行全面布局。&nbsp;</p>\n  <p>在过去的这十年里，上汽从未停止对智能驾驶技术研发以及商业落地的探索，包括催化了智能驾驶第一梯队成员智己汽车的诞生。&nbsp;</p>\n  <p>为了保证技术迭代的连续性、以及通过不同级别的智能驾驶技术为用户提供多维服务，智己L2、L3、L4共享一段式端到端大模型，只需根据不同级别的智驾需求增减硬件即可。&nbsp;</p>\n  <p>基于同源开发架构，上述三者之间可形成牢固的互补关系。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241029/v2_a67cfd1fe02a4d93b1160b33e5e1b60c@6129457_oswg81936oswg1080oswg810_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">来源：智己官方&nbsp;</p>\n  <p>通过L2级别大量的工程实践、数据获取及训练，反哺L3、L4级别的技术迭代；同时，L3、L4级别的技术研发成果，又能进一步赋能于L2场景，实现更安全、舒适的智驾体验。&nbsp;</p>\n  <p>如今正值十年的关键节点，智己汽车也交上了一份亮眼的智驾答卷，成为了全国首个同时具备L2、L3、L4智能驾驶量产能力的品牌。&nbsp;</p>\n  <p>比如前文提到的，今年10月，“IM AD 3.0 无图NOA”全国开通，推送全系车型。&nbsp;</p>\n  <p>再比如，今年7月，智己汽车拿下中国首批L3自动驾驶上路通行试点名单，顺利开启L3级自动驾驶道路测试；目前，华为小鹏等玩家尚不在名单之中。&nbsp;</p>\n  <p>智己汽车计划，今年之内获得L4级无驾驶人道路测试牌照；到2026年，正式具备L3级自动驾驶方案量产条件。&nbsp;</p>\n  <p>可以预见的是，随着L3/L4高级自动驾驶的落地和推进，智己汽车的L2量产智能驾驶在未来几年内还会迎来新一轮能力提升。&nbsp;</p>\n  <p>在智己汽车的看来，智能驾驶将遵循摩尔定律，软件体验呈现出指数级提升，两年提升10倍、四年提升100倍。&nbsp;</p>\n  <p>他们的决心是，让所有智己用户的智驾体验领先一代。&nbsp;</p>",
        "published": "2024-10-31 09:26:00",
        "id": "ff2ae721-6b50-4872-ac1b-e6ab441bc316",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 5
            },
            "keyFacts": "智己汽车举办智能驾驶技术发布日，宣布IM AD 3.0具备基于人工智能的直觉能力，这得益于其一段式端到端智驾大模型，智己汽车采用长短期记忆结合模式等创新方式解决数据问题，在软硬结合下做到无图NOA，成为领跑行业的品牌，且朝着完全自动驾驶愿景迈进"
        },
        "tokens": 3839
    },
    {
        "title": "阿里元境大裁员，继Yun OS后，再放弃元宇宙OS",
        "link": "https://36kr.com/p/3016154760737664?f=rss",
        "description": "<p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_a65d717f3f634d4f8dec69ccecc0746e@6030424_oswg1616554oswg1080oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">图源：福州国际数字产品博览会&nbsp;</p>\n  <p>在2021年Meta改名掀起元宇宙热潮时，阿里巴巴重金投入并打造了\"元境\"的团队，希望在这场新赛道竞争中抢得先机。但三年过去，这个曾经投入数十亿元、数百人的团队，如今却在互联网寒冬中按下暂缓键。 &nbsp;&nbsp;</p>\n  <p>10月31日，AI鲸选社独家获悉，阿里元境团队大裁员，很多员工反馈31日是“last day”，元境杭州和上海两地团队都被波及。这是阿里继YunOS之后，主动退场的又一操作系统，\"元境\"被阿里定义为元宇宙操作系统。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f1930e87c0a645aa948bb16abde86f2b@6030424_oswg18211oswg486oswg254_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>在理想与现实博弈的故事，也是中国科技巨头在新赛道上探索的缩影。&nbsp;</p>\n  <p>阿里云游戏事业部实际成立于2019年，服务了游戏行业、文旅行业以及一些商务行业，早期元境是阿里巴巴旗下以实时云渲染、实时云流为主要核心技术的技术提供方。&nbsp;</p>\n  <p>在事业部最初成立的6个月后，阿里巴巴发布了第一个版本的PaaS平台，提供了PC端游在安卓端上的类似体验，元境开始在云游戏业务启航。2021年9月，阿里巴巴正式发布了品牌“元境”，并于同年10月推出云游戏开发者平台。&nbsp;</p>\n  <p>“云游戏会是元宇宙的一个起点”，在2021年元宇宙大热后，元境开始在这里领域大规模布局。&nbsp;</p>\n  <p>在元宇宙赛道上，各家巨头选择了不同的切入点：Meta all in AR/VR硬件，投入超过100亿美元；腾讯则围绕社交和游戏展开布局；字节跳动通过Pico切入VR内容生态。而阿里的选择打造元宇宙时代的操作系统和云游戏平台。&nbsp;</p>\n  <p>\"目标是成为元宇宙时代的安卓系统。\"一位元境团队人员比喻。为实现这一目标，阿里投入了大量资源。据公开信息，元境布局全国2800+个云服务节点部署实时云渲染服务，通过广覆盖解决用户最后一公里的触达。&nbsp;</p>\n  <p>在操作系统层面，元境的野心不可谓不大。团队开发的渲染引擎可支持10万级同屏在线用户，这在业内属于领先水平。在工业应用上，元境还与某汽车制造商合作，将数字孪生技术应用于生产线优化，据称帮助对方提升了15%的生产效率。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f6c8ff1f037d4037a0dcbb80273ed14f@6030424_oswg142962oswg1080oswg599_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">元境的东湖樱园项目&nbsp;</p>\n  <p>云游戏同样是元境的重要布局。依托阿里云遍布全球的数据中心，元境推出的云游戏服务在延迟控制上具有优势，平均延迟低于40ms。2023年初，元境还与多家游戏厂商达成合作，计划将3A级大作搬上云端。&nbsp;</p>\n  <p>阿里元境CTO郭旷野还在2023年表示：以今年AI技术的发展，Diffusion、LLM、NeRF这些技术的快速发展，在内容创作和生成上对元宇宙的发展是非常有帮助的。元境也确实在大规模利用AIGC技术提升业务，也布局了AI数智人等比较好商业化的产品。&nbsp;</p>\n  <p>看似元境迎来更好的技术迭代期，然而，理想很丰满，现实很骨感。&nbsp;</p>\n  <p>首先是市场教育成本远超预期。\"要让企业相信并采用元宇宙解决方案，比我们想象的要困难得多。\"行业人士表示。据了解，即便给出技术验证的免费方案，企业仍持观望态度。一份行业数据显示，2023年元宇宙的企业级客户数量仅达到预期的30%。&nbsp;</p>\n  <p>其次是生态建设遇到瓶颈。截至2024年初，元境平台上的开发者数量不足。有开发者反映，平台工具链不够完善，商业化渠道有限，难以支撑持续开发投入。&nbsp;</p>\n  <p>面对这些挑战，元境开始调整战略重心。知情人士透露，目前团队主要围绕两个方向：一是深耕旅游和工业元宇宙，与制造业头部企业合作，打造样板案例；二是将云游戏能力整合进阿里云，作为云计算的重要组件。&nbsp;</p>\n  <p>尽管元境的商业化进程不及预期，但在数字孪生、实时渲染等领域积累的技术资产仍然宝贵。&nbsp;</p>\n  <p>值得一提的是，元境的转型也折射出整个元宇宙行业的降温。尤其Meta在2021—2023年这三年里，元宇宙累计贡献了64亿美元的营收，但却带来了超过500亿美元的亏损，这已经让meta决心裁员及大规模削减元宇宙方面的投入。&nbsp;</p>\n  <p>当然，不可否认，在工业互联网、数字孪生等细分领域，市场需求仍在稳步增长。这或许给出了新的发展方向。&nbsp;</p>\n  <p>展望未来，元境的故事仍在继续。正如一位业内人士所说：\"在新赛道上，重要的不是谁跑得最快，而是谁能跑得最远。\"对阿里来说，如何在保持创新的同时实现商业可持续，将是一个需要持续探索的命题。&nbsp;</p>\n  <p>本文来自微信公众号<a href=\"https://mp.weixin.qq.com/s/2rB7O6ONNhtK3YAcl4fsYw\" rel=\"noopener noreferrer\" target=\"_blank\">“AI鲸选社”</a>，作者：杨晓鹤，36氪经授权发布。</p>",
        "published": "2024-10-31 09:17:35",
        "id": "dee6b8d4-2a29-4070-8d70-65703e8a2abc",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "2024年10月31日消息，阿里元境团队大裁员，其曾被定义为元宇宙操作系统，元境在市场教育成本、生态建设方面遇挑战致商业化不及预期。"
        },
        "tokens": 2305
    },
    {
        "title": "一个月生成1500万条广告，Meta继续加码AI",
        "link": "https://36kr.com/p/3016131273188612?f=rss",
        "description": "<p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_364f0fb96150457fa465cbac9bb929af@000000_oswg282143oswg1024oswg576_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">来源 | ad exchanger</p>\n  <p>Meta在第三季度赚了个盆满钵满，但投资者对其AI方面的支出感到担忧。</p>\n  <p>北京时间10月31日，Facebook母公司Meta发布了该公司截至9月30日的2024财年第三季度未经审计财报。报告显示，Meta第三季度营收为405.80亿美元，与去年同期的341.46亿美元相比增长19%；净利润为156.88亿美元，与去年同期的115.83亿美元相比增长35%；每股摊薄收益为6.03美元，与去年同期的4.39美元相比增长37%。</p>\n  <p>尽管第三季度业绩超出预期，但Meta在发布第三季度财报后，该公司股价在盘后交易中下跌超过15%，原因是有消息称公司今年和明年的支出将增加。Meta公司2024年的资本支出将在380亿至400亿美元之间，比之前的预期高出约10亿美元。首席财务官Susan Li表示，Meta预计2025年将继续保持 \"大量资本支出\"。</p>\n  <p>这些钱主要用在哪里？答案就是AI。毕竟，由AI驱动的Ray-Ban Meta智能眼镜和支持它们的服务器不是Meta自己开发和生产的。</p>\n  <p>首席执行官扎克伯格说：“我们的人工智能投资仍然需要大量的基础设施。我也将继续在这方面进行大量投资。” Meta AI是Meta面向消费者的ChatGPT。<strong> 据扎克伯格称，今年到目前为止，已经有超过100万广告主使用了Meta的人工智能生成工具，仅在过去的一个月里，这些工具就生成了超过1500万条广告。据Meta估计，使用其图片生成技术的企业的转化率提高了约 7%。</strong>因此，Meta正全力加码人工智能，以提高盈利效率和整体营销业绩。</p>\n  <p>具体到业务层面，Meta最近部署了新的学习和建模技术，使其广告系统能够捕捉并且联系到一个人在看到广告前后所采取的一系列行动。这个技术可以帮助广告主了解在单个用户会话中何时何地显示广告最合适。</p>\n  <p>Susan Li表示：\"以前，我们的广告系统只能将这些行动汇总在一起，而不能映射这些行动的顺序，这种新方法使我们的系统能够更好地预测受众对特定广告的反应。这也使我们能够在不增加广告数量的情况下推动收入和转化率的增长。自今年上半年实施新模型以来，Meta的广告客户在某些细分市场的转化率提高了2%至4%。</p>\n  <p>模型的另一个改善领域在用户体验，包括改进内容推荐。Susan Li说，\"因为我们发现，如果我们将模型规模和计算能力扩大到一定程度，性能就无法扩展。所以最近，Meta 开始尝试大型语言模型的扩展规律，即当模型在越来越庞大的数据库里接受训练时，其质量会随着时间的推移而发生变化。”</p>\n  <p>去年，Meta开发了新的模型架构，能够更有效地从更大的数据群中学习。现在，Meta正在探索新模型是否能对其他平台的推荐带来类似的改进。</p>\n  <p>更长远来看，Meta甚至计划将不同网页界面的数据结合起来，为其模型提供支持。\"我们将寻求在这些模型中引入跨界面数据，这样我们的系统就能从Meta的界面上了解某人感兴趣的内容，并利用这些内容给另一个网站推荐更合适的内容”Susan Li说，“这需要时间来执行。”</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzA3NDQ3NDEzMQ==&amp;mid=2653706146&amp;idx=2&amp;sn=78e093185b084f08a6dbb5d6691e1046&amp;chksm=85aa6e0d716e9c687713f1e6f2ef7a073a9004a9c56606881c59d753c7165e529f9cc2f44f8b&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“wj00816”（ID：Morketing）</a>，作者：Tiana，36氪经授权发布。</p>",
        "published": "2024-10-31 09:34:17",
        "id": "38054599-14d6-47eb-b7be-81eea333ed5c",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "Meta 2024财年第三季度营收和净利润增长，但财报发布后股价因支出增加下跌，钱主要用于AI，超百万广告主使用其AI生成工具，一个月生成1500万条广告，Meta在业务层面部署新技术提升广告和推荐效果且将探索更多改进并计划引入跨界面数据。"
        },
        "tokens": 1814
    },
    {
        "title": "全自动打工「人」，波士顿动力Atlas进厂视频火了，不断电不下班",
        "link": "https://36kr.com/p/3015930540434692?f=rss",
        "description": "<blockquote>\n   <p>波士顿动力Atlas进厂打工，不靠远程操控，转身动作像惊悚电影。</p>\n  </blockquote>\n  <p>波士顿动力的人形机器人，进厂了。</p>\n  <p>本周三，波士顿动力发来一条喜讯。其最新披露的视频展示了机器人在工厂环境中的任务完成能力。机器人现在已经可以全自动干活了，它可以在储物柜之间搬动汽车发动机零件：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_54165d264d7f4cffb24036aee42805e5@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>搬运的这个东西是汽车的发动机盖。视频里可见新版 Atlas 机器人是在寻找零件并挑选位置放置，还附带识别过程的展示：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_a2ce6a6141f34108a21303e2520116a8@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>搬着搬着 Atlas 看到有人拍摄，突然虎躯一震（其实是东西没放对位置）。没关系，我很稳：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_e800cd4415b34ae2839443ee2c342276@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>我们注意到，在拿到东西和放好东西后需要转身的瞬间，Atlas 并没有像人类一样转过来，而是以腰部为中心进行旋转，该动作最大限度地减少了移动，从而节省了过程中宝贵的几秒钟。不过，这也让人联想到一些惊悚电影里，主人公身子不动，头直接转过来的画面。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_44cbf8695a344e6392dc1941d26ccbeb@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>‍</p>\n  <p>一通操作看下来，机器人在工厂完成一些简单工作看起来是游刃有余了。</p>\n  <p>在社交网络上，网友们纷纷表示：太强了，看起来已经可以商业化了。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_e61cb8c0d8344d7e87956f1320919075@000000_oswg283202oswg1029oswg618_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>但众所周知，实现这样的愿景还有很长一段路要走。</p>\n  <p>波士顿动力指出，目前该公司的人形机器人已经能够通过视觉、受力和本体感受传感器的组合来检测环境变化（例如移动固定装置）和动作故障（例如无法插入盖子、绊倒、环境碰撞）并做出反应。</p>\n  <p>需要注意的是：波士顿动力这次强调了<strong>演示视频中的机器人是完全自主运行的，没有「预设程序或遥控动作」</strong>，它可以使用机器学习算法理解并适应真实世界的环境。这一声明似乎是在 cue 谁，但就是没有明说。</p>\n  <p>最近一段时间，哪家的人形机器��上过头条？应该是在特斯拉「We, Robot」活动中大放异彩的擎天柱 Optimus。马斯克还说，人形机器人的数量将在不到 20 年内超过人类，「这工作要由我来干」。</p>\n  <p>特斯拉的 Optimus 机器人展示了与观众互动，玩游戏、跳舞，甚至进行简单对话的能力，好不先进。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_722bcbd65a78416eb6b77c56f8718af7@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>然而，很多人指出，Optimus 在展示过程中实际上部分依赖于人类的远程操控，这引发了外界对于其自主能力的质疑。由于种种原因，特斯拉的股价当时还瞬间跌了 10%。</p>\n  <p>与 Figure、Tesla 和 Apptronik 等竞争对手一样，波士顿动力公司人形机器人的首次应用包括在汽车工厂的工作。考虑到该公司现在属于现代汽车公司，而现代汽车公司刚刚选择与丰田汽车的研究部门达成协议，关注 Atlas 这一应用是很有意义的。几十年来，汽车行业在自动化领域也一直遥遥领先。或许有一天，Atlas 真的会变身一名「汽车工人」。</p>\n  <h2><strong>波士顿动力也玩转型：Atlas 电动化之后</strong></h2>\n  <p>今年 4 月，波士顿动力曾跟全世界开了一个玩笑，先是官宣人形机器人 Atlas 退役，狠狠来了一波回忆杀。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_15c4eecea82749dd96b8da80a41191dd@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>紧接着，就在第二天，他们又放出了一个新的人形机器人视频。新机器人也叫 Atlas，不过由原来的液压改为了电动，身材更为小巧、灵活。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_0f61506a196645c1b36c89728d9cf676@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>‍此时，外界才反应过来，原来波士顿动力并不是要放弃人形机器人，而是转变了研发方向，让机器人更加适应工业环境。当时，该公司表示，这个电动版的 Atlas 将于明年初在韩国现代汽车工厂里开始进行试点测试，并会在几年后全面投产。看来，试点的时间可能提前了一些。</p>\n  <p>和之前的液压版 Atlas 一样，电动版的 Atlas 也是有一些绝活在身上的，比如随手就来一个俯卧撑：&nbsp;&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_49543cbc972d445e9afbd372a38c0e2d@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_c98661e30fb94edd97288ec8296f2118@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>做完俯卧撑后，Atlas 还能自己站起来。</p>\n  <p>倒立行走：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_ea0c4c9df70f4c70a0879cc10b8cac37@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>这些「绝活」是怎么做到的呢？前段时间，在机器人顶会 RSS 的一场技术分享中，MIT 博士、波士顿动力机器人工程师 Robin Deits 介绍了 Atlas 机器人过去几年的研发历程，以及从中学到的经验、教训。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_40dd2b3b9c2c42eabbe07d48a5001069@000000_oswg249648oswg1080oswg483_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>具体来说，Robin Deits 主要介绍了 Atlas 控制器的核心 ——MPC（模型预测控制）。他表示，波士顿动力从 2019 年以来实现的所有机器人动作都是依靠 MPC 来完成的，包括跑酷、体操、跳舞、后空翻等等。最近，他们还展示了 MPC 用于操纵物体的效果。2024 款纯电驱动的 Atlas 新版本也是由 MPC 驱动的。</p>\n  <p>对于这场技术分享，机器之心也做了详细报道，感兴趣的读者可以抽时间详细阅读（参见《波士顿动力技术揭秘：后空翻、俯卧撑与翻车，6 年经验、教训总结》）。</p>\n  <p>其实，除了内部研发，波士顿动力也在加强与外部的基础研究合作。就在两周前，波士顿动力和丰田研究院（TRI）官宣建立新的合作伙伴关系，以「利用 TRI 的大型行为模型和波士顿动力的 Atlas 机器人，加速通用人形机器人的开发」。</p>\n  <p>根据 IEEE Spectrum 的报道，TRI 的大型行为模型（LBM）其实类似于大型语言模型（LLM），只不过它的应用场景是在物理世界中工作的机器人。TRI 长期以来一直致力于开发基于人工智能的学习技术，以应对各种复杂的操作挑战。在与波士顿动力合作后，他们将通过 Atlas 获取更多物理世界的数据，这些数据将反过来用于支持高级 LBM 的训练。双方合作有一定的互补作用。</p>\n  <p>在基础研究层面，人工智能能否给 Atlas 带来新的突破，欢迎在评论区讨论。</p>\n  <p>参考内容：</p>\n  <p>https://techcrunch.com/2024/10/30/boston-dynamics-electric-atlas-humanoid-executes-autonomous-automotive-parts-picking/</p>\n  <p>https://x.com/BostonDynamics/status/1851624026424197434</p>\n  <p>https://spectrum.ieee.org/boston-dynamics-toyota-research</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650941110&amp;idx=1&amp;sn=4ad66cf26d54d7bb1e7af84a840fb1ca&amp;chksm=8531d80ed8ccef758b8b3abd5defb896f94619c0aab519f65609852a94bd46c4783e5a07226d&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“机器之心”（ID：almosthuman2014）</a>，编辑：泽南、张倩，36氪经授权发布。</p>",
        "published": "2024-10-31 09:33:18",
        "id": "1f29c067-868c-4d0a-ac0c-0cfe5c3b367a",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "波士顿动力的人形机器人Atlas在工厂展示全自动任务完成能力，其电动版有多种技能，研发基于MPC，波士顿动力还与丰田研究院合作，这一情况与特斯拉Optimus机器人形成对比。"
        },
        "tokens": 3687
    },
    {
        "title": "o1驾驶无人机后空翻，OpenAI开发者日惊掉下巴，2分钟爆改代码写App",
        "link": "https://36kr.com/p/3016057832433154?f=rss",
        "description": "<p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_add2c919e5e74938899ca1f1b870e2ac@46958_oswg268658oswg1069oswg397_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p><strong>【导读】</strong>OpenAI伦敦开发者日上，首次曝出了o1五大核心能力，还有图像理解。o1两分钟构建应用驾驶无人机、电话订餐、讲解太阳系，现场演示让所有开发者沸腾。</p>\n  <p>完整版o1的解禁，离我们不远了！&nbsp;</p>\n  <p>就在刚刚举办的OpenAI伦敦开发者日上，开发者体验主管Romain Huet带着o1模型来秀场了。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_5b9596bee22e42c5a446cf70fa7cd6d1@46958_oswg968810oswg1080oswg1153_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>o1 mini联动Cursor在不到2分钟时间内，搭建了一个可以交互的应用，驾驶无人机表演后空翻。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_82d61c64f89d4ec8918595f3ea548e4d@46958_oswg580891oswg1080oswg599_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>现场数百名开发者， 掌声不断。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_c5cc19a551c34e9f95c1f18a369bf16a@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>还有利用RealTimeAPI，构建的实时语音AI智能体向人一样，电话卖家订购200个派。而且，o1还不忘了幽默风趣，对话情商非常高。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_92eb012964214568b254b78bc3be380e@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>不仅如此，有了o1构建的太阳系可视化介绍应用，想必未来的教学一定非常有趣。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_40b44a905dbb4e0f9da500f207de91c4@46958_oswg500511oswg1080oswg421_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>这还仅仅是预览版+mini版o1的功能，在演讲末，一张PPT展示了未来o1的五大能力：&nbsp;</p>\n  <blockquote>\n   <p>函数调用、开发者message、流式传输、结构化输出、图像理解。&nbsp;</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_50ac3090f69c40cd858dcc331fead049@46958_oswg1086470oswg1080oswg617_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <h2><strong>o1演示炸场，台下观众欢呼不断</strong></h2>\n  <h3><strong>写代码搭App，驾驶无人机后空翻</strong></h3>\n  <p>整场最让人震撼的是，用o1 mini+Cursor搭建应用驾驶无人机飞行。&nbsp;</p>\n  <p>Romain Huet告诉o1，我现在有一架无人机，还缺少一个用JavaScript编写的交互界面，但是我不会如何编程。&nbsp;</p>\n  <p>接下来，他要求o1去构建这个应用，并设定好所有的交互按钮和组件。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_7acbad884f034c86930c2e2b0c0687fd@46958_oswg552601oswg1080oswg588_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>并且，他向模型发送了一个样本视频，作为参考。&nbsp;</p>\n  <p>o1收到请求后，开始执行所有的任务。&nbsp;</p>\n  <p>在这过程中，为了确保应用程序搭建能够实时更新，Huet通过在UDP数据库上发送可能与用户-按钮交互相关信息，从而实现实时数据传输。&nbsp;</p>\n  <p>并且，这个操作也非常简单，只需要点击每个按钮，并发送更改评论，便可以在应用中得到更新。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_341a1e16450d4ecda8a2bf2edb990e9a@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>最后，我们就得到了这样的一个交互界面。&nbsp;</p>\n  <p>左边黑的的框框是无人机摄像头显示屏，右边就是各种交互的按钮了。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_3b3f9310b29646adb281819dc2e37f31@46958_oswg421016oswg1080oswg670_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>见证奇迹的时刻到了，Huet将无人机放置在地面上，打开终端，开始运行o1搭建的应用。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_b2112aa254f949caac205dc053cf964b@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>只见o1驾驶的无人机演讲台上飞起，与台下的观众来了一张大合影。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_af4850134f574dbe8f7bd92450cb1fac@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>更惊喜的是，无人机现场还来一个360度的运镜。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_46f1a1356aa34227b91b2f8352b9acf6@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>整个应用构建，用了不到2分钟的时间。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_263aadaf458d477694ad1da898cf08af@46958_oswg83266oswg1080oswg145_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>无人机demo完整视频，如下：&nbsp;</p>\n  <h3><strong>AI实时语音订购派，堪比真人</strong></h3>\n  <p>另外，Huet还秀了一波用RealTimeAPI构建实现实时语音的能力。这一功能实际上在上个月已经推出。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_16f4249f137044dd850e3eca225a86f0@46958_oswg785050oswg1072oswg742_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>他表示，现在能够实现更长时间、更稳定的对话。&nbsp;</p>\n  <p>旅行应用程序Wanderlust中，Huet假设自己正计划伦敦和新加坡之旅，假设下周就要去新加坡。&nbsp;</p>\n  <p>他问道，你能给我提供一些游览的景点吗？&nbsp;</p>\n  <p>随后，在屏幕右边可视化图中，展示除了新加坡一些著名打卡点。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_fca8b4b0eb914805b0fe59c82d712442@46958_oswg527644oswg1080oswg552_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>紧接着，Huet又让他为自己推荐酒店，以及更多细节。&nbsp;</p>\n  <p>实时语音一边说，一边给出了结果。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_3335f9b4c6b847a28835a9c2803a3248@46958_oswg458225oswg1080oswg557_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>接下里，他又让o1在伦敦chiswell街区的当地商店订购一份pie。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_05abe3aa068246c78c5f216edf38722d@46958_oswg553927oswg1080oswg838_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">OpenAI开发者论坛负责人Spencer Bentley分享&nbsp;</p>\n  <p>Huet：我们台下有数百位开发者，他们可能喜欢吃一些甜点，你能帮我看看附近这儿可能有哪些商店？&nbsp;</p>\n  <p>o1：这是一些关于pie商店更多的细节。&nbsp;</p>\n  <p>不过，o1给出的结果中，第二个并非是真实存在的商店，只有其余两个是。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_19219b7423b04da1a86b5f8dd1bfb66b@46958_oswg509647oswg1080oswg596_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>为了方便演示，让现场工作人员担任卖家，Huet邀请同伴上台，一起来完成这个任务。&nbsp;</p>\n  <p>「在预算允许的情况下，帮我们订购200个pie，可以是肉和蔬菜的混合馅料」。&nbsp;</p>\n  <p>o1直接给IIan's Poah Pies打去了电话，并像人一样主动订购。&nbsp;</p>\n  <h3><strong>介绍太阳系，让教学更有趣</strong></h3>\n  <p>另外一个用RealTimeAPI构建太阳系导航应用程序，利用o1实时语音能力介绍星系。&nbsp;</p>\n  <p>从太阳系中最大的木星，到地球，再到火星深入介绍，o1全部都能娓娓道来。&nbsp;</p>\n  <p>OpenAI研究员表示，这一功能教会了自己：在教女儿新知识时如何成为有趣的父母。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_71411c7da30d457d8112b773a0c069d3@46958_oswg113442oswg1080oswg242_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <h2><strong>奥特曼QA环节，自曝最敬佩Cursor</strong></h2>\n  <p>没有Sam Altamn的开发者日，就不算是完整的。在整场演讲结束后，最后一个环节，就是Altamn QA问答了。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_40e87a41c1b94b5ca81449653fb5f336@46958_oswg79530oswg1080oswg143_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>在线上，奥特曼抛出了一个深刻的思考：&nbsp;</p>\n  <p>人们总是倾向于用历史上的技术革命来类比当前的AI革命。&nbsp;</p>\n  <p>但这种类比方式本身是存在问题的。比如说，互联网革命就与现在的情况有很大的不同。&nbsp;</p>\n  <p>也许拿晶体管来做比较会更恰当。&nbsp;</p>\n  <p>晶体管是物理学领域的重大发现，它具有惊人的规模化潜力，并且迅速在全球范围内得到应用和普及。&nbsp;</p>\n  <p>虽然晶体管技术让整个人类社会受益，但现在人们并不会把那些最早开发晶体管的公司仅仅定义为「晶体管公司」。&nbsp;</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_8bfded649ff447229ef81b4c6ce6a0c9@46958_oswg813538oswg1080oswg687_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">「我不祈求上帝站在我这边，而是祈求自己能够站在上帝这边。在开发这些人工智能模型的过程中，我确实感觉自己是在追随光明与正义的道路。」&nbsp;</p>\n  <p><strong>问：我们应该期待像o1这样的模型还是更大规模的模型？</strong></p>\n  <p>奥特曼：希望全面提升大语言模型的性能，但这个推理思路很重要。&nbsp;</p>\n  <p>「不方便透露太多细节...但我预计视觉模型领域会有突破性进展。」（这似乎暗示即将推出比GPT-4更强大的视觉模型）&nbsp;</p>\n  <p><strong>问：在技术整合方面会达到什么程度？基于OpenAI构建产品的AI创业公司应该如何规划？</strong></p>\n  <p>奥特曼：建议创始人应该打造这样的公司——既能充分利用当前大语言模型的优势，又能在未来模型升级时获得更大发展空间。&nbsp;</p>\n  <p><strong>问：开源的定位是什么？</strong></p>\n  <p>奥特曼：答案表明开源确实有其存在空间，但同时也需要很好地整合专有模型。不过，这个回答似乎没有提供太多实质性内容。&nbsp;</p>\n  <p><strong>问：什么是AI智能体（Agent）？</strong></p>\n  <p>奥特曼：「一个可以接受长期任务并且在执行过程中只需少量监督的系统。」我认为Harrison Chase在Langchain的博客中给出的定义更加严谨，但从商业角度来看，这个定义很实用。&nbsp;</p>\n  <p><strong>问：AI智能体能做什么？</strong></p>\n  <p>奥特曼：它们能够完成人类因能力限制而无法完成的任务，比如同时与300家餐厅进行通话，让AI智能体在每家餐厅进行交谈并即时收集信息。&nbsp;</p>\n  <p>或者说，它像一位极其智慧的高级同事，你可以放心地交给他两天或一周的工作任务。&nbsp;</p>\n  <p>说实话，我很讨厌「agentic」这个词。不让我们一起边讨论边思考，然后创造一个新词吧！&nbsp;</p>\n  <p><strong>问：在过去10年里，他的领导方式发生了哪些变化？</strong></p>\n  <p>奥特曼：公司发展速度惊人，仅用两年时间就实现了数十亿美元的营收规模。从追求10%的提升转向追求10倍的突破，这需要进行大量的调整和改变。&nbsp;</p>\n  <p><strong>问：对于Peter Thiel「要招聘30岁以下的员工」的建议，你怎么看？</strong></p>\n  <p>奥特曼：我创立OpenAI时就已经过了30岁。团队需要不同年龄层的人才，真正重要的是要始终保持极高的人才标准。&nbsp;</p>\n  <p><strong>问：你最担忧的是什么？</strong></p>\n  <p>奥特曼：从整个行业角度来看，我们正在尝试解决的问题的系统性复杂度。&nbsp;</p>\n  <p><strong>问：如果现在要创建新公司，他会选择什么方向？</strong></p>\n  <p>奥特曼：专注于某个特定领域，比如开发AI法律顾问或AI工程师助手。&nbsp;</p>\n  <p><strong>问：你觉得有什么重要信息需要让更多人知道？</strong></p>\n  <p>奥特曼：一个能够全面了解并陪伴你生活的智能助手。&nbsp;</p>\n  <p><strong>问：除了OpenAI的团队，你最敬佩谁？</strong></p>\n  <p>奥特曼：Cursor团队——他们打造了一个极具突破性的AI应用体验。&nbsp;</p>\n  <p><strong>问：如果能够实现理想中的未来，你觉得会是什么样子？</strong></p>\n  <p>奥特曼：在接下来的5年里，我们可能会看到AI技术以难以想象的速度进步。但有趣的是，社会表面的变化可能并不会那么剧烈——真正的影响可能要在更长远的未来才会完全显现。&nbsp;</p>\n  <p>参考资料：&nbsp;</p>\n  <p>https://x.com/tarekayed00/status/1851570058285232392</p>\n  <p>https://x.com/morqon/status/1851580985562779890</p>\n  <p>https://x.com/caromcc_/status/1851570587287601237</p>\n  <p>https://x.com/Foxalabs/status/1851574681112879535</p>\n  <p class=\"editor-note\">本文来自微信公众号<a href=\"https://mp.weixin.qq.com/s/av0SAV0oKCDpeEWvichRuQ\" rel=\"noopener noreferrer\" target=\"_blank\">“新智元”</a>，编辑：桃子 好困，36氪经授权发布。</p>",
        "published": "2024-10-31 09:32:28",
        "id": "726b4c7b-62e1-4444-99c9-c3db69e088d2",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 5
            },
            "keyFacts": "OpenAI伦敦开发者日展示o1模型，包括o1 mini联动Cursor不到2分钟搭建驾驶无人机应用、利用RealTimeAPI构建实时语音AI智能体订购派、构建太阳系可视化介绍应用，还展示了o1未来的五大能力，最后奥特曼参与QA环节回答多个问题。"
        },
        "tokens": 6049
    },
    {
        "title": "飞书上浮，钉钉下沉",
        "link": "https://36kr.com/p/3016146270905603?f=rss",
        "description": "<p>中国的to B软件发展缓慢，很大一个原因是中国企业分布是头部很大，尾部很多，但腰部企业很少。</p>\n  <p>王慧文在多年前对企服市场的判断有如一道魔咒，要求大多数企服企业必须上浮或是下沉，才能定位到足量的客群与市场，从而有针对性地开发产品。</p>\n  <p>仅协同办公三巨头来看，除了背靠微信私域护城河的企微外，钉钉与飞书的路径均在相似中走向了分化。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_226996181f33493393cdade60d1f0463@000000_oswg137865oswg1080oswg602_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>相似的是，两者均在死磕AI的路上，将低代码作为底层重要的落地载体。钉钉布道低代码多年，本就是钉钉开放生态中的底层设计；反观飞书，其明星产品多维表格本身就是低代码的一种落地形式，而亮相于今年飞书大会的多维表格数据库与低代码平台则可被视作是AI+低代码的进一步落地实践。</p>\n  <p>不同的是，一贯奉行开放生态的钉钉兜售能力，或主动或被动地“选择”下沉，在普惠的道路上一骑绝尘；而从未放弃all in one的飞书兜售工具，不可避免地上浮，试图以极致工具提效来做高客单价。</p>\n  <p>这一点，我们自两者不同的定价模型中可窥一二：钉钉按组织收费，而飞书则是按人头计算。以基础付费为例，200人的组织便是分水岭。</p>\n  <p>愈发明晰的路径分野，指向了钉钉与飞书各自的“舒适区”。</p>\n  <h2><strong>PaaS让步aPaaS？</strong></h2>\n  <p>大厂做to B都是有基因的，马云那句“让天下没有难做的生意”，催生了堪称最成功的“PaaS生态”淘宝，这也一定程度上推动钉钉成为时下用户最多的协同办公产品。</p>\n  <p>海量的用户分属不同群体，对应着的是极为复杂的落地场景，这意味着打造一个满足所有人的通用产品基本不可能。另一方面，作为协同办公龙头，钉钉是为这个市场做出完整用户教育的先行者，在投资人眼中，它同样承载着这个市场想象空间的上限。</p>\n  <p>“如果中国企业更愿意为办公软件付钱的话，必然发生的变化是钉钉一定更赚钱。”按照这个逻辑，规模化成为钉钉故事里的主线，底座定位与开放生态亦由此衍生。与其自己越做越重，不如开放生态让合作伙伴一起做大做强。</p>\n  <p>钉钉的低代码也是该逻辑下的产物：既然无法满足企业组织的精细化需求，那就打造一个通用的低代码平台，有什么业务需求，需要什么业务逻辑，“让他们自己搞”。</p>\n  <p>在这个“剧本”中，钉钉只需要不断扩大生态做一个流量入口，安心等待国内企业付费意愿增长。不过在这一过程中，钉钉与合作伙伴逐渐从合作走向了“竞合”。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_befd20f59b6e4295a03bfd0419b59a28@000000_oswg188288oswg660oswg330_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>在钉钉这个基座之上，低代码平台（aPaaS）“宜搭”是第二层，而“酷应用”（PaaS生态应用）则是第三层。当宜搭在开发侧的门槛足够低，且企业自主搭建的内部应用足够满足基础数字化需求之后，可能会对应用市场的生态造成挤压。</p>\n  <p>通常来说，钉钉PaaS生态内的合作伙伴如北极星与e签宝等，在钉钉应用分发平台上发布的产品一般是入门级别的标准款。这些应用大多年费一万元上下，即使只面向OKR、CRM等套件级别的类目或某条垂直赛道，其精细化程度依旧有限，甚至可以说是吸引企业主进一步按项目付费的“试用版”。</p>\n  <p>而企业自主通过宜搭搭建的内部应用可以起到“平替”的作用。由于自主搭建这一过程与定制化无二，低代码应用甚至会相对更适配企业本身。随着低代码的AI化，无论是自开发侧将AI能力更顺畅地输出自台前，或是通过AI识别逻辑等面向无经验开发者的功能，都进一步放大了低代码可触达的空间。</p>\n  <p>另一方面，低代码本身不过是购买钉钉专属版的“附加产品”，9800元的年费几乎就是企业唯一的软件支出。</p>\n  <p>自ROI的角度来看，使用宜搭自建应用相对更有“性价比”。</p>\n  <p>不受限地使用低代码以及随之而来的数据存储、流转等能力，是钉钉撬动渴望数字化转型的海量中小企业的重要杠杆。但这也带出了一个新问题：低代码与生态之间的关系该如何处理？</p>\n  <p>目前来看，钉钉的解法是与生态伙伴共同开发开箱即用的原子化套件，为生态伙伴增加创收渠道和露出机会。</p>\n  <p>比较典型的是钉钉协同套件，作为生态底座自己的开箱即用工具，据官方今年6月在生态大会上放出的消息，包括北极星、e签宝、Moka在内的生态伙伴在钉钉套件的合作收入已接近1亿元。</p>\n  <p>另一方面，钉钉也在生态的基础上不断巩固开���性。以电商行业为例，国内头部电商平台均向钉钉开放了数据接口，其便可以作为数据汇总与处理的中台。而不同电商生态的企服工具也能自其中找到变现空间。</p>\n  <p>PaaS与开放生态并非易事，因为生态伙伴的渠道权重会随着平台商业化的脚步而不断变化。公约数越做越大的同时，钉钉也需要思考该怎么把好这根平衡木。</p>\n  <h2><strong>认清现实</strong></h2>\n  <p>大厂的to B基因论放到飞书上同样适用，不过出于赛道的特殊性，飞书在继承了部分字节的基因的同时，还在极力对抗其中的某些东西。</p>\n  <p>字节的核心是效率，从头条、内涵段子到抖音、西瓜视频和番茄小说，本质上都是信息流的变体。而这“吸引人眼球”的生意，都遵循相似的商业模式——提升获客+留存（时长+频次）的效率，便足以一定程度上构建信息的入口效应，掌握分发权。</p>\n  <p>至此，字节商业化需要做的只有尽可能提高单位时间内的变现效率。无论是商业化的前置链路还是本身，极致效率都是其追求的至高目标。这一点延伸到飞书上，具体呈现为我们所看到的多维表格、People、仪表盘等产品，但是to B却是南辕北辙的一套逻辑。</p>\n  <p>流量与算法的不适用，以及老生常谈的以to C思维做to B都是飞书需要对抗的。此外，更大的问题或许在于其在产品上的过度投入，与产品变现效率的倒挂。</p>\n  <p>于外部来看，中小企业用人成本偏低，而边际亦不明显，这意味着飞书在其之上的变现效率并不高。为了匹配飞书all in one式产品的开发维护强度，提高客单价成为飞书最后不得不走的“独木桥”。</p>\n  <p>试问一下，按飞书最新发布的产品来看，什么规格的企业用得上单表容量突破100万行的多维表格，或是可以统计1000万行数据的仪表盘？</p>\n  <p>更进一步说，在飞书的产品矩阵中，原子化的套件工具是其输出效率的核心。在AI的重构下，前述其亮眼的产品表现也是集成了包括月之暗面、智谱AI等明星创企的能力。例如新多维表格的“字段捷径”功能，便是通过封装AI来批量操作表格中的结构化数据。</p>\n  <p>当集成的工具已经达到一定整合程度，其他内部应用就愈发显得多余——客户需要按飞书的逻辑开展业务，而低代码平台不过是开放给少数客户“打补丁”的工具。反观钉钉则视低代码为抓手和底座，更大的权限留在客户手上，AI相对更偏向于“打辅助”的定位。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_6029491573a0499fa28e24020142b52a@000000_oswg407229oswg719oswg551_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>如此一来，变现效率低的中小企业组织很“自然”地被飞书所抛弃。我们了解到，东南地区一家约200人的销售组织，其每年在飞书上的开销就达到了20万。而另一家千人规模的SaaS企业，飞书年费高达200万，占据公司现金流的五分之一。</p>\n  <p>按舶来的概念做定义的话，飞书的增长逻辑被迫从相对纯粹的PLG（产品驱动）演变为PLG+SLG（销售驱动）。不过，正如此前提到的基因冲突，飞书既无法在内部负担庞杂的销售组织，也未能打造成熟的层级代理体系。</p>\n  <p>一位曾与飞书有过接触的集成商告诉光子星球，飞书的代理体系一度不够清晰，还出现过内部直销和代理“抢食”的情况：“基本上直销接触过的客户，我们很难拿到返点”。</p>\n  <p>此外，飞书对集成商的组织亦有要求，除少数一线城市，难有集成商能满足飞书需求的售前组织规模。</p>\n  <p>幸运的是，作为字节落地自身理念的“工具”，飞书的增长还有字节背书的助力。一位接近飞书人士表示，飞书囿于自身的定位，做KA的决心相比另外两家友商更大。另一方面，字节的高速增长也让KA客群产生了一定FOMO心态。这让飞书成为2020~2022年间啃下相当多KA，收入规模上一度成为三家之首。</p>\n  <p>“（高客单价下）客户不会轻易迁移，你稍微涨涨价他也能接受，这就保证了稳定的收入。”</p>\n  <p>在明晰自身定位以及战略性放弃中小组织后，飞书果断发起裁员，自负盈亏似乎也近在眼前。在KA战略下一路狂奔的飞书，守着很大但很少的头部，离腰尾部好像更远了。</p>\n  <h2><strong>找到新故事</strong></h2>\n  <p>上浮与下沉既是战略，也是钉钉与飞书在企服市场寻求增长的必然。但这并不意味着其完全放弃在优势领域之外的增长机会。</p>\n  <p>正如我们看到时下越来越臃肿的Super App，当一个产品做到规模化后，产品本身是无法收敛的。于是我们能看到飞书在积极开放生态，而钉钉也在持续推出原子化的套件。</p>\n  <p>不过这些动作的意义更多在于“覆盖”，随着AI潜移默化地改变我们生活的方方面面，由AI赋能的个人成为协同巨头新一轮财富分配的机会。</p>\n  <p>众所周知，早在AI浪潮前，钉钉文档的前身Wolai与飞书文档便早已是许多内容生产者与数字游民的心头好。这足以证明移动互联网时代下，个体与组织具备相似的对效率的追求。</p>\n  <p>随着协同巨头在OA基础上的扩张，这些原“属于”个体的效率应用也在市场作用下为其所鲸吞。这并不意味着个体对效率的需求就此消弭，反倒是个体先于企业一步感受并接纳了AI带来的生产力革命。</p>\n  <p>微软和 LinkedIn 此前发布的 2024 年工作趋势指数年度报告指出，有 78% 的精通 AI 的员工，在未经雇主许可或是在雇主不知情的情况下使用了AI。个体的敏捷性决定了其必然会先于组织拥抱技术革命，甚至产品在海量个体的使用中，其边界还能得到更有有效的拓展。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_cb003aec4d014e20be2437ffbf0b54f8@000000_oswg86180oswg1080oswg593_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>学者吴晨在一次小规模分享中也提到，AI时代不一定诞生超级巨头，但一定会催生超级个体。正如开源社区对AI发展的推动一般，个体相对企业也是更敏捷的反馈体系建立者。</p>\n  <p>甚至在商业模式上，我们也能找到路径相似的先行者。据WPS的母公司金山办公2023年报，其2023年收入45亿，其中订阅收入36亿，净利润13亿。</p>\n  <p>做个体，或者说to P（professional ）的生意，飞书的产品本身便存在一定优势。不过自布局上看，钉钉却是两家中的先行者。据悉，钉钉最新推出的钉钉365会员便是针对这个市场的订阅制付费产品，集成了包括AI助理、AI搜索以及包括自动回复在内的更为个性化的设置。</p>\n  <p>此外 ，协同办公的新变化，还很可能发生在出海与国内企业新旧交替的这一轮重新分配上。数月前钉钉出海，以及其与飞书围绕具身智能、AI创企等新兴市场的客户争夺，便是产品之外的拉锯战。</p>\n  <p>随着钉钉与飞书这两家公司的战略与路径愈发明晰，僵持的格局也开始有了被打破的可能。这为更有可能打破僵局的AI，争取了“发育时间”。</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzA4MjUxODMwMg==&amp;mid=2649660082&amp;idx=1&amp;sn=a5dc2f07bd5a3e32cf964bced8d09908&amp;chksm=86fceb365114d0279fb914ed90d2abe8de2fb76eda7de3e46259c4fa79b12d8c119472ab308a&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“guangzi0088”（ID：TMTweb）</a>，作者：吴坤谚，36氪经授权发布。</p>",
        "published": "2024-10-31 09:22:31",
        "id": "cf4bdd2f-6bc2-4364-9929-2334fddea3d4",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "在中国to B软件发展受企业分布影响背景下，钉钉与飞书在相似中走向分化，钉钉奉行开放生态兜售能力下沉，飞书兜售工具上浮，两者均重视AI与低代码，且各自面临不同问题，同时两者也在尝试寻找新的增长机会。"
        },
        "tokens": 4329
    },
    {
        "title": "起底交个朋友的烂尾电商课：公司更名、团队曾解散",
        "link": "https://36kr.com/p/3016040414651907?f=rss",
        "description": "<p>想赚钱，先当了“韭菜”，电商出海到底有多难？</p>\n  <p>2022年以来，越来越多的电商平台及从业者将视野投向海外，这也让MCN机构有了做“卖铲人”生意的市场。</p>\n  <p>近期，时代财经收到多名电商学员独家爆料，称2023年8月曾花费近万元在“交个朋友海外电商学苑”报名“美区出海掘金”电商课，先后遭遇账号封禁、陪跑时间未达承诺、培训机构“消失”、课程烂尾等多重问题，部分学员的学费也伴随老师离职、公司变更等问题至今无法退回。</p>\n  <p>天眼查APP显示，2023年，上述学员缴纳学费的收款公司主体为北京交个朋友出海科技有限公司（下称“北京交个朋友”），现已更名为北京百瑞出海科技有限公司（下称“北京百瑞”）。杭州交个朋友教育科技有限公司（下称“杭州交个朋友”）为北京百瑞的企业法人，同时持股比例60%为大股东。</p>\n  <p>不少学员称，报名时是“奔着罗永浩和交个朋友名气来的”。时代财经注意到，交个朋友多项电商出海课程海报中出现了罗永浩的身影。那么，这门电商出海课烂尾背后，到底发生了什么？罗永浩与交个朋友控股（01450.HK）在其中扮演了什么角色？电商出海课程的商业模式是如何设计的？</p>\n  <p>围绕这些问题，时代财经展开深度调查，挖掘乱象背后的复杂关系。</p>\n  <h2><strong>上市公司实控人身影浮现</strong></h2>\n  <p>交个朋友控股是国内较早开启直播带货且转型成功的MCN机构。</p>\n  <p>据多家媒体报道，2022年11月，由罗永浩、黄贺等人联合创立的交个朋友正式宣布入局海外电商培训业务，其业务划分为海外事业部和海外电商学苑两个版块，前者主做红人营销、海外达人BD以及国内品牌方的达人流量赋能，后者则主做跨境电商培训。</p>\n  <p>那么，运营跨境电商培训的公司是否与罗永浩及交个朋友控股有关联？</p>\n  <p>事实上，早在2022年6月，罗永浩就宣布退出交个朋��管理层重新创业，但仍会以主播身份参与直播。</p>\n  <p>天眼查显示，交个朋友的早期实体公司为北京交个朋友数码科技有限公司，黄贺在2023年5月已退出该公司。2023年7月，北京交个朋友数码科技有限公司借壳世纪睿科登陆港股市场。据交个朋友控股发布的2024年Q3业绩公告，前三季度累计完成GMV约人民币88.6亿元，较2023年前三季度增长约18.44%。</p>\n  <p>时代财经通过天眼查股权穿透图及交个朋友控股的财报进一步发现，交个朋友控股与杭州交个朋友在股东上存在关联。交个朋友控股董事长李钧为杭州交个朋友股东，持股比例为21%，交个朋友控股执行董事李亮则担任杭州交个朋友董事长，持股比例为23%，且为杭州交个朋友世纪控制人。上述公司的股东名单中均未出现罗永浩的身影。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_5bb8916f3d314474bcdf1656bfcb1ad1@000000_oswg121144oswg960oswg310_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">图源：天眼查、企业财报</p>\n  <p>罗永浩虽然与上市公司无直接关联，但罗永浩多次在公开场合提到李钧，李钧和罗永浩以及交个朋友也长期保持密切关系。2020年的尚纬股份收购案，详细披露了交个朋友母公司星空野望的股权结构——其中，通过黄贺代持，李钧持股星空野望18.18%，罗永浩的亲兄弟罗永秀持股17.22%。</p>\n  <p>时代财经翻阅世纪睿科2022年年报、交个朋友控股2023年年报及2024年中报，上述售卖电商出海课程的杭州交个朋友、北京百瑞两家公司均未出现过，且非上市公司附属企业，出海相关业务在财报中也从未提及。</p>\n  <p>由此看来，杭州交个朋友的出海业务与上市公司交个朋友控股的业务有明显切割。</p>\n  <p>然而杭州交个朋友出海业务的运营并不顺利。</p>\n  <p>学员杜峰介绍，他在2023年报名交个朋友电商出海课程，先在某电商平台购买299元的基础课程，售课老师承诺90天陪跑、手把手教学、有卖货佣金等服务，而后将学员引流至私域再售卖客单价更高的课程，学费8800元起步。</p>\n  <p>为了顺利运营海外电商账号，学员还要以1千元1部旧iPhone的价格购买二手手机用于账号操作，并且需要额外购买海外账号。</p>\n  <p>“但课程开启约30天后，大家的美区账号先后出现限流、收款账号封禁、不能挂商品链接等问题。”杜峰表示。陪跑老师先是承诺更换东南亚账号、提供新货盘视频素材，随后逐渐离职消失，课程烂尾。</p>\n  <p>2023年9月，由于课程交付出现诸多问题，学员们对课程收费产生不满，陆续开始申请退费。当申请退费遭遇困难，学员才意识到价值万元的课程甚至没有签订合同、没有开具发票。出于对交个朋友品牌的信任，许多学员在老师催促付款时并未签订合同，开课后也没有老师提供发票等证明。</p>\n  <p>学员薇薇在黑猫投诉的记录显示，2023年9月13号，“交个朋友海外电商学苑”的官方帐号停止更新，账号内的海外课程也全部下架，随后注销无法搜索到。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_adda975948a54c08bef2367d28389a99@000000_oswg100303oswg960oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">原交个朋友海外电商学苑店铺及课程页面 图源：店铺课程记录</p>\n  <p>学员杜峰告诉时代财经，“由于联系不上陪跑老师，我曾打12315投诉要求退费，后来北京工商局反馈，公司注册办公地址已人去楼空，电话无法联系，找不到这家公司了。”</p>\n  <p>时隔一年，一名已离职的交个朋友电商出海课程陪跑老师严真向时代财经透露：“老师们离职是因为整个项目裁撤了，2023年9月15号，北京团队直接原地解散。”</p>\n  <p>关于项目裁撤原因，据严真介绍，当时“北京交个朋友”是由职业经理人负责打理，班主任是内部员工，但讲师是外聘的，整个项目的运营团队人数内外部加起来约50人，实际内部员工只有10~20人，月营收为数百万元。</p>\n  <p>“个人感觉职业经理人操之过急，团队有些盲目扩张了”，严真说道。</p>\n  <p>在严真看来，2023年国内市场对于出海培训的需求其实还不够。“对于培训而言，跨境出海、AI等概念在国内较为前沿。国内大部分普通人并不了解平台运营规则，往往只通过单一渠道获客，加上海外平台有风控因素，经常遇到封号等问题。这些因素都会对课程交付有影响。”</p>\n  <p>就课程过度承诺无法实现等问题，交个朋友电商学苑相关负责人向时代财经表示：“在针对电商学苑业务审查过程中，发现部分学员确实存在合同未签署、销售承诺陪跑90天等情况。北京百瑞所有业务在2023年9月份关停，主要原因正是该公司在运营过程中存在重大问题，经过多次整顿，依然没有达到效果，因此只能选择关停，从而避免对交个朋友品牌造成更大损失。”</p>\n  <h2><strong>项目重启，学费找谁要？</strong></h2>\n  <p>事实上，虽然团队原地解散，但课程并未完全消失。</p>\n  <p>时代财经注意到，上述课程烂尾问题尚未解决，类似的课程又已重新上架。</p>\n  <p>近日，在“交个朋友电商学苑”账号上，新上架的电商出海课程费用从299元到6.8万元不等，多项学费数万元的课程宣传海报中赫然放着罗永浩的大幅形象照，标注“名誉校长”，背景还写着“做直播，光宗耀祖”。此外，交个朋友矩阵直播间在售卖电商课程时也出现了罗永浩头像，下方写着“名誉校长，为你保障”。</p>\n  <p>时代财经就罗永浩与课程的关系咨询交个朋友电商学苑的客服，对方表示：“罗永浩是交个朋友电商学苑的名誉校长，只是挂名。”交个朋友电商学苑相关负责人也明确回应：“罗永浩本人并未参与课程研发及制作。”</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_67c00c305245475283828bc5ddc986e0@000000_oswg108156oswg960oswg540_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">交个朋友矩阵直播间罗永浩宣传图 图源：电商平台截图</p>\n  <p>时代财经随即在该账号购买了299元课程，缴费后显示收费公司主体为杭州交个朋友，即去年课程的收费公司主体北京百瑞的大股东。</p>\n  <p>一名知情人士透露，原“交个朋友海外电商学苑”的北京团队部分员工在项目解散后并未离开，而是转入海外事业部，在今年出海课程重启后，再次回到电商学苑项目负责运营，中间间隔周期约半年。这一点可以通过交个朋友各类账号的内容发布停滞及更新时间得到交叉验证。</p>\n  <p>一名交个朋友在职员工也向时代财经确认，电商出海课程仍由海外电商学苑团队负责。</p>\n  <p>换句话说，时隔约半年，电商出海课程项目重启。一名学员认为，“这简直就是换马甲。”</p>\n  <p>那么，退费有何标准？未退还的学费应该由哪家公司承担？带着学员疑问，时代财经通过前员工及交个朋友方面找到了更多答案。</p>\n  <p>据严真回忆，公司对于课程有退费标准。“陪跑过程中，老师如果离职，可退款处理。对于参与课程后提出异议的学员，我们把能够交付的交付完，这部分课程成本扣除，剩下的按比例退款返给学员。部分不能退费的，可能申请得太晚了。”</p>\n  <p>一名奔着罗永浩名气报课的学员不禁感叹：“花费了大量精力想‘掘金’，却先被‘割韭菜’了。”</p>\n  <p>9月23日，交个朋友电商学苑相关负责人回应时代财经称：“罗永浩已看到相关客诉，并一直敦促公司完成学员的退费和员工的离职补偿工作。公司积极承担责任，妥善解决课程退费问题。”</p>\n  <p>对于学员质疑北京交个朋友更名是“换马甲”行为，上述负责人回应，关停原因是集团整顿业务，绝不是为了“换马甲”躲避退费。将该公司的业务回收到杭州总部进行，更有利于总部直接监督管理，同时可以更好的利用杭州电商出海生态。</p>\n  <p>此外，关于课程交付与退费问题，上述负责人也向时代财经表示，公司有明确交付规定和退费规定。公司业务停止之前，所有业务相关工作人员主动联系解决未行课学员进行全额退费，已上课学员在此后的时间段，按照实际的上课时长进行比例退费，课程结束之日起，不再产生退费。</p>\n  <p>目前，交个朋友方面已提供退费方案，学员正陆续填写退费信息。</p>\n  <p>据时代财经拿到的退费方案显示，学员付款类型分为纯课程或课程+手机两种形式，纯课程按照课程报名金额的75%进行退款，课程+手机形式需退还手机，并按照课程报名金额的75%以及手机单独购买的全款金额进行退款。若手机选择自留或损坏无法正常归还，可按照课程报名金额的75%比例退款。</p>\n  <h2><strong>“洗人”的层层设计</strong></h2>\n  <p>退费难背后，学员更疑惑的是一家知名MCN公司的电商出海课为何会烂尾？电商出海课到底有哪些套路？</p>\n  <p>对于学员而言，尽管“交个朋友海外电商学苑”课程昂贵，但课程内容设计得十分丰富。</p>\n  <p>据董成、薇薇等学员提供的课程录播、陪跑老师沟通截图、账号封禁记录等资料显示，9800元进阶课的服务包含两个TikTok美区5000粉丝基础账号、交个朋友选品的视频切片素材（商品为挖耳勺），还包括二手苹果手机、4节直播课及视频剪辑、买品、卖品等运营指导，并承诺陪跑90天，带货佣金可到手20%以上，每个月15号、30号都可以申请回款。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_6a1dc6630dbe4dc28f0701d3a9849bc3@000000_oswg493985oswg960oswg623_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">微信社群开课通知及学员学习打卡记录 图源：学员提供&nbsp;</p>\n  <p>杜峰告诉时代财经：“老师们一开始介绍了很多爆款销量的成功案例，比如莫桑比钻、指尖螺旋玩具等，似乎只要视频挂着就有人买，海外市场好赚钱。”</p>\n  <p>操作似乎也很简单，陪跑老师负责提供海外账号并根据商品提供视频剪辑素材，要求学员自己剪辑视频、去水印、查重、发布，保持账号活跃度。在学员看来，只要听老师的指导按时剪辑视频，发布后挂上商品链接，就有机会卖货。</p>\n  <p>“每天作业只要发3-6条视频，老师会检查。”薇薇说道。在便利的赚钱模式下，学员们心动了。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1a79688b54fb4ec48b06835d9bb05ca5@000000_oswg88460oswg960oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">课程中提供的选品及售卖案例截图 图源：学员提供</p>\n  <p>但后来杜峰等人才意识到，“想赚钱，每一步都在筛选人”。“视频素材重复、商品链接挂不上、没有流量、账号反复封禁、机械剪视频的毅力都是关卡，我们更像是在给交个朋友打工。”杜峰说道。</p>\n  <p>学员薇薇回忆，此前交个朋友的课程团队组成相对复杂，价值近万元的直播课程主要内容仅为4节大课，课程讲师均聘请外部人员。</p>\n  <p>但关于电商出海课程的运营套路，学员们还有许多不知道的事。另一头，吸引学员购买课程的销售过程被称为“洗人”。</p>\n  <p>对于电商课程的商业运转模式，一名交个朋友的已离职员工透露：“整个流程简单来说分为流量端和交互端，流量端首先需要一个IP讲师带流量，通过视频、直播等形式，加投放获取流量。以99元到299元不等的小直播课吸引学员，随后配置校长团队，校长团队也可称为前端部门，主要由IP讲师和课程销售组成，他们会从基础课程学员里‘洗出来’进阶课学员，购买1~2万元课程，交付完后会配有讲师、辅导员、陪跑老师，后续还可能再转其他课程，费用可能达到5~10万元。”</p>\n  <p>说服学员的销售话术也有专门的设计，上述交个朋友前员工说道：“销售的转化话术的SOP由北京团队的销售负责人制定，合同也是前端销售负责的。我了解到的情况是有一部分学员签署合同，但是也有部分没有签订。”</p>\n  <p>时代财经注意到，在各大内容电商平台，售卖交个朋友电商出海课程的账号不下100个，前缀均为“交个朋友”，其中部分为蓝V认证，部分未认证，令人眼花缭乱真伪难辨。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1b545f88bdee4656b4543385239e29d7@000000_oswg119946oswg960oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"img-desc\">交个朋友抖音、视频号、小红书的矩阵号（部分） 图源：平台截图</p>\n  <p>一名知情人士告诉时代财经：“基本都是交个朋友的矩阵号，没认证的账号是因为抖音账号实名有数量限制，要获取流量必须要矩阵开播。”</p>\n  <p>开设多个账号也是销售模式的一环。上述前员工介绍，海外培训其实是复制已有模式，比如先买一个千粉账号开播，开播后以达人身份挂课程进行售卖。“整个矩阵都在滚动播放或者一起播放，就是为了流量和推广效果最大化。不同账号权重不一样，流量效果就不一样。”</p>\n  <p>严真给时代财经算了一笔账，交个朋友整套出海课程成本包含销售、讲师、陪跑老师的成本，以及提供给学员的账号、素材等课程设计成本。其中，讲师线下课费用可能在3万元左右/次，每次1~2小时；线上课则是采用项目分成制，而不是按照课时费结算。陪跑老师的工资为底薪+提成，底薪是10k左右，提成是8%。</p>\n  <p>“去掉各类成本，其实不太赚钱。”严真表示。</p>\n  <h2><strong>线上课程售后如何规范？</strong></h2>\n  <p>在抖音之外，近期交个朋友的电商出海课还入驻了小红书、微信视频号等平台，据时代财经观察，各平台都出现了用户在账号下方追讨学费的情况。</p>\n  <p>关于众多账号的真伪，交个朋友电商学苑相关负责人回应时提到，不能以是否为“蓝v”账号判断归属，应按照账号的企业主体是否关联为交个朋友旗下公司进行判定。当前市面确实存在部分伪装账号，需注意辨别。</p>\n  <p>尽管电商出海课程售卖售后尚无明确规范，但各个平台已出台了相关规则。</p>\n  <p>微信的视频号现行《教育培训类目准入标准》中提到，经营“互联网产品与运营、电子商务”线上课程的商家，除了需要遵守平台基础的运营规范和缴存相应的保证金，还需提供《增值电信业务经营许可证》，核准业务种类含“信息服务业务（仅限互联网信息服务）”或《ICP备案》。</p>\n  <p>百联咨询创始人庄帅也对时代财经表示。“电商课程培训主要还是属于教育培训范畴，应该参照教育培训的行业规范。对于电商出海从业者，平台运营经验才是最重要的，包含海外电商平台的营销规则、营销方式、运营流程、商品跨境检疫、仓储物流等流程。”</p>\n  <p>时代财经向交个朋友电商学苑已入驻的平台抖音、微信视频号、小红书发去采访函，截至发稿未获回复。</p>\n  <p>（时代周报记者叶曼至对本文亦有贡献；应受访者要求，文中杜峰、薇薇、严真、董成均为化名）</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzI0NzQ4OTIyMQ==&amp;mid=2247532690&amp;idx=2&amp;sn=1d13027eea86c53b0d099b59a9192311&amp;chksm=e800c93490eb6e2a58e899abd174cc67b6bbcb5beef2c9ccc27c08b94a96a1915f19585acfa1&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“时代财经APP”（ID：tf-app）</a>，作者：何珊珊，36氪经授权发布。</p>",
        "published": "2024-10-31 09:01:21",
        "id": "0ba88a6a-7557-4f00-891b-17094d5d4e32",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "交个朋友的电商出海课程存在烂尾问题，包括账号封禁、陪跑未达承诺、培训机构消失等，背后涉及公司更名、团队解散，课程存在套路，现课程重启且提供了退费方案，各平台也面临电商出海课程售后规范的问题。"
        },
        "tokens": 5939
    },
    {
        "title": "AI 原生时代，字节想要复刻第三次增长奇迹",
        "link": "https://36kr.com/p/3015897245181445?f=rss",
        "description": "<p>2024年被业内称为「AI应用元年」，字节跳动布局AI的节奏明显加快了。&nbsp;</p>\n  <p>过去一年，字节推出十余款AI应用，覆盖Agent 定制、聊天、社交、图像/视频、办公、教育等多个领域，甚至还涉足了硬件产品。</p>\n  <p>移动互联网时代，字节就有「APP工厂」之称，所以相继打造今日头条和抖音两个爆款产品，借此成为最赚钱的中国互联网公司之一。</p>\n  <p>大模型时代，他们还要创造新的增长奇迹，寻找第三个超级应用的可能性。</p>\n  <p>这不是字节的专利。置身于AIGC浪潮之年，发力大模型的公司都是这样设想、计划且实施的。</p>\n  <p>在今年5月的一场行业大会上，小冰公司CEO李笛提到，「当前 AI 的商业价值已从单纯的技术输出转变为通过高附加值产品实现收益。」</p>\n  <p>更早些时候，朱啸虎在接受腾讯新闻的采访时也很现实地谈到，对于AGI（通用人工智能），他信仰能马上商业化的应用。但对于大模型到底在什么点让to C应用爆发，他也不知道。他只是说，「to C应用必须刚需、高频，长远才有机会守得住。」</p>\n  <p>如同争夺移动互联网的船票一样，现在大家瞄准的是AI时代的船票。字节当然也知道，现在是押宝的阶段，所以探索型布局显得尤为重要。</p>\n  <h2><strong>字节需要AI时代的抖音&nbsp;</strong></h2>\n  <p>过去一年，Chatbot、AI虚拟社交、AI Agent是AI圈热点讨论的三大产品领域。字节无一遗漏。</p>\n  <p>除了对标ChatGPT、Kimi的智能对话产品豆包；定位为「AI虚拟角色聊天互动社交」星野的「猫箱」（原「话炉」）；还有AI Bot开发平台扣子（Coze），对标OpenAI推出的‌GPT Store。</p>\n  <p>这是由多支AI团队内部赛马的结果。短时间快速推出产品，再经由市场检验，优胜劣汰，这是字节一贯的作风。移动互联网时代，字节就有「App 工厂」之称。《晚点 LatePost》曾统计，2018-2020 年，字节跳动自研/收购了大量项目，在App Store上线的应用约140 个。</p>\n  <p>其中不论在字节体系内，还是AI行业，豆包都以用户规模第一的姿态处于探索前锋。</p>\n  <p>而且豆包承载着字节在AI时代的雄心——即打造类似今日头条、抖音这样的爆款、国民级应用。</p>\n  <p>简而言之，豆包是一款通用型产品，既是生产力变革工具，也有情感陪护功能。甚至应用与大模型同名，都是豆包。</p>\n  <p>所以，基于豆包大模型（原云雀大模型）开发的 AI bot，豆包可以实现AI智能聊天、对话、问答工具，预制了多个智能体，支持学习、生活、情感等多个场景，同时也允许用户定制自己的AI智能体。</p>\n  <p>QuestMobile数据显示，截止到今年7月份，AI原生应用（也即APP）月活用户规模已经突破了6630万，其中，豆包以3042万的月活用户规模位居榜一，比百度的文小言多出2000多万的月活用户规模。</p>\n  <p>这是字节信奉大力出奇迹的结果。在抖音，看不到什么AI应用的投放广告，除了豆包。据说，字节几乎屏蔽了同类型产品在抖音的所有投放，将海量用户转化的机会都留给了自家豆包。</p>\n  <p>除豆包、星野等AI原生应用外，字节根据自身业务特性，甚至战略需要，基于现有产品孵化中一些新的应用，最典型的就是即梦。</p>\n  <p>作为全球短视频最大平台，字节CEO梁汝波曾在内部发起警觉，「AI技术对于内容创作会有很大的颠覆，甚至会产生新的创作平台」。尤其Sora横空出世之后，文生视频的风浪可能会进一步冲击抖音现有的行业地位。</p>\n  <p>假如，中国版Sora不是出自字节系，而是其他创作平台呢？并且这样的事情一而再再而三的发生呢？后果是，抖音就会失去视频创作入口。久而久之，更是可能失去发布入口。</p>\n  <p>这是张楠被调任剪映的一个重要背景，即梦正是她掌管的剪映业务团队推出的一个AI视频生成应用。</p>\n  <p>但是从反馈来看，这个新产品距离被用户广泛接受，还有很长距离要走。截至今年10月15日，七麦数据显示，即梦AI的累计下载量为79.14万。此外，这款产品近30天日均下载量为22978次。</p>\n  <p>从时间表来看，即梦的雏形Dreamina比快手可灵推出时间要更早，却被后者赚足风头。一个未经证实的消息是，可灵的营收已经过千万，内部还切蛋糕小小庆祝了一下。大模型时代，视频行业格局发生变数的可能性增加了。</p>\n  <p>左手豆包，右手剪映，这是字节在大模型时代迅速打出的两张王牌。一个是纯粹的AI原生应用，一个是基于现有核心业务的进化迭代，本质都是在争取AI时代的一张船票。</p>\n  <h2><strong>APP工厂依然在继续&nbsp;</strong></h2>\n  <p>豆包和即梦只是字节押注AI应用的两个明星产品。这个移动时代的APP工厂，依然在广泛布局。</p>\n  <p>而且他们是全产业链推进。从底层最基础的模型层，到中间做性能优化匹配的连接层，再到最后落地C端的应用层，字节跳动都全面涉及覆盖。</p>\n  <p>据新莓daybreak不完全统计，去年 8 月至今，字节跳动在 AI 领域一共推出包括豆包大模型家族在内的 9款大模型、2 个智能体开发平台，和十余个 AI 应用。9月份，字节甚至推出首款AI硬件产品——AI智能体耳机Ola Friend，试图在硬件市场找到AI落地的新入口。</p>\n  <p>根据非凡资本旗下商业数字化及创新研究中心「非凡产研」的数据榜单显示，除了百度，字节是国内目前AI产品数量最多的企业。</p>\n  <p>对比两家大厂，百度AI产品的布局策略是从覆盖热门品类，到赋能日常衣食住行场景。比如AI对话产品文心一言，以及接入百度人脸识别技术的中关村在线科技生活馆、整合百度AI技术，为景区搭建智能票务系统等。</p>\n  <p>字节则更多集中聚焦市场热门领域中的高频使用场景品类。比如图片/视频生成、聊天、社交、办公、教育、音乐等领域。</p>\n  <p>据新莓daybreak不完全统计，今年年初以来，字节更新或上线了扣子、猫箱、星绘、河马爱学等8款AI应用产品，且大多都在今年上半年推出。</p>\n  <p>值得注意的是，这些孵化更新的产品并不仅产生于一个团队，除了聚焦于应用层的AI创新业务部门Flow，剪映、今日头条、大力教育、巨量引擎等业务部门也在发力AI，这些团队目前推出的产品分别聚焦于图像/视频、办公、教育、电商内容创作等领域。</p>\n  <p>此外，字节AI产品策略还有一个典型特征，就如移动互联网时代那样，更注重海内外同步布局。国内有豆包，海外有cici；国内是扣子，海外是Coze；国内推出猫箱，海外就有BagelBelI……</p>\n  <p>其中，字节跳动旗下海外产品Gauth、Cici AI、Hypic三款应用均进入2024年9月的全球APP下载榜Top30。</p>\n  <p>而且字节试图C端和B端通吃。典型如既面向C端又面向B端开发者的Agent定制产品「扣子」。</p>\n  <p>除了大模型，Agent（智能体）是另一个被广泛讨论的概念，业内共同的认知是，「Al agent是AI与具体应用场景结合落地的一种方式，大模型需要Agent来实现功能的扩展。」</p>\n  <p>「扣子」是目前Agent应用开发平台领域用户规模最高，品牌热度最大的产品。根据第三方平台Similarweb的数据，截至2024年6月，「扣子」的月访问用户数达到约200万左右。</p>\n  <p>上线初期，一位知乎用户分享自己使用扣子的实际体验，「基于文本的对话体验挺好，速度反应比原生GPT4速度快，流畅。也支持部署到discord上当Agent使用。但是多模态挺bug的，上传的文本和图像基本存在问题，完全不能用。」</p>\n  <p>尽管问题重重，依然没有阻挡大厂推出Agent开发平台的热情。</p>\n  <p>百度智能云千帆APPBuilder于2023年10月17日推出，是上线时间最早的产品，百度今年二季度财报数据显示，App Builder构建应用量达数十万。腾讯元器于今年5月上线，但目前并未公布用户规模，不过用户创建的智能体可以一键分发至QQ、微信客服、微信公众号等腾讯全域。</p>\n  <p>核心领域都要覆盖，国内海外同步，C端B端通吃，字节产品布局策略有效的前提是，大模型底座足够过硬。</p>\n  <h2><strong>大模型底座够不够支撑字节的野心？&nbsp;</strong></h2>\n  <p>字节在忙着布局应用的同时，豆包大模型也在快速更新。</p>\n  <p>2023年8月，字节自研推出了一款大规模预训练语言模型——云雀大模型，定位于自然语言内容生成和内容理解的AI大模型。豆包就是基于该模型开发出来的。7个月之后，云雀大模型升级为豆包大模型家族，并一口气发布了9个豆包模型。</p>\n  <p>字节大模型差不多是每两个月更新一次。今年5月，字节火山引擎先是一口气推出9款豆包大模型家族，两个月之后又发布了豆包·图生图模型；再到今年9月，推出了PixelDance和 Seaweed两款豆包·视频生成模型。</p>\n  <p>截至目前，豆包模型家族已经集齐了文本、语音、图片、音乐、视频等，共计13款模型。</p>\n  <p>今年5月，火山引擎总裁谭待接受虎嗅采访时提到，字节的模型在不断进化，一旦进化到某一个层次，就有可能去上面做应用。字节的两款TOP级AI原生应用——扣子和豆包，都是基于豆包大模型。</p>\n  <p>并且谭待还提到字节对外发布豆包大模型相对较晚的原因，「因为豆包到了大家可以广泛用它去做应用的程度，只有越来越多的人去做更多应用，反过来又可以驱动技术进步，形成一个正循环。」</p>\n  <p>最近，硅星人引用字节内部人士的话解释豆包大模型，「现在的豆包更像是一个统一的出口，连接用户。豆包产品后端连接了多个模型测试，是一个训练场，通过API的结果反馈来评测模型效果，但一时间很难有取舍。」</p>\n  <p>此外，定位为字节跳动的 ToB 云平台「火山引擎」，肩负着字节在B端市场数字化落地的大任。随着AI大模型的出现，「云服务+大模型」就成了云服务市场转型的重点。</p>\n  <p>目前，火山引擎已经形成云基础设施、豆包大模型和火山方舟2.0的一套产品体系。一方面可以帮助企业实现AI转型，一方面为内部输血。</p>\n  <p>比如上月底发布的豆包视频生成模型，在训练过程中就采用了火山引擎的大模型训练视频预处理方案。而火山引擎视频云团队提供的点播解决方案，也保障模型完成商业化应用。</p>\n  <p>在字节内部，火山引擎给自己的定位是「为他人做嫁衣」。去年，谭待对外表示，「火山引擎自己不做大模型，首先服务好国内做大模型创业的公司。」</p>\n  <p>当时，他对媒体强调，火山的任务一方面是算力供给，另一方面是搭建云原生机器学习平台，帮助企业应用好算力，进一步提升效率，把大模型训练得又快又稳定。</p>\n  <p>置身于AI大模型应用的浪潮年。在完成商业落地的同时，业内奋起争当「中国OpenAI」，底层能力的搭建至关重要。</p>\n  <p>上个月，谭待在解释为何此时才发布视频模型时提到，模型是影响未来十年、二十年的长远的东西。</p>\n  <p>AI计算正在加速渗入各行各业，一场看不见的革命也许就在不久的未来。但同时，字节在AI时代的野心也需要得到物理世界的验证。</p>\n  <p>而种一棵树，最好的时机首先是在十年前，其次是当下。</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzAxNzAyMjExNw==&amp;mid=2247491319&amp;idx=1&amp;sn=950e02c7cee02041a9ecb1cf40b3bdb2&amp;chksm=9a5c0e4e684cf1fe1a89eda9ddd7190d12c32ac773cd95548c85a661e5b00fc5100a6d5e4730&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“新莓daybreak”（ID：new-daybreak）</a>，作者：李欢，36氪经授权发布。</p>",
        "published": "2024-10-31 08:52:48",
        "id": "e867a1e2-b184-4619-b3a2-d3e0d162a392",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "2024年被称为「AI应用元年」，字节跳动加快AI布局，推出众多AI应用、模型、平台及硬件产品，希望复刻移动互联网时代的增长奇迹，打造类似今日头条、抖音的爆款AI应用。"
        },
        "tokens": 3929
    },
    {
        "title": "大众汽车利润暴跌63.7%：关停德国工厂，裁员数万人，全员降薪10%",
        "link": "https://36kr.com/p/3015906198578437?f=rss",
        "description": "<p>大众最狠的一刀，割在了自己身上。</p>\n  <p>周一，大众汽车工会主席表示，将关闭位于德国的<strong>至少三家工厂</strong>，裁掉<strong>数万名</strong>员工，欧洲剩余工厂也一并缩减规模。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_4cac54079e0e4947848ae436c584a657@000000_oswg810812oswg907oswg601_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>工人和大众管理层之间的冲突再度升级，总部门口抗议声不断，数万名工人已准备好要从12月1日开始罢工。</p>\n  <p>大众汽车工会主席丹尼埃拉·卡瓦洛则发出警告：</p>\n  <blockquote>\n   <p>管理层对这些计划非常认真，不要将其提议仅仅视作一种谈判策略，而予以拒绝。</p>\n  </blockquote>\n  <p>火药味弥漫德国。</p>\n  <h2><strong>大众在德国本土“动刀”</strong></h2>\n  <p>这周一，大众宣布了一系列高达<strong>40亿欧元</strong>（约308.4亿元）的降本计划，几乎波及大众在德国的整个产业：</p>\n  <p>包括关闭位于德国的<strong>至少三家工厂</strong>，裁掉<strong>数万名员工</strong>，甚至连<strong>欧洲</strong>其他地区也被殃及，工厂的规模都会进行缩减。</p>\n  <p>除此以外，剩余员工的薪资将<strong>至少削减 10%</strong>，并且可能在2025年和2026年工资冻结。</p>\n  <p>这是一次比以往、比预期更加大刀阔斧的重组。</p>\n  <p>具体哪些工厂受到影响，30万名员工裁去的比例有多少，都还是悬而未决的变数。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_ec5d6b5589af4751b5224a9bb30e690d@000000_oswg419721oswg762oswg478_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>劳工代表在大众监事会中拥有<strong>半数投票权</strong>，这使得大众很难强制关闭工厂，双方原本寄希望于<strong>谈判</strong>来解决问题。</p>\n  <p>这场谈判早在9月25日就已开始，但一个月的协商下来，结果却令工人非常不满。</p>\n  <p>在成立87年的沃尔夫斯堡总部门口，数千名工人吹着号角和口哨，坚称任何一家工厂都不应关闭。</p>\n  <p><strong>德国工会</strong>的谈判代表托尔斯滕·格罗格尔（Thorsten Groeger）表示：</p>\n  <blockquote>\n   <p>如果大众汽车周三确认其<strong>反乌托邦路线</strong>，董事会必须承担相应的后果。</p>\n  </blockquote>\n  <p>如此一来，工会可能将行使合法权利，从<strong>12月1日</strong>起举行<strong>罢工</strong>。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_fb0cd1603b1d4698b949e3a4441f8b43@000000_oswg755029oswg756oswg502_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>大众管理层的态度也很强硬，警告工人认真对待计划，不要当作是谈判策略，否则大众将提前中断谈判。</p>\n  <p>大众裁员关停工厂的消息，很快也惊动了德国政府，<strong>总理朔尔茨</strong>次日就介入到该事件，认为纠正过去的错误，不应以牺牲劳动力为代价，呼吁 “保留和保障就业”。</p>\n  <p>但短期内，还没有商议出更好的解决办法。</p>\n  <h2><strong>空前的降本规模是为何</strong></h2>\n  <p>硬着头皮大规模重组，实际上也是无奈之举，因为大众本身也承受着巨大的压力。</p>\n  <p>德国作为世界第三大经济体，正在呈现一个更加广泛的趋势：许多关键领域都在受到挑战，竞争对手更加灵活，成本更低。这其中也包括工业支柱<strong>汽车行业</strong>。</p>\n  <p>随之而来的是，德国的经济也开始出现紧张趋势，连续第二年出现收缩。</p>\n  <p>大众汽车作为德国最大的工业集团，处境也是如出一辙。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f8abafcc03174331b7855f67e347fa70@000000_oswg374910oswg943oswg550_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>最直观的表现就是<strong>销量下滑</strong>。今年1月到9月，大众汽车全球销量为<strong>652.43万辆</strong>，同比<strong>下降2.8%</strong>。</p>\n  <p>来自<strong>中国</strong>和<strong>欧洲</strong>的需求减弱，是销量下滑的主要因素。今年1-9月，欧洲市场出售279.3万辆，同比略下降0.8%；中国市场销量为205.66万辆，同比大跌10.2%。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_ef42f3b539164af9bb6d484229046848@000000_oswg83969oswg888oswg667_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>销量萎靡不振，体现在大众的财报当中也不乐观：</p>\n  <p>今年1月-9月，大众集团的<strong>营业收入</strong>为2372.79亿欧元（约1.83万亿元），和去年相比基本持平；第三季度的总营收为784.78亿欧元（约6054亿元），同比下降0.4%，环比下降5.8%。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f45af6c53aae41338d9b3eda45a2dce4@000000_oswg80017oswg934oswg534_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>其中，前三季度<strong>汽车部门</strong>营收1934亿欧元（约1496.5亿元），同比下降0.9%。</p>\n  <p>前三季度的<strong>税后净利润</strong>为89.17亿欧元（约），相比去年同期的128.68亿欧元（约688亿元），同比大跌<strong>30.7%</strong>；第三季度税后净利润为15.76亿欧元（约121.6亿元），同比暴跌<strong>63.7%</strong>，缩减到三年以来的最低水平。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_0d1f468359b148879e32cd3066e8aafb@000000_oswg82140oswg900oswg535_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>由于受盈利影响，大众的<strong>总现金流</strong>为326亿欧元（约2514.8亿元），比去年三季度末同比减少35欧元（约270亿元）。</p>\n  <p>三个月内，大众两次下调全年预期，把2024年全年的收入预期下调到3200亿欧元（约2.47万亿元），比2023年下降 0.7%。</p>\n  <p>董事会认为，当前形势已经非常严峻。如果再不采取全面措施恢复竞争力，大众可能将无法承担未来必要的投资。</p>\n  <p>同时，大众正在面临的，还有来自整个汽车行业的考验。</p>\n  <p>全球市场范围内，新能源汽车市场快速增长，但传统消费汽车市场已经趋于饱和，传统汽车的需求增速也在降低，给大众这样的传统汽车厂带来巨大压力。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_6e5526d606ae4eba95cce9f0ef9ee124@000000_oswg1086679oswg1080oswg675_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>电动化转型是大势所趋，但大众的转型进度，远比预期更慢。</p>\n  <p>一方面，大众这类传统车企的<strong>电动化起步较晚</strong>，既没有先行优势，还要面对强有力的对手，全球核心市场竞争激烈。</p>\n  <p>大众CEO奥博穆的原话是：</p>\n  <blockquote>\n   <p>蛋糕变小了，餐桌上的客人却越来越多了。</p>\n  </blockquote>\n  <p>另一方面，转型需要长期耗费巨额资金，制造电动汽车的<strong>成本太过高昂</strong>。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_5f3cda23d4f9442b831e50b6fbf4fef8@000000_oswg525036oswg748oswg412_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>伦敦数据分析提供商GlobalData表示，德国工会和政界人士敦促大众<strong>在德国</strong>生产电动汽车，无异于“<strong>饮鸩止渴</strong>”。</p>\n  <p>因为这意味着大众需要使用其<strong>最昂贵</strong>的工厂，生产<strong>高成本</strong>的电动汽车，销量却达不到预期。</p>\n  <p>大众部门主管托马斯·谢费尔透露，德国工厂的运营成本，高出目标成本的<strong>25-50%</strong>，一些工厂的<strong>成本</strong>要比竞争对手<strong>高出两倍</strong>，但生产效率却不高。</p>\n  <p>援引德国汽车工业协会VDA的数据，与国际水平相比，德国汽车行业工人的工资最高，2022年为<strong>每小时59欧元</strong>（约455元）。</p>\n  <p>而在中国，工资仅为<strong>每小时3美元</strong>（约23元）。</p>\n  <h2><strong>希望落在中国</strong></h2>\n  <p>大众裁员、关厂的事情，同样也在中国发生。</p>\n  <p>9月份，大众宣布，计划在明年关闭和上汽在南京的合资工厂，10月又有多家媒体爆料大众中国裁员，而且直接对大众中国总部下手。</p>\n  <p>但大众在中国重组资产，并非完全是败退止损，而是凝聚力量，寻求“重开”一局。</p>\n  <p>大众最大的希望，押注在中国，更具体地说，是在<strong>安徽合肥</strong>。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_02ca748349774277bb449e1310bbb480@000000_oswg788615oswg876oswg576_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>大众裁进口车业务给出的员工安置去向，就是<strong>大众安徽</strong>：由大众持股75%，主导整个企业的生产、运营、销售。</p>\n  <p>最重要的是，<strong>新的研发力量</strong>也被大众搬到了合肥，包括已有的软件公司CRIAD、新成立的大众汽车（中国）科技有限公司（VCTC）。</p>\n  <p>现在整个大众集团将重心放在了VCTC——承担最重要的新车平台研发工作，命名为CMP。</p>\n  <p>之前的研发核心CRIAD因为一系列交付延迟，现在的角色是为CMP配套研发EE架构。</p>\n  <p>这次机会对于大众而言，很可能是破釜沉舟，背水一战。</p>\n  <p>合资智能化自力更生，基本都以失败告终，保险起见，大众选择不再单打独斗。</p>\n  <p>这个下一代车型核心的CMP平台，其实就是以<strong>小鹏</strong>合作的<strong>G9平台</strong>为基础，至于智能驾驶部分，则交给了和<strong>地平线</strong>合资的公司<strong>酷睿程</strong>。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_88506d03ab5c4924a4ca3817dd58d1d8@000000_oswg806062oswg1080oswg589_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>如今大众火烧眉毛，向自己开刀，也是为了拥抱中国技术，追逐转型的新出路。</p>\n  <p>本文来自微信公众号 <a href=\"https://mp.weixin.qq.com/s?__biz=MzkzOTE3Nzc5MA==&amp;mid=2247535916&amp;idx=1&amp;sn=53baa238d608af70cba9c659ee94dced&amp;chksm=c3f0bbd2eae7b2e987def871e2e269b94dd94826979d4826046054b725e4fe48ab7a12594d00&amp;scene=0&amp;xtrack=1#rd\" rel=\"noopener noreferrer\" target=\"_blank\">“智能车参考”（ID：AI4Auto）</a>，作者：杰西卡&nbsp;，36氪经授权发布。</p>",
        "published": "2024-10-31 08:44:42",
        "id": "58b15ce4-ec79-4c15-8e94-b235470a8b10",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "大众汽车因销量下滑、盈利减少面临巨大压力，计划在德国关闭至少三家工厂、裁员数万人、全员降薪10%，德国政府已介入，大众在中国也有重组动作，且将重心放在安徽合肥寻求转型出路。"
        },
        "tokens": 4535
    },
    {
        "title": "波士顿动力电驱机器人进厂打工，全程自主无遥控，50万人在线围观",
        "link": "https://36kr.com/p/3016058173285635?f=rss",
        "description": "<p>登上油管热榜，吸引50万网友围观，波士顿动力人形机器人又放大招了——</p>\n  <p>无远程遥控（Fully Autonomous），Atlas可完全自主打工了。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1d2e17c7f9be41db971b356b0fdb5d6c@46958_oswg987917oswg1080oswg860_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>只需告诉Atlas前后搬运的位置坐标，它就能全自动分装物件，动作be like：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_6dc0c513ebed441bac2d4fc97746974b@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>而在Atlas的第一视角下，它看到的是酱紫的：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_a11041fc8cc14941b47e8c242d4d140e@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>面对“刁难”（物件在底层位置），Atlas直接一个帅气下蹲，再次成功完成任务。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_8d73278f21754a5994dd012d10e45c82@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>更有意思的是，当发现自己弄错位置后，Atlas突然以一个鬼畜完成了瞬间纠错。(笑死，怪突然的）</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_ebee6607af4448aeaea3a79721c10e94@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>总之，在近3分钟demo中，Atlas进行了一系列秀肌肉操作：头部、上半身、髋关节都能360°旋转，可随时转向、倒退行走……</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_1f4834bc58cb4e30bc904910f76d5345@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>有网友惊呼，其他机器人还在学走路，Atlas已经开始朝九晚五，甚至007式打工了！</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_fd5c25354bbd4559af0a414b3b62e9d2@46958_oswg81645oswg758oswg564_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>同行（通用仓库机器人nimble ai创始人）大赞：Atlas已经遥遥领先了。</p>\n  <blockquote>\n   <p>只有机器人专家才知道Atlas有多棒</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_393674087ccb4acd96ea66062355edd4@46958_oswg750914oswg1080oswg994_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <h2><strong>网友：完全自主？你引起了我的注意</strong></h2>\n  <p>自从今年4月宣布改液压为电驱后，这是波士顿动力人形机器人为数不多的露面。</p>\n  <p>上一次还是8月底，他们展示了Atlas能够一口气做俯卧撑、深蹲等热身运动，当时就震惊了上百万网友。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_3a47ef3508b84261865d17c0d8fca167@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>而在最新demo中，Atlas又瞄准了自动化控制，现在它能在集装箱和移动小车间自主移动发动机盖了。</p>\n  <p>据波士顿动力介绍，Atlas使用<strong>机器学习视觉模型</strong>来检测和定位环境固定装置和单个箱子，并且会使用专门的抓取策略，通过不断估计被操纵物体的状态来完成任务。</p>\n  <blockquote>\n   <p>机器人能够结合视觉、力和感知来检测环境变化（如移动固定装置）和动作故障（如未能插入盖子、绊倒、环境碰撞）并做出反应。</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_372a1ac9b34f4633bf382e58c724831b@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>看完一系列最新表现，果不其然又惊倒了一片网友：</p>\n  <blockquote>\n   <p>完全自主？现在你引起了我的注意</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f60da22180584e5a853ee0904686afa5@46958_oswg67304oswg1080oswg234_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_99dca0a4550b49a3908214c16e8d7dcb@46958_oswg80222oswg1080oswg236_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>网友们也是纷纷cue起了特斯拉人形机器人Optimus~</p>\n  <p>前一阵，Optimus在特斯拉的发布会上同样大秀肌肉（开场热舞、与人交谈猜丁壳、倒酒等一个不落），不过最后被多方证明存在现场远程操控。</p>\n  <p>后来特斯拉也发布了一个展示Optimus自主导航的demo：</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_761249423fe142bf93beb663e46e8db9@46958_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>对于这两家人形机器人领域同样炙手可热的竞争对手，网友们也开始各自站台，并最终达成了一个“共识”。</p>\n  <blockquote>\n   <p>二者的差距在于<strong>量产</strong>。波士顿动力单兵能力强，而特斯拉在商业化量产方面更具优势。</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_7814e59117d14fd2aeb6ad66f7665981@46958_oswg191406oswg1080oswg434_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>背后的逻辑也很简单，人形机器人最终还是要走向消费市场。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_2a8b4459535c4d58848afd3f21ed873a@46958_oswg94841oswg1080oswg221_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>不过不管怎样，Atlas展现的细节已十分惊艳，比如可以360°旋转的身体、头部。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_13f1f0afdf584ecc841eea78a6820c87@46958_oswg89498oswg1080oswg230_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_b27b6982b52f4ba79357905e6014df64@46958_oswg142970oswg1080oswg377_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>虽然也有人吐槽这很诡异，不过大多数人表示看好：</p>\n  <blockquote>\n   <p>人形机器人能够被设计而不是进化，意味着一旦我们弄清楚工程原理，各种变形金刚和驱魔人式的能力都可能发生。</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_83249af0c7cc47f5a00826cb711568c1@46958_oswg101461oswg1080oswg245_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>另外，还有人疑惑为什么Atlas不搞个360°全景摄像头，还需要转动头部呢？</p>\n  <p>对此，有网友推测最大原因还是<strong>控成本</strong>。</p>\n  <blockquote>\n   <p>更高分辨率的深度相机价格昂贵（带宽和计算），因此将超密集传感器限制在工作空间的位置是很有意义的。</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_62d96f2a39b441618b38bdb57a94d89a@46958_oswg125885oswg1080oswg281_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>实在不行，也可以“低质量的360°全景视觉+面向单一方向的高质量相机/激光雷达”（网友支招有）。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_7f69767c5de544218ec53f5b989a004e@46958_oswg132401oswg1080oswg291_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <h2><strong>One More Thing</strong></h2>\n  <p>在reddit网友一片热议中，部分网友发出了灵魂拷问：</p>\n  <blockquote>\n   <p>机器人完成这种任务（分装物件）好像没啥大意义？能不能更贴近现实生活。</p>\n  </blockquote>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_b2f8c6ba2a8046d08a473f401f2ca28f@46958_oswg213996oswg1080oswg344_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>对此，也有人对Atlas采用的技术表达担忧：基于点和规划器/优化器在泛化能力上可能不如神经网络等。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_789588ab1a214aebafbaaebfb973e39c@46958_oswg179627oswg1080oswg333_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>且就在刚刚，英伟达新发布了<strong>HOVER</strong>，一个1.5M参数的神经网络控制器，用于人形机器人的全身运动和操作协调。</p>\n  <p>据负责英伟达具身智能实验室（GEAR）的Jim Fan介绍：</p>\n  <blockquote>\n   <p>人类在行走、保持平衡以及操纵四肢到达期望位置时，需要大量的潜意识处理。我们在HOVER中捕捉了这种<strong>“潜意识”</strong>，这是一个单一模型，学习如何协调人形机器人的电机以支持运动和操纵。</p>\n   <p>我们在NVIDIA Isaac中训练了HOVER，这是一个GPU驱动的仿真套件，能够实现比现实时间<strong>快10000倍的物理模拟速度</strong>。</p>\n   <p>为了直观理解这个数字，机器人在虚拟“道场”中经历了一年的密集训练，但在一块GPU卡上仅花费了<strong>大约50分钟的真实时间</strong>。然后，神经网络无需微调即可零样本迁移到现实世界。</p>\n  </blockquote>\n  <p>简单说，HOVER可以被“提示”执行各种指令，英伟达称之为<strong>“控制模式”</strong>。比如：</p>\n  <ul>\n   <li>头部和手部姿势：可以通过XR设备如苹果的Vision Pro捕捉</li>\n   <li>全身姿势：通过动作捕捉或RGB相机</li>\n   <li>全身关节角度：外骨骼</li>\n   <li>根速度指令：操纵杆</li>\n  </ul>\n  <p>概括而言，HOVER提供了一个统一接口，允许使用任何方便的输入设备来控制机器人。</p>\n  <p>它简化了收集全身遥控操作数据的方式，以便于训练；且作为一个上游的视觉-语言-动作模型，只要提供运动指令，HOVER就能将其转换为高频的低级电机信号。</p>\n  <p class=\"image-wrapper\"><img src=\"https://img.36krcdn.com/hsossms/20241031/v2_f991370890bf45c2b79feed78a09aa8c@46958_oswg1026257oswg1080oswg2500_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1\" /></p>\n  <p>对此，你怎么看？</p>\n  <p>参考链接：[1]https://www.youtube.com/watch?v=F_7IPm7f1vI[2]https://www.reddit.com/r/singularity/comments/1gfmytj/new_creepy_atlas_video_dropped/[3]https://twitter.com/simonkalouche/status/1851632608171679817</p>\n  <p class=\"editor-note\">本文来自微信公众号<a href=\"https://mp.weixin.qq.com/s/8l_qCGgR22XTYtojm8mQ2A\" rel=\"noopener noreferrer\" target=\"_blank\">“量子位”</a>，作者：一水，36氪经授权发布。</p>",
        "published": "2024-10-31 08:30:22",
        "id": "6b4b164d-18fe-48f5-899e-dc2f30d4fa0c",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "波士顿动力人形机器人Atlas展示完全自主打工能力，其一系列操作引发网友热议，同时网友将其与特斯拉人形机器人Optimus对比，此外英伟达新发布用于人形机器人的神经网络控制器HOVER也被提及。"
        },
        "tokens": 5070
    },
    {
        "title": "三七互娱10月31日放量下跌1.27%；三七互娱间接投资北京智谱华章科技有限公司",
        "link": "https://36kr.com/p/3016163443582212?f=rss",
        "description": "",
        "published": "2024-10-31 08:40:26",
        "id": "b9eb48c2-5d09-4d6e-9515-7c20e4c645eb",
        "source": "36氪",
        "section": "文章资讯",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "10月31日三七互娱放量下跌1.27%且间接投资北京智谱华章科技有限公司"
        },
        "tokens": 593
    },
    {
        "title": "三星暗示有望近期开始向英伟达供应HBM芯片",
        "link": "https://36kr.com/newsflashes/3016141164586505?f=rss",
        "description": "韩国三星电子公司周四暗示，有可能在近期向美国人工智能巨头英伟达提供先进的高带宽存储器（HBM）。这家韩国科技巨头一直在努力让其HBM3E芯片通过英伟达的质量测试，而其本土竞争对手SK海力士公司最近已开始量产业界领先的12层HBM3E芯片。三星电子内存业务副总裁Kim Jae-june在第三季度财报公布后召开的电话会议上表示：“目前，我们正在量产8层和12层HBM3E产品。”（新浪财经）",
        "published": "2024-10-31 08:44:21",
        "id": "aa0189ba-894c-4ca2-9a90-99d21c4141d7",
        "source": "36氪",
        "section": "最新快讯",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "三星暗示近期可能向英伟达供应HBM芯片，其正在量产8层和12层HBM3E产品，SK海力士已开始量产业界领先的12层HBM3E芯片。"
        },
        "tokens": 715
    },
    {
        "title": "DAWN: Designing Distributed Agents in a Worldwide Network",
        "link": "https://arxiv.org/abs/2410.22339",
        "description": "arXiv:2410.22339v1 Announce Type: new \nAbstract: The rapid evolution of Large Language Models (LLMs) has transformed them from basic conversational tools into sophisticated entities capable of complex reasoning and decision-making. These advancements have led to the development of specialized LLM-based agents designed for diverse tasks such as coding and web browsing. As these agents become more capable, the need for a robust framework that facilitates global communication and collaboration among them towards advanced objectives has become increasingly critical. Distributed Agents in a Worldwide Network (DAWN) addresses this need by offering a versatile framework that integrates LLM-based agents with traditional software systems, enabling the creation of agentic applications suited for a wide range of use cases. DAWN enables distributed agents worldwide to register and be easily discovered through Gateway Agents. Collaborations among these agents are coordinated by a Principal Agent equipped with reasoning strategies. DAWN offers three operational modes: No-LLM Mode for deterministic tasks, Copilot for augmented decision-making, and LLM Agent for autonomous operations. Additionally, DAWN ensures the safety and security of agent collaborations globally through a dedicated safety, security, and compliance layer, protecting the network against attackers and adhering to stringent security and compliance standards. These features make DAWN a robust network for deploying agent-based applications across various industries.",
        "published": "2024-10-31 04:00:00",
        "id": "0c5e880b-070a-494c-a5cd-8f9b431ff95f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "DAWN是一个多功能框架，将基于大型语言模型（LLM）的代理与传统软件系统集成，提供三种操作模式并确保全球代理协作的安全，适用于多种用例的智能体应用部署。"
        },
        "tokens": 858
    },
    {
        "title": "Testing GPT-4-o1-preview on math and science problems: A follow-up study",
        "link": "https://arxiv.org/abs/2410.22340",
        "description": "arXiv:2410.22340v1 Announce Type: new \nAbstract: In August 2023, Scott Aaronson and I reported the results of testing GPT4 with the Wolfram Alpha and Code Interpreter plug-ins over a collection of 105 original high-school level and college-level science and math problems (Davis and Aaronson, 2023). In September 2024, I tested the recently released model GPT-4o1-preview on the same collection. Overall I found that performance had significantly improved, but was still considerably short of perfect. In particular, problems that involve spatial reasoning are often stumbling blocks.",
        "published": "2024-10-31 04:00:00",
        "id": "e7972c0e-7cbf-4360-bcad-4225e14450a4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "2024年9月，作者测试了GPT - 4o1 - preview在之前用于测试GPT4的高中和大学水平的科学与数学问题集上的表现，发现其性能有显著提升但仍不够完美，尤其是空间推理问题常成为阻碍。"
        },
        "tokens": 745
    },
    {
        "title": "GleanVec: Accelerating vector search with minimalist nonlinear dimensionality reduction",
        "link": "https://arxiv.org/abs/2410.22347",
        "description": "arXiv:2410.22347v1 Announce Type: new \nAbstract: Embedding models can generate high-dimensional vectors whose similarity reflects semantic affinities. Thus, accurately and timely retrieving those vectors in a large collection that are similar to a given query has become a critical component of a wide range of applications. In particular, cross-modal retrieval (e.g., where a text query is used to find images) is gaining momentum rapidly. Here, it is challenging to achieve high accuracy as the queries often have different statistical distributions than the database vectors. Moreover, the high vector dimensionality puts these search systems under compute and memory pressure, leading to subpar performance. In this work, we present new linear and nonlinear methods for dimensionality reduction to accelerate high-dimensional vector search while maintaining accuracy in settings with in-distribution (ID) and out-of-distribution (OOD) queries. The linear LeanVec-Sphering outperforms other linear methods, trains faster, comes with no hyperparameters, and allows to set the target dimensionality more flexibly. The nonlinear Generalized LeanVec (GleanVec) uses a piecewise linear scheme to further improve the search accuracy while remaining computationally nimble. Initial experimental results show that LeanVec-Sphering and GleanVec push the state of the art for vector search.",
        "published": "2024-10-31 04:00:00",
        "id": "b10167fc-39f0-4977-bb66-2e8ef1a593a4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出新的线性和非线性降维方法（LeanVec-Sphering和GleanVec）以加速高维向量搜索并保持准确性，初始实验结果显示它们推动了向量搜索技术发展。"
        },
        "tokens": 854
    },
    {
        "title": "Search Engines in an AI Era: The False Promise of Factual and Verifiable Source-Cited Responses",
        "link": "https://arxiv.org/abs/2410.22349",
        "description": "arXiv:2410.22349v1 Announce Type: new \nAbstract: Large Language Model (LLM)-based applications are graduating from research prototypes to products serving millions of users, influencing how people write and consume information. A prominent example is the appearance of Answer Engines: LLM-based generative search engines supplanting traditional search engines. Answer engines not only retrieve relevant sources to a user query but synthesize answer summaries that cite the sources. To understand these systems' limitations, we first conducted a study with 21 participants, evaluating interactions with answer vs. traditional search engines and identifying 16 answer engine limitations. From these insights, we propose 16 answer engine design recommendations, linked to 8 metrics. An automated evaluation implementing our metrics on three popular engines (You.com, Perplexity.ai, BingChat) quantifies common limitations (e.g., frequent hallucination, inaccurate citation) and unique features (e.g., variation in answer confidence), with results mirroring user study insights. We release our Answer Engine Evaluation benchmark (AEE) to facilitate transparent evaluation of LLM-based applications.",
        "published": "2024-10-31 04:00:00",
        "id": "a9b85ad5-d53a-4d36-bf07-2028e85c0874",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究人员通过21人参与的研究评估Answer Engines与传统搜索引擎的交互，提出16项设计建议和8项指标，用这些指标对三个流行引擎进行自动评估并发布AEE基准，以理解LLM应用的局限性。"
        },
        "tokens": 834
    },
    {
        "title": "Quality-Aware End-to-End Audio-Visual Neural Speaker Diarization",
        "link": "https://arxiv.org/abs/2410.22350",
        "description": "arXiv:2410.22350v1 Announce Type: new \nAbstract: In this paper, we propose a quality-aware end-to-end audio-visual neural speaker diarization framework, which comprises three key techniques. First, our audio-visual model takes both audio and visual features as inputs, utilizing a series of binary classification output layers to simultaneously identify the activities of all speakers. This end-to-end framework is meticulously designed to effectively handle situations of overlapping speech, providing accurate discrimination between speech and non-speech segments through the utilization of multi-modal information. Next, we employ a quality-aware audio-visual fusion structure to address signal quality issues for both audio degradations, such as noise, reverberation and other distortions, and video degradations, such as occlusions, off-screen speakers, or unreliable detection. Finally, a cross attention mechanism applied to multi-speaker embedding empowers the network to handle scenarios with varying numbers of speakers. Our experimental results, obtained from various data sets, demonstrate the robustness of our proposed techniques in diverse acoustic environments. Even in scenarios with severely degraded video quality, our system attains performance levels comparable to the best available audio-visual systems.",
        "published": "2024-10-31 04:00:00",
        "id": "e4130b7a-df0a-449c-9445-81efe2f2a1f8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出质量感知端到端视听神经说话人分离框架，含三项关键技术，实验结果证明其在不同声学环境中的鲁棒性。"
        },
        "tokens": 815
    },
    {
        "title": "Improving the accuracy of food security predictions by integrating conflict data",
        "link": "https://arxiv.org/abs/2410.22342",
        "description": "arXiv:2410.22342v1 Announce Type: new \nAbstract: Violence and armed conflicts have emerged as prominent factors driving food crises. However, the extent of their impact remains largely unexplored. This paper provides an in-depth analysis of the impact of violent conflicts on food security in Africa. We performed a comprehensive correlation analysis using data from the Famine Early Warning Systems Network (FEWSNET) and the Armed Conflict Location Event Data (ACLED). Our results show that using conflict data to train machine learning models leads to a 1.5% increase in accuracy compared to models that do not incorporate conflict-related information. The key contribution of this study is the quantitative analysis of the impact of conflicts on food security predictions.",
        "published": "2024-10-31 04:00:00",
        "id": "2ca2aee3-e6c8-4c70-a8f5-0fa6d50356e9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究发现将冲突数据整合进机器学习模型可提高非洲粮食安全预测准确性，相比不使用冲突相关信息的模型准确率提高1.5%。"
        },
        "tokens": 725
    },
    {
        "title": "Efficient Machine Translation with a BiLSTM-Attention Approach",
        "link": "https://arxiv.org/abs/2410.22335",
        "description": "arXiv:2410.22335v1 Announce Type: new \nAbstract: With the rapid development of Natural Language Processing (NLP) technology, the accuracy and efficiency of machine translation have become hot topics of research. This paper proposes a novel Seq2Seq model aimed at improving translation quality while reducing the storage space required by the model. The model employs a Bidirectional Long Short-Term Memory network (Bi-LSTM) as the encoder to capture the context information of the input sequence; the decoder incorporates an attention mechanism, enhancing the model's ability to focus on key information during the translation process. Compared to the current mainstream Transformer model, our model achieves superior performance on the WMT14 machine translation dataset while maintaining a smaller size.\n  The study first introduces the design principles and innovative points of the model architecture, followed by a series of experiments to verify the effectiveness of the model. The experimental includes an assessment of the model's performance on different language pairs, as well as comparative analysis with traditional Seq2Seq models. The results show that while maintaining translation accuracy, our model significantly reduces the storage requirements, which is of great significance for translation applications in resource-constrained scenarios. our code are available at https://github.com/mindspore-lab/models/tree/master/research/arxiv_papers/miniformer . Thanks for the support provided by MindSpore Community.",
        "published": "2024-10-31 04:00:00",
        "id": "66e11c67-2c71-4e5c-9330-dfe25c0c97a8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文提出一种新的Seq2Seq模型，采用Bi - LSTM为编码器、解码器含注意力机制，在WMT14数据集上性能优于Transformer模型且尺寸更小，实验验证了模型有效性，代码可在特定网址获取。"
        },
        "tokens": 875
    },
    {
        "title": "Requirements for a Digital Library System: A Case Study in Digital Humanities (Technical Report)",
        "link": "https://arxiv.org/abs/2410.22358",
        "description": "arXiv:2410.22358v1 Announce Type: new \nAbstract: Archives of libraries contain many materials, which have not yet been made available to the public. The prioritization of which content to provide and especially how to design effective access paths depend on potential users' needs. As a case study we interviewed researchers working on topics related to one German philosopher to map out their information interaction workflow. Additionally, we deeply analyze study participants' requirements for a digital library system. Moreover, we discuss how existing methods may meet their requirements, but we also discuss what implications these methods have in practice, e.g., computational costs and hallucinations. In brief, this paper contributes the findings of our digital humanities case study resulting in system requirements.",
        "published": "2024-10-31 04:00:00",
        "id": "f6da46a2-6bab-48a3-b322-21b14dde92ff",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "以德国一位哲学家相关主题的研究者为案例，分析数字图书馆系统需求，探讨现有方法满足需求的情况及其在实践中的影响。"
        },
        "tokens": 724
    },
    {
        "title": "ArxivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
        "link": "https://arxiv.org/abs/2410.22360",
        "description": "arXiv:2410.22360v1 Announce Type: new \nAbstract: When conducting literature reviews, scientists often create literature review tables - tables whose rows are publications and whose columns constitute a schema, a set of aspects used to compare and contrast the papers. Can we automatically generate these tables using language models (LMs)? In this work, we introduce a framework that leverages LMs to perform this task by decomposing it into separate schema and value generation steps. To enable experimentation, we address two main challenges: First, we overcome a lack of high-quality datasets to benchmark table generation by curating and releasing arxivDIGESTables, a new dataset of 2,228 literature review tables extracted from ArXiv papers that synthesize a total of 7,542 research papers. Second, to support scalable evaluation of model generations against human-authored reference tables, we develop DecontextEval, an automatic evaluation method that aligns elements of tables with the same underlying aspects despite differing surface forms. Given these tools, we evaluate LMs' abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references). Finally, through a human evaluation study we find that even when LMs fail to fully reconstruct a reference table, their generated novel aspects can still be useful.",
        "published": "2024-10-31 04:00:00",
        "id": "74e8ea86-8086-443e-9b57-d890bb6b9282",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍利用语言模型将科学文献合成为表格的框架，克服数据质量和评估两大挑战，评估语言模型重建参考表的能力并发现相关情况。"
        },
        "tokens": 858
    },
    {
        "title": "Accelerating Augmentation Invariance Pretraining",
        "link": "https://arxiv.org/abs/2410.22364",
        "description": "arXiv:2410.22364v1 Announce Type: new \nAbstract: Our work tackles the computational challenges of contrastive learning methods, particularly for the pretraining of Vision Transformers (ViTs). Despite the effectiveness of contrastive learning, the substantial computational resources required for training often hinder their practical application. To mitigate this issue, we propose an acceleration framework, leveraging ViT's unique ability to generalize across inputs of varying sequence lengths. Our method employs a mix of sequence compression strategies, including randomized token dropout and flexible patch scaling, to reduce the cost of gradient estimation and accelerate convergence. We further provide an in-depth analysis of the gradient estimation error of various acceleration strategies as well as their impact on downstream tasks, offering valuable insights into the trade-offs between acceleration and performance.\n  We also propose a novel procedure to identify an optimal acceleration schedule to adjust the sequence compression ratios to the training progress, ensuring efficient training without sacrificing downstream performance. Our approach significantly reduces computational overhead across various self-supervised learning algorithms on large-scale datasets. In ImageNet, our method achieves speedups of 4$\\times$ in MoCo, 3.3$\\times$ in SimCLR, and 2.5$\\times$ in DINO, demonstrating substantial efficiency gains.",
        "published": "2024-10-31 04:00:00",
        "id": "4b317c9a-3b20-42d8-b3fb-eb57dca50cf6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决对比学习方法计算挑战，提出加速框架，采用序列压缩策略减少梯度估计成本、加速收敛，还提出确定最优加速计划的程序，在ImageNet中大幅提高多种自监督学习算法效率。"
        },
        "tokens": 844
    },
    {
        "title": "Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders",
        "link": "https://arxiv.org/abs/2410.22366",
        "description": "arXiv:2410.22366v1 Announce Type: new \nAbstract: Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.\n  Code is available at https://github.com/surkovv/sdxl-unbox",
        "published": "2024-10-31 04:00:00",
        "id": "a4d708de-e8b8-4421-9808-ebd839979213",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究人员探究用稀疏自动编码器（SAEs）学习像SDXL Turbo这类几步式文本到图像扩散模型的可解释特征，发现其学到的特征可解释、对生成过程有因果影响且揭示模块间的分工，相关代码已开源。"
        },
        "tokens": 876
    },
    {
        "title": "Survey of User Interface Design and Interaction Techniques in Generative AI Applications",
        "link": "https://arxiv.org/abs/2410.22370",
        "description": "arXiv:2410.22370v1 Announce Type: new \nAbstract: The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.",
        "published": "2024-10-31 04:00:00",
        "id": "7bda5f75-ba96-4882-90a4-d0323440c6e1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章介绍了一项针对生成式AI应用中用户界面设计和交互技术的调查，旨在创建用户交互模式汇编，为设计师和开发者提供参考并降低学习门槛。"
        },
        "tokens": 783
    },
    {
        "title": "Error Bounds for Deep Learning-based Uncertainty Propagation in SDEs",
        "link": "https://arxiv.org/abs/2410.22371",
        "description": "arXiv:2410.22371v1 Announce Type: new \nAbstract: Stochastic differential equations are commonly used to describe the evolution of stochastic processes. The uncertainty of such processes is best represented by the probability density function (PDF), whose evolution is governed by the Fokker-Planck partial differential equation (FP-PDE). However, it is generally infeasible to solve the FP-PDE in closed form. In this work, we show that physics-informed neural networks (PINNs) can be trained to approximate the solution PDF using existing methods. The main contribution is the analysis of the approximation error: we develop a theory to construct an arbitrary tight error bound with PINNs. In addition, we derive a practical error bound that can be efficiently constructed with existing training methods. Finally, we explain that this error-bound theory generalizes to approximate solutions of other linear PDEs. Several numerical experiments are conducted to demonstrate and validate the proposed methods.",
        "published": "2024-10-31 04:00:00",
        "id": "6036c0e8-830f-4ee3-9597-085f14826319",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究表明可通过物理信息神经网络近似随机微分方程的概率密度函数解，分析了近似误差并构建误差界限，还解释该理论可推广到其他线性偏微分方程近似解。"
        },
        "tokens": 784
    },
    {
        "title": "Lecture Notes on Grid Modeling of Renewable Energy",
        "link": "https://arxiv.org/abs/2410.22361",
        "description": "arXiv:2410.22361v1 Announce Type: new \nAbstract: These lecture notes provide a comprehensive guide on Grid Modeling of Renewable Energy, offering a foundational overview of power system network modeling, power flow, and load flow algorithms critical for electrical and renewable energy engineering. Key topics include steady-state, dynamic, and frequency domain models, with a particular focus on renewable energy integration, simulation techniques, and their effects on grid stability and power quality. Practical examples using Matpower and Pandapower tools are included to reinforce concepts, ensuring that students gain hands-on experience in modeling and analyzing modern energy systems under variable conditions.",
        "published": "2024-10-31 04:00:00",
        "id": "e27e773d-7e46-4ea1-bbd9-3bf11b989bfa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "arXiv发布的关于可再生能源电网建模的讲义笔记，涵盖电力系统网络建模、潮流算法等关键内容，含Matpower和Pandapower工具的实例以强化概念。"
        },
        "tokens": 706
    },
    {
        "title": "Project MPG: towards a generalized performance benchmark for LLM capabilities",
        "link": "https://arxiv.org/abs/2410.22368",
        "description": "arXiv:2410.22368v1 Announce Type: new \nAbstract: There exists an extremely wide array of LLM benchmarking tasks, whereas oftentimes a single number is the most actionable for decision-making, especially by non-experts. No such aggregation schema exists that is not Elo-based, which could be costly or time-consuming. Here we propose a method to aggregate performance across a general space of benchmarks, nicknamed Project \"MPG,\" dubbed Model Performance and Goodness, additionally referencing a metric widely understood to be an important yet inaccurate and crude measure of car performance. Here, we create two numbers: a \"Goodness\" number (answer accuracy) and a \"Fastness\" number (cost or QPS). We compare models against each other and present a ranking according to our general metric as well as subdomains. We find significant agreement between the raw Pearson correlation of our scores and those of Chatbot Arena, even improving on the correlation of the MMLU leaderboard to Chatbot Arena.",
        "published": "2024-10-31 04:00:00",
        "id": "e9bebb23-886c-4938-8e48-9285e4c1bddc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出名为Project 'MPG'的方法来汇总基准测试的性能，生成'Goodness'（答案准确性）和'Fastness'（成本或QPS）两个数字，比较模型并进行排名，其分数与Chatbot Arena的原始皮尔逊相关性显著一致。"
        },
        "tokens": 813
    },
    {
        "title": "RuleRAG: Rule-guided retrieval-augmented generation with language models for question answering",
        "link": "https://arxiv.org/abs/2410.22353",
        "description": "arXiv:2410.22353v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) framework has shown promising potential in knowledge-intensive question answering (QA) by retrieving external corpus and generating based on augmented context. However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-Guided Retrieval-Augmented Generation with LMs, which explicitly introduces symbolic rules as demonstrations for in-context learning (RuleRAG-ICL) to guide retrievers to retrieve logically related documents in the directions of rules and uniformly guide generators to generate answers attributed by the guidance of the same set of rules. Moreover, the combination of queries and rules can be further used as supervised fine-tuning data to update retrievers and generators (RuleRAG-FT) to achieve better rule-based instruction following capability, leading to retrieve more supportive results and generate more acceptable answers. To emphasize the attribution of rules, we construct five rule-aware QA benchmarks, including three temporal and two static scenarios, and equip RuleRAG with several kinds of retrievers and generators. Experiments demonstrate that training-free RuleRAG-ICL effectively improves the retrieval quality of +89.2% in Recall@10 scores and generation accuracy of +103.1% in exact match scores over standard RAG on average across the five benchmarks, and further fine-tuned RuleRAG-FT consistently yields more significant performance enhancement. Extensive analyses indicate that RuleRAG scales well with increasing numbers of retrieved documents and exhibits generalization ability for untrained rules.",
        "published": "2024-10-31 04:00:00",
        "id": "42e2be6d-c127-4073-90c3-b95c1fb17e55",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出Rule-Guided Retrieval-Augmented Generation with LMs（RuleRAG），包含RuleRAG-ICL和RuleRAG-FT两种方式，实验证明RuleRAG-ICL有效提升检索和生成质量，RuleRAG-FT性能增强更显著。"
        },
        "tokens": 967
    },
    {
        "title": "Neuromorphic Programming: Emerging Directions for Brain-Inspired Hardware",
        "link": "https://arxiv.org/abs/2410.22352",
        "description": "arXiv:2410.22352v1 Announce Type: new \nAbstract: The value of brain-inspired neuromorphic computers critically depends on our ability to program them for relevant tasks. Currently, neuromorphic hardware often relies on machine learning methods adapted from deep learning. However, neuromorphic computers have potential far beyond deep learning if we can only harness their energy efficiency and full computational power. Neuromorphic programming will necessarily be different from conventional programming, requiring a paradigm shift in how we think about programming. This paper presents a conceptual analysis of programming within the context of neuromorphic computing, challenging conventional paradigms and proposing a framework that aligns more closely with the physical intricacies of these systems. Our analysis revolves around five characteristics that are fundamental to neuromorphic programming and provides a basis for comparison to contemporary programming methods and languages. By studying past approaches, we contribute a framework that advocates for underutilized techniques and calls for richer abstractions to effectively instrument the new hardware class.",
        "published": "2024-10-31 04:00:00",
        "id": "c678af05-7605-45ca-bb0e-89cd8b408b24",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "阐述神经形态编程对神经形态计算机的重要性，分析其与传统编程的差异，提出新框架以更好利用这类硬件的特性。"
        },
        "tokens": 768
    },
    {
        "title": "A Hierarchical Language Model For Interpretable Graph Reasoning",
        "link": "https://arxiv.org/abs/2410.22372",
        "description": "arXiv:2410.22372v1 Announce Type: new \nAbstract: Large language models (LLMs) are being increasingly explored for graph tasks. Despite their remarkable success in text-based tasks, LLMs' capabilities in understanding explicit graph structures remain limited, particularly with large graphs. In this work, we introduce Hierarchical Language Model for Graph (HLM-G), which employs a two-block architecture to capture node-centric local information and interaction-centric global structure, effectively enhancing graph structure understanding abilities. The proposed scheme allows LLMs to address various graph queries with high efficacy, efficiency, and robustness, while reducing computational costs on large-scale graph tasks. Furthermore, we demonstrate the interpretability of our model using intrinsic attention weights and established explainers. Comprehensive evaluations across diverse graph reasoning and real-world tasks of node, link, and graph-levels highlight the superiority of our method, marking a significant advancement in the application of LLMs to graph understanding.",
        "published": "2024-10-31 04:00:00",
        "id": "a8102993-ee47-4b4a-97bd-256a76e9cf6a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出用于图的分层语言模型HLM - G，其采用两块架构提升对图结构的理解能力，在多种图推理和任务中有优越表现且具有可解释性。"
        },
        "tokens": 773
    },
    {
        "title": "Analytic Continual Test-Time Adaptation for Multi-Modality Corruption",
        "link": "https://arxiv.org/abs/2410.22373",
        "description": "arXiv:2410.22373v1 Announce Type: new \nAbstract: Test-Time Adaptation (TTA) aims to help pre-trained model bridge the gap between source and target datasets using only the pre-trained model and unlabelled test data. A key objective of TTA is to address domain shifts in test data caused by corruption, such as weather changes, noise, or sensor malfunctions. Multi-Modal Continual Test-Time Adaptation (MM-CTTA), an extension of TTA with better real-world applications, further allows pre-trained models to handle multi-modal inputs and adapt to continuously-changing target domains. MM-CTTA typically faces challenges including error accumulation, catastrophic forgetting, and reliability bias, with few existing approaches effectively addressing these issues in multi-modal corruption scenarios. In this paper, we propose a novel approach, Multi-modality Dynamic Analytic Adapter (MDAA), for MM-CTTA tasks. We innovatively introduce analytic learning into TTA, using the Analytic Classifiers (ACs) to prevent model forgetting. Additionally, we develop Dynamic Selection Mechanism (DSM) and Soft Pseudo-label Strategy (SPS), which enable MDAA to dynamically filter reliable samples and integrate information from different modalities. Extensive experiments demonstrate that MDAA achieves state-of-the-art performance on MM-CTTA tasks while ensuring reliable model adaptation.",
        "published": "2024-10-31 04:00:00",
        "id": "6c613942-622d-438e-b95a-108b6a35a4b9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为Multi - modality Dynamic Analytic Adapter (MDAA)的新方法用于多模态持续测试时间适应(MM - CTTA)任务，将分析学习引入测试时间适应(TTA)，实验证明该方法在MM - CTTA任务上达到最先进性能。"
        },
        "tokens": 881
    },
    {
        "title": "Machine Unlearning using Forgetting Neural Networks",
        "link": "https://arxiv.org/abs/2410.22374",
        "description": "arXiv:2410.22374v1 Announce Type: new \nAbstract: Modern computer systems store vast amounts of personal data, enabling advances in AI and ML but risking user privacy and trust. For privacy reasons, it is desired sometimes for an ML model to forget part of the data it was trained on. This paper presents a new approach to machine unlearning using forgetting neural networks (FNN). FNNs are neural networks with specific forgetting layers, that take inspiration from the processes involved when a human brain forgets. While FNNs had been proposed as a theoretical construct, they have not been previously used as a machine unlearning method. We describe four different types of forgetting layers and study their properties. In our experimental evaluation, we report our results on the MNIST handwritten digit recognition and fashion datasets. The effectiveness of the unlearned models was tested using Membership Inference Attacks (MIA). Successful experimental results demonstrate the great potential of our proposed method for dealing with the machine unlearning problem.",
        "published": "2024-10-31 04:00:00",
        "id": "a2a48f10-69c1-4401-9ee8-6b376a7d3366",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出用遗忘神经网络（FNN）进行机器遗忘的新方法，描述四种遗忘层及其特性，用MNIST数据集测试遗忘模型有效性。"
        },
        "tokens": 780
    },
    {
        "title": "Rethinking Code Refinement: Learning to Judge Code Efficiency",
        "link": "https://arxiv.org/abs/2410.22375",
        "description": "arXiv:2410.22375v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities in understanding and generating codes. Due to these capabilities, many recent methods are proposed to automatically refine the codes with LLMs. However, we should rethink that the refined codes (from LLMs and even humans) are not always more efficient than their original versions. On the other hand, running two different versions of codes and comparing them every time is not ideal and time-consuming. Therefore, in this work, we propose a novel method based on the code language model that is trained to judge the efficiency between two different codes (generated across humans and machines) by either classifying the superior one or predicting the relative improvement. We validate our method on multiple programming languages with multiple refinement steps, demonstrating that the proposed method can effectively distinguish between more and less efficient versions of code.",
        "published": "2024-10-31 04:00:00",
        "id": "39d3d1b1-5fcb-4e45-ac5b-23ba15d8bae5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "由于大语言模型在理解和生成代码方面能力很强，很多方法被用于自动优化代码，但优化后的代码不一定更高效，所以提出一种基于代码语言模型的新方法来判断不同代码间的效率，该方法在多种编程语言上得到验证。"
        },
        "tokens": 779
    },
    {
        "title": "Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance",
        "link": "https://arxiv.org/abs/2410.22376",
        "description": "arXiv:2410.22376v1 Announce Type: new \nAbstract: State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at https://github.com/krafton-ai/Rare2Frequent.",
        "published": "2024-10-31 04:00:00",
        "id": "dbb456a0-19d1-4f81-a0a9-11204000b07d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种名为R2F的免训练方法，通过利用大型语言模型中的丰富语义知识，在扩散推理过程中进行从稀有到常见概念的引导，可显著提升扩散模型对稀有概念的组合生成能力，在相关实验中表现优于现有模型。"
        },
        "tokens": 871
    },
    {
        "title": "A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification",
        "link": "https://arxiv.org/abs/2410.22377",
        "description": "arXiv:2410.22377v1 Announce Type: new \nAbstract: In recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture dependencies among variables and across time points. The objective of the presented systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and over 150 journal papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive collection of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in future studies. To the best of our knowledge, this is the first systematic literature review presenting a detailed comparison of the results of current spatio-temporal GNN models in different domains. In addition, in its final part this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability.",
        "published": "2024-10-31 04:00:00",
        "id": "e677ebed-2af3-4cb9-954a-d0bbc329e4e3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "对时空图神经网络（GNNs）用于时间序列分类和预测的建模方法和应用领域进行系统文献综述，分析当前的研究现状、局限和挑战。"
        },
        "tokens": 831
    },
    {
        "title": "Discrete Modeling via Boundary Conditional Diffusion Processes",
        "link": "https://arxiv.org/abs/2410.22380",
        "description": "arXiv:2410.22380v1 Announce Type: new \nAbstract: We present an novel framework for efficiently and effectively extending the powerful continuous diffusion processes to discrete modeling. Previous approaches have suffered from the discrepancy between discrete data and continuous modeling. Our study reveals that the absence of guidance from discrete boundaries in learning probability contours is one of the main reasons. To address this issue, we propose a two-step forward process that first estimates the boundary as a prior distribution and then rescales the forward trajectory to construct a boundary conditional diffusion model. The reverse process is proportionally adjusted to guarantee that the learned contours yield more precise discrete data. Experimental results indicate that our approach achieves strong performance in both language modeling and discrete image generation tasks. In language modeling, our approach surpasses previous state-of-the-art continuous diffusion language models in three translation tasks and a summarization task, while also demonstrating competitive performance compared to auto-regressive transformers. Moreover, our method achieves comparable results to continuous diffusion models when using discrete ordinal pixels and establishes a new state-of-the-art for categorical image generation on the Cifar-10 dataset.",
        "published": "2024-10-31 04:00:00",
        "id": "57945227-d03a-48a8-81d0-a897e0cb1840",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种将连续扩散过程扩展到离散建模的新框架，通过两步前向过程解决离散数据与连续建模之间的差异，在语言建模和离散图像生成任务中取得良好效果。"
        },
        "tokens": 811
    },
    {
        "title": "Robust training of implicit generative models for multivariate and heavy-tailed distributions with an invariant statistical loss",
        "link": "https://arxiv.org/abs/2410.22381",
        "description": "arXiv:2410.22381v1 Announce Type: new \nAbstract: Traditional implicit generative models are capable of learning highly complex data distributions. However, their training involves distinguishing real data from synthetically generated data using adversarial discriminators, which can lead to unstable training dynamics and mode dropping issues. In this work, we build on the \\textit{invariant statistical loss} (ISL) method introduced in \\cite{de2024training}, and extend it to handle heavy-tailed and multivariate data distributions.\n  The data generated by many real-world phenomena can only be properly characterised using heavy-tailed probability distributions, and traditional implicit methods struggle to effectively capture their asymptotic behavior. To address this problem, we introduce a generator trained with ISL, that uses input noise from a generalised Pareto distribution (GPD). We refer to this generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate that Pareto-ISL accurately models the tails of the distributions while still effectively capturing their central characteristics.\n  The original ISL function was conceived for 1D data sets. When the actual data is $n$-dimensional, a straightforward extension of the method was obtained by targeting the $n$ marginal distributions of the data. This approach is computationally infeasible and ineffective in high-dimensional spaces. To overcome this, we extend the 1D approach using random projections and define a new loss function suited for multivariate data, keeping problems tractable by adjusting the number of projections. We assess its performance in multidimensional generative modeling and explore its potential as a pretraining technique for generative adversarial networks (GANs) to prevent mode collapse, reporting promising results and highlighting its robustness across various hyperparameter settings.",
        "published": "2024-10-31 04:00:00",
        "id": "63261ab2-4348-4138-b377-676e1d9b72ae",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章基于不变统计损失（ISL）方法，通过让生成器使用广义帕累托分布（GPD）的输入噪声来处理重尾数据分布，用随机投影扩展一维ISL方法以处理多元数据，评估其在多维生成建模中的性能及作为预训练技术防止模式崩溃的潜力。"
        },
        "tokens": 972
    },
    {
        "title": "Exploiting Semantic Scene Reconstruction for Estimating Building Envelope Characteristics",
        "link": "https://arxiv.org/abs/2410.22383",
        "description": "arXiv:2410.22383v1 Announce Type: new \nAbstract: Achieving the EU's climate neutrality goal requires retrofitting existing buildings to reduce energy use and emissions. A critical step in this process is the precise assessment of geometric building envelope characteristics to inform retrofitting decisions. Previous methods for estimating building characteristics, such as window-to-wall ratio, building footprint area, and the location of architectural elements, have primarily relied on applying deep-learning-based detection or segmentation techniques on 2D images. However, these approaches tend to focus on planar facade properties, limiting their accuracy and comprehensiveness when analyzing complete building envelopes in 3D.\n  While neural scene representations have shown exceptional performance in indoor scene reconstruction, they remain under-explored for external building envelope analysis. This work addresses this gap by leveraging cutting-edge neural surface reconstruction techniques based on signed distance function (SDF) representations for 3D building analysis. We propose BuildNet3D, a novel framework to estimate geometric building characteristics from 2D image inputs. By integrating SDF-based representation with semantic modality, BuildNet3D recovers fine-grained 3D geometry and semantics of building envelopes, which are then used to automatically extract building characteristics. Our framework is evaluated on a range of complex building structures, demonstrating high accuracy and generalizability in estimating window-to-wall ratio and building footprint. The results underscore the effectiveness of BuildNet3D for practical applications in building analysis and retrofitting.",
        "published": "2024-10-31 04:00:00",
        "id": "70398bef-8adf-4517-b9f8-c70ae5c1f169",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出BuildNet3D框架，利用基于符号距离函数表示的神经表面重建技术，从2D图像输入估计建筑几何特征，经复杂建筑结构评估，在估计窗墙比和建筑占地面积方面准确性和通用性高。"
        },
        "tokens": 894
    },
    {
        "title": "Mobile Phone Application Data for Activity Plan Generation",
        "link": "https://arxiv.org/abs/2410.22386",
        "description": "arXiv:2410.22386v1 Announce Type: new \nAbstract: Activity-based models in transport are crucial for providing a comprehensive and realistic understanding of individuals' activity-travel patterns. Traditionally, travel surveys have been used to develop these models, but they are often costly and have small sample sizes. Mobile phone application data, one example of emerging data sources, offers an alternative with wider population coverage over extended periods for developing activity-based models. However, the challenges of using these data include sampling biases in the population coverage and individual-level data sparsity due to intermittent and irregular data collection. To synthesise activity-travel plans, we propose a novel model that combines mobile phone application data with travel survey data, addressing their limitations. Our generative model simulates multiple average weekday activity schedules for over 263,000 individuals living in Sweden, approximately 2.6% of Sweden's population. We also introduce a temporal-score approach to improve home and work location identification approaches. We assess the model's performance against an existing large-scale agent-based model of Sweden (SySMo) and a dummy model using only mobile application data. The generated activity-travel plans are comparable to the SySMo model's output and significantly surpass the dummy model's results, suggesting the proposed model's capability to generate reasonable activity-travel schedules. The proposed model is adaptable to other regions with similar travel surveys and emerging data sources, like call detail records, advancing the use of these data for activity-based models in a cost-effective, easily updated manner.",
        "published": "2024-10-31 04:00:00",
        "id": "80f6d625-002c-4fef-8bd0-63c394d4e225",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种结合手机应用数据和旅行调查数据的新模型来合成活动旅行计划，通过模拟瑞典超26.3万人的工作日活动安排评估模型性能，结果显示该模型能生成合理的活动旅行计划且具有可移植性。"
        },
        "tokens": 909
    },
    {
        "title": "Embedding Watermarks in Diffusion Process for Model Intellectual Property Protection",
        "link": "https://arxiv.org/abs/2410.22445",
        "description": "arXiv:2410.22445v1 Announce Type: new \nAbstract: In practical application, the widespread deployment of diffusion models often necessitates substantial investment in training. As diffusion models find increasingly diverse applications, concerns about potential misuse highlight the imperative for robust intellectual property protection. Current protection strategies either employ backdoor-based methods, integrating a watermark task as a simpler training objective with the main model task, or embedding watermarks directly into the final output samples. However, the former approach is fragile compared to existing backdoor defense techniques, while the latter fundamentally alters the expected output. In this work, we introduce a novel watermarking framework by embedding the watermark into the whole diffusion process, and theoretically ensure that our final output samples contain no additional information. Furthermore, we utilize statistical algorithms to verify the watermark from internally generated model samples without necessitating triggers as conditions. Detailed theoretical analysis and experimental validation demonstrate the effectiveness of our proposed method.",
        "published": "2024-10-31 04:00:00",
        "id": "1f8afa0f-c4d8-4d80-b3cb-b3f59980d058",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对扩散模型在实际应用中需大量投入训练且存在知识产权保护需求，本文提出将水印嵌入整个扩散过程的新水印框架，理论确保最终输出样本无额外信息，可利用统计算法验证水印且无需触发条件，理论分析和实验验证了方法的有效性。"
        },
        "tokens": 794
    },
    {
        "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks",
        "link": "https://arxiv.org/abs/2410.22391",
        "description": "arXiv:2410.22391v1 Announce Type: new \nAbstract: In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.",
        "published": "2024-10-31 04:00:00",
        "id": "cb335388-2d7f-4af8-a011-b9f950124b10",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出以xLSTM为核心的大型循环动作模型LRAM，在6个领域的432个任务实验中，LRAM在性能和速度上优于Transformer，其具有线性时间推理复杂度和自然序列长度外推能力。"
        },
        "tokens": 809
    },
    {
        "title": "AAAR-1.0: Assessing AI's Potential to Assist Research",
        "link": "https://arxiv.org/abs/2410.22394",
        "description": "arXiv:2410.22394v1 Announce Type: new \nAbstract: Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.",
        "published": "2024-10-31 04:00:00",
        "id": "40f12872-6ffb-4a6d-a4a4-7689403a327f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "AAAR - 1.0是一个基准数据集，用于评估大型语言模型在三项研究任务中的性能，对开源和专有模型的评估揭示了其在研究任务中的潜力和局限，该数据集将持续迭代。"
        },
        "tokens": 871
    },
    {
        "title": "Power side-channel leakage localization through adversarial training of deep neural networks",
        "link": "https://arxiv.org/abs/2410.22425",
        "description": "arXiv:2410.22425v1 Announce Type: new \nAbstract: Supervised deep learning has emerged as an effective tool for carrying out power side-channel attacks on cryptographic implementations. While increasingly-powerful deep learning-based attacks are regularly published, comparatively-little work has gone into using deep learning to defend against these attacks. In this work we propose a technique for identifying which timesteps in a power trace are responsible for leaking a cryptographic key, through an adversarial game between a deep learning-based side-channel attacker which seeks to classify a sensitive variable from the power traces recorded during encryption, and a trainable noise generator which seeks to thwart this attack by introducing a minimal amount of noise into the power traces. We demonstrate on synthetic datasets that our method can outperform existing techniques in the presence of common countermeasures such as Boolean masking and trace desynchronization. Results on real datasets are weak because the technique is highly sensitive to hyperparameters and early-stop point, and we lack a holdout dataset with ground truth knowledge of leaking points for model selection. Nonetheless, we believe our work represents an important first step towards deep side-channel leakage localization without relying on strong assumptions about the implementation or the nature of its leakage. An open-source PyTorch implementation of our experiments is provided.",
        "published": "2024-10-31 04:00:00",
        "id": "54386413-f141-40bc-873d-bbfb1e75074e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种通过深度学习中的对抗训练定位电源侧信道泄漏的技术，在合成数据集上效果较好，但在真实数据集上受超参数和早停点影响效果不佳，提供了开源实现。"
        },
        "tokens": 841
    },
    {
        "title": "FNDEX: Fake News and Doxxing Detection with Explainable AI",
        "link": "https://arxiv.org/abs/2410.22390",
        "description": "arXiv:2410.22390v1 Announce Type: new \nAbstract: The widespread and diverse online media platforms and other internet-driven communication technologies have presented significant challenges in defining the boundaries of freedom of expression. Consequently, the internet has been transformed into a potential cyber weapon. Within this evolving landscape, two particularly hazardous phenomena have emerged: fake news and doxxing. Although these threats have been subjects of extensive scholarly analysis, the crossroads where they intersect remain unexplored. This research addresses this convergence by introducing a novel system. The Fake News and Doxxing Detection with Explainable Artificial Intelligence (FNDEX) system leverages the capabilities of three distinct transformer models to achieve high-performance detection for both fake news and doxxing. To enhance data security, a rigorous three-step anonymization process is employed, rooted in a pattern-based approach for anonymizing personally identifiable information. Finally, this research emphasizes the importance of generating coherent explanations for the outcomes produced by both detection models. Our experiments on realistic datasets demonstrate that our system significantly outperforms the existing baselines",
        "published": "2024-10-31 04:00:00",
        "id": "0ba189ea-58ae-4614-9990-13359856877f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一种利用三种变压器模型检测假新闻和人肉搜索的FNDEX系统，采用三步匿名化过程确保数据安全，实验表明该系统优于现有基准。"
        },
        "tokens": 796
    },
    {
        "title": "Do Large Language Models Align with Core Mental Health Counseling Competencies?",
        "link": "https://arxiv.org/abs/2410.22446",
        "description": "arXiv:2410.22446v1 Announce Type: new \nAbstract: The rapid evolution of Large Language Models (LLMs) offers promising potential to alleviate the global scarcity of mental health professionals. However, LLMs' alignment with essential mental health counseling competencies remains understudied. We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating LLMs across five key mental health counseling competencies. Testing 22 general-purpose and medical-finetuned LLMs, we find frontier models exceed minimum thresholds but fall short of expert-level performance, with significant variations: they excel in Intake, Assessment & Diagnosis yet struggle with Core Counseling Attributes and Professional Practice & Ethics. Medical LLMs surprisingly underperform generalist models accuracy-wise, while at the same time producing slightly higher-quality justifications but making more context-related errors. Our findings highlight the complexities of developing AI systems for mental health counseling, particularly for competencies requiring empathy and contextual understanding. We found that frontier LLMs perform at a level exceeding the minimal required level of aptitude for all key mental health counseling competencies, but fall short of expert-level performance, and that current medical LLMs do not significantly improve upon generalist models in mental health counseling competencies. This underscores the critical need for specialized, mental health counseling-specific fine-tuned LLMs that rigorously aligns with core competencies combined with appropriate human supervision before any responsible real-world deployment can be considered.",
        "published": "2024-10-31 04:00:00",
        "id": "505c57d8-12dc-4305-8244-7862e9aee382",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "研究通过CounselingBench基准测试22个通用和医疗微调的大型语言模型，发现前沿模型超最低阈值但未达专家水平且差异大，医疗模型准确率低于通用模型，强调开发心理健康咨询AI系统的复杂性及需专门针对心理健康咨询微调的模型并结合人类监管。"
        },
        "tokens": 897
    },
    {
        "title": "Addressing Issues with Working Memory in Video Object Segmentation",
        "link": "https://arxiv.org/abs/2410.22451",
        "description": "arXiv:2410.22451v1 Announce Type: new \nAbstract: Contemporary state-of-the-art video object segmentation (VOS) models compare incoming unannotated images to a history of image-mask relations via affinity or cross-attention to predict object masks. We refer to the internal memory state of the initial image-mask pair and past image-masks as a working memory buffer. While the current state of the art models perform very well on clean video data, their reliance on a working memory of previous frames leaves room for error. Affinity-based algorithms include the inductive bias that there is temporal continuity between consecutive frames. To account for inconsistent camera views of the desired object, working memory models need an algorithmic modification that regulates the memory updates and avoid writing irrelevant frames into working memory. A simple algorithmic change is proposed that can be applied to any existing working memory-based VOS model to improve performance on inconsistent views, such as sudden camera cuts, frame interjections, and extreme context changes. The resulting model performances show significant improvement on video data with these frame interjections over the same model without the algorithmic addition. Our contribution is a simple decision function that determines whether working memory should be updated based on the detection of sudden, extreme changes and the assumption that the object is no longer in frame. By implementing algorithmic changes, such as this, we can increase the real-world applicability of current VOS models.",
        "published": "2024-10-31 04:00:00",
        "id": "09040a1f-0dc0-44c4-8eb3-bd4a3a12cfd2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种简单算法改进，可应用于现有基于工作记忆的视频对象分割模型，以提升在不一致视图下的性能，从而增加模型的现实适用性。"
        },
        "tokens": 865
    },
    {
        "title": "Brain age identification from diffusion MRI synergistically predicts neurodegenerative disease",
        "link": "https://arxiv.org/abs/2410.22454",
        "description": "arXiv:2410.22454v1 Announce Type: new \nAbstract: Estimated brain age from magnetic resonance image (MRI) and its deviation from chronological age can provide early insights into potential neurodegenerative diseases, supporting early detection and implementation of prevention strategies. Diffusion MRI (dMRI), a widely used modality for brain age estimation, presents an opportunity to build an earlier biomarker for neurodegenerative disease prediction because it captures subtle microstructural changes that precede more perceptible macrostructural changes. However, the coexistence of macro- and micro-structural information in dMRI raises the question of whether current dMRI-based brain age estimation models are leveraging the intended microstructural information or if they inadvertently rely on the macrostructural information. To develop a microstructure-specific brain age, we propose a method for brain age identification from dMRI that minimizes the model's use of macrostructural information by non-rigidly registering all images to a standard template. Imaging data from 13,398 participants across 12 datasets were used for the training and evaluation. We compare our brain age models, trained with and without macrostructural information minimized, with an architecturally similar T1-weighted (T1w) MRI-based brain age model and two state-of-the-art T1w MRI-based brain age models that primarily use macrostructural information. We observe difference between our dMRI-based brain age and T1w MRI-based brain age across stages of neurodegeneration, with dMRI-based brain age being older than T1w MRI-based brain age in participants transitioning from cognitively normal (CN) to mild cognitive impairment (MCI), but younger in participants already diagnosed with Alzheimer's disease (AD). Approximately 4 years before MCI diagnosis, dMRI-based brain age yields better performance than T1w MRI-based brain ages in predicting transition from CN to MCI.",
        "published": "2024-10-31 04:00:00",
        "id": "1247e2d0-e088-4234-8bc3-4dfb29244b3a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种从扩散磁共振成像（dMRI）识别脑年龄的方法，使用12个数据集的13398名参与者的成像数据进行训练和评估，比较了基于dMRI和基于T1加权（T1w）MRI的脑年龄模型在预测神经退行性疾病方面的差异。"
        },
        "tokens": 988
    },
    {
        "title": "Image2Struct: Benchmarking Structure Extraction for Vision-Language Models",
        "link": "https://arxiv.org/abs/2410.22456",
        "description": "arXiv:2410.22456v1 Announce Type: new \nAbstract: We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images. Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data. In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot). The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score. This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures. We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention. We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs. Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty. For transparency, we release the full results at https://crfm.stanford.edu/helm/image2struct/v1.0.1/.",
        "published": "2024-10-31 04:00:00",
        "id": "367be201-6cf1-4700-b4a0-0c3f7ee66338",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Image2Struct基准，用于评估视觉 - 语言模型（VLMs）从图像中提取结构的能力，包含三个领域，用五种图像度量自动评估14个著名VLMs，结果有较大差异且不同领域最佳得分不同，已公开全部结果。"
        },
        "tokens": 960
    },
    {
        "title": "Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset",
        "link": "https://arxiv.org/abs/2410.22457",
        "description": "arXiv:2410.22457v1 Announce Type: new \nAbstract: Advancements in Large Language Models (LLMs) are revolutionizing the development of autonomous agentic systems by enabling dynamic, context-aware task decomposition and automated tool selection. These sophisticated systems possess significant automation potential across various industries, managing complex tasks, interacting with external systems to enhance knowledge, and executing actions independently. This paper presents three primary contributions to advance this field:\n  - Advanced Agentic Framework: A system that handles multi-hop queries, generates and executes task graphs, selects appropriate tools, and adapts to real-time changes.\n  - Novel Evaluation Metrics: Introduction of Node F1 Score, Structural Similarity Index (SSI), and Tool F1 Score to comprehensively assess agentic systems.\n  - Specialized Dataset: Development of an AsyncHow-based dataset for analyzing agent behavior across different task complexities.\n  Our findings reveal that asynchronous and dynamic task graph decomposition significantly enhances system responsiveness and scalability, particularly for complex, multi-step tasks. Detailed analysis shows that structural and node-level metrics are crucial for sequential tasks, while tool-related metrics are more important for parallel tasks. Specifically, the Structural Similarity Index (SSI) is the most significant predictor of performance in sequential tasks, and the Tool F1 Score is essential for parallel tasks. These insights highlight the need for balanced evaluation methods that capture both structural and operational dimensions of agentic systems. Additionally, our evaluation framework, validated through empirical analysis and statistical testing, provides valuable insights for improving the adaptability and reliability of agentic systems in dynamic environments.",
        "published": "2024-10-31 04:00:00",
        "id": "052888ba-b54c-4f63-964f-837c7471f6cd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出三个贡献以推动自主智能体系统发展，包括高级智能体框架、新评估指标和专门数据集，并得出异步动态任务图分解可提升系统性能等结论。"
        },
        "tokens": 914
    },
    {
        "title": "A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents",
        "link": "https://arxiv.org/abs/2410.22476",
        "description": "arXiv:2410.22476v1 Announce Type: new \nAbstract: In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "2f32716c-7fd4-482a-9ce6-3c143e262f9b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于指针网络的架构MLMCID，用于提取意图跨度和检测多意图，构建新数据集MLMCID - dataset，分析表明该系统在多数据集上优于基线方法。"
        },
        "tokens": 792
    },
    {
        "title": "Faster Algorithms for Average-Case Orthogonal Vectors and Closest Pair Problems",
        "link": "https://arxiv.org/abs/2410.22477",
        "description": "arXiv:2410.22477v1 Announce Type: new \nAbstract: We study the average-case version of the Orthogonal Vectors problem, in which one is given as input $n$ vectors from $\\{0,1\\}^d$ which are chosen randomly so that each coordinate is $1$ independently with probability $p$. Kane and Williams [ITCS 2019] showed how to solve this problem in time $O(n^{2 - \\delta_p})$ for a constant $\\delta_p > 0$ that depends only on $p$. However, it was previously unclear how to solve the problem faster in the hardest parameter regime where $p$ may depend on $d$.\n  The best prior algorithm was the best worst-case algorithm by Abboud, Williams and Yu [SODA 2014], which in dimension $d = c \\cdot \\log n$, solves the problem in time $n^{2 - \\Omega(1/\\log c)}$. In this paper, we give a new algorithm which improves this to $n^{2 - \\Omega(\\log\\log c /\\log c)}$ in the average case for any parameter $p$.\n  As in the prior work, our algorithm uses the polynomial method. We make use of a very simple polynomial over the reals, and use a new method to analyze its performance based on computing how its value degrades as the input vectors get farther from orthogonal.\n  To demonstrate the generality of our approach, we also solve the average-case version of the closest pair problem in the same running time.",
        "published": "2024-10-31 04:00:00",
        "id": "8862c74d-10d1-43a4-ba6d-c0dbcf518dd1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究平均情况正交向量问题，给出新算法改进之前的结果，并用同样方法解决平均情况最接近对问题。"
        },
        "tokens": 908
    },
    {
        "title": "Learning Identifiable Factorized Causal Representations of Cellular Responses",
        "link": "https://arxiv.org/abs/2410.22472",
        "description": "arXiv:2410.22472v1 Announce Type: new \nAbstract: The study of cells and their responses to genetic or chemical perturbations promises to accelerate the discovery of therapeutic targets. However, designing adequate and insightful models for such data is difficult because the response of a cell to perturbations essentially depends on its biological context (e.g., genetic background or cell type). For example, while discovering therapeutic targets, one may want to enrich for drugs that specifically target a certain cell type. This challenge emphasizes the need for methods that explicitly take into account potential interactions between drugs and contexts. Towards this goal, we propose a novel Factorized Causal Representation (FCR)\n  learning method that reveals causal structure in single-cell perturbation data from several cell lines. Based on the framework of identifiable deep generative models, FCR learns multiple cellular representations that are disentangled, comprised of covariate-specific ($\\mathbf{z}_x$), treatment-specific ($\\mathbf{z}_{t}$), and interaction-specific ($\\mathbf{z}_{tx}$) blocks. Based on recent advances in non-linear ICA theory, we prove the component-wise identifiability of $\\mathbf{z}_{tx}$ and block-wise identifiability of $\\mathbf{z}_t$ and $\\mathbf{z}_x$. Then, we present our implementation of FCR, and empirically demonstrate that it outperforms state-of-the-art baselines in various tasks across four single-cell datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "77b010a4-fc57-4516-b2df-e60724a6b9f2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的因子化因果表示（FCR）学习方法，可揭示单细胞扰动数据中的因果结构，证明了相关组件的可识别性，并在四个单细胞数据集的各种任务中优于现有基线。"
        },
        "tokens": 888
    },
    {
        "title": "The State of Data Curation at NeurIPS: An Assessment of Dataset Development Practices in the Datasets and Benchmarks Track",
        "link": "https://arxiv.org/abs/2410.22473",
        "description": "arXiv:2410.22473v1 Announce Type: new \nAbstract: Data curation is a field with origins in librarianship and archives, whose scholarship and thinking on data issues go back centuries, if not millennia. The field of machine learning is increasingly observing the importance of data curation to the advancement of both applications and fundamental understanding of machine learning models - evidenced not least by the creation of the Datasets and Benchmarks track itself. This work provides an analysis of dataset development practices at NeurIPS through the lens of data curation. We present an evaluation framework for dataset documentation, consisting of a rubric and toolkit developed through a literature review of data curation principles. We use the framework to assess the strengths and weaknesses in current dataset development practices of 60 datasets published in the NeurIPS Datasets and Benchmarks track from 2021-2023. We summarize key findings and trends. Results indicate greater need for documentation about environmental footprint, ethical considerations, and data management. We suggest targeted strategies and resources to improve documentation in these areas and provide recommendations for the NeurIPS peer-review process that prioritize rigorous data curation in ML. Finally, we provide results in the format of a dataset that showcases aspects of recommended data curation practices. Our rubric and results are of interest for improving data curation practices broadly in the field of ML as well as to data curation and science and technology studies scholars studying practices in ML. Our aim is to support continued improvement in interdisciplinary research on dataset practices, ultimately improving the reusability and reproducibility of new datasets and benchmarks, enabling standardized and informed human oversight, and strengthening the foundation of rigorous and responsible ML research.",
        "published": "2024-10-31 04:00:00",
        "id": "2e81d5b3-3e4c-4020-9cc4-096c89219c46",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文通过数据管理的视角分析NeurIPS数据集开发实践，提出评估框架并评估2021 - 2023年NeurIPS数据集和基准测试轨道中的60个数据集，指出需要改进之处并给出建议。"
        },
        "tokens": 954
    },
    {
        "title": "Unleashing Multicore Strength for Efficient Execution of Transactions",
        "link": "https://arxiv.org/abs/2410.22460",
        "description": "arXiv:2410.22460v1 Announce Type: new \nAbstract: Blockchain technology is booming up the digital world in recent days and thus paved a way for creating separate blockchain network for various industries. This technology is characterized by its distributed, decentralized, and immutable ledger system which serves as a fundamental platform for managing smart contract transactions (SCTs). However, these self-executing codes implemented using blockchains undergo sequential validation within a block which introduces performance bottlenecks. In response, this paper introduces a framework called the Multi-Bin Parallel Scheduler (MBPS) designed for parallelizing blockchain smart contract transactions to leverage the capabilities of multicore systems. Our proposed framework facilitates concurrent execution of SCTs, enhancing performance by allowing non-conflicting transactions to be processed simultaneously while preserving deterministic order. The framework comprises of three vital stages: conflict detection, bin creation and execution. We conducted an evaluation of our MBPS framework in Hyperledger Sawtooth v1.2.6, revealing substantial performance enhancements compared to existing parallel SCT execution frameworks across various smart contract applications. This research contributes to the ongoing optimization efforts in blockchain technology demonstrating its potential for scalability and efficiency in real-world scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "6fd24c43-2308-44e9-9287-442ef1050afb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出名为Multi - Bin Parallel Scheduler (MBPS)的框架用于并行化区块链智能合约交易，在Hyperledger Sawtooth v1.2.6中的评估显示该框架相比现有框架在性能上有显著提升。"
        },
        "tokens": 836
    },
    {
        "title": "Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection",
        "link": "https://arxiv.org/abs/2410.22461",
        "description": "arXiv:2410.22461v1 Announce Type: new \nAbstract: Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (\\ie, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (\\ie, 1$\\%$ and 5$\\%)$, while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.",
        "published": "2024-10-31 04:00:00",
        "id": "bf3e30f9-b7cd-4c68-ba0a-34ce135cc0b2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出统一域泛化和自适应（UDGA）方法，通过多视图重叠深度约束缓解源和目标域间几何差距，用标签高效域自适应方法处理少量标签的陌生目标，在nuScenes、Lyft和Waymo大规模基准测试中性能优于现有方法。"
        },
        "tokens": 887
    },
    {
        "title": "Designing robot swarms: a puzzle, a problem, and a mess",
        "link": "https://arxiv.org/abs/2410.22478",
        "description": "arXiv:2410.22478v1 Announce Type: new \nAbstract: Framing an issue as a puzzle, problem, or mess is an illustrative approach to characterizing the issue's complexity within organizational theory and systems thinking. We use this approach to characterize the issue of designing collective behaviors for robot swarms and discuss how various research goals have shaped the current state of the field. We contextualize our discussion at these three levels by highlighting relevant literature. Our aim is to emphasize key challenges that arise in the development of robot swarms for real-world applications and to motivate further work on promising research directions.",
        "published": "2024-10-31 04:00:00",
        "id": "f25a4a23-e942-420f-bea7-43023000bc6e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文将设计机器人群体的集体行为问题以谜题、问题和混乱的角度进行分析，通过相关文献在这三个层面展开讨论，旨在强调机器人群体在实际应用开发中的关键挑战并激励进一步研究。"
        },
        "tokens": 716
    },
    {
        "title": "Heterogeneous Team Coordination on Partially Observable Graphs with Realistic Communication",
        "link": "https://arxiv.org/abs/2410.22482",
        "description": "arXiv:2410.22482v1 Announce Type: new \nAbstract: Team Coordination on Graphs with Risky Edges (\\textsc{tcgre}) is a recently proposed problem, in which robots find paths to their goals while considering possible coordination to reduce overall team cost. However, \\textsc{tcgre} assumes that the \\emph{entire} environment is available to a \\emph{homogeneous} robot team with \\emph{ubiquitous} communication. In this paper, we study an extended version of \\textsc{tcgre}, called \\textsc{hpr-tcgre}, with three relaxations: Heterogeneous robots, Partial observability, and Realistic communication. To this end, we form a new combinatorial optimization problem on top of \\textsc{tcgre}. After analysis, we divide it into two sub-problems, one for robots moving individually, another for robots in groups, depending on their communication availability. Then, we develop an algorithm that exploits real-time partial maps to solve local shortest path(s) problems, with a A*-like sub-goal(s) assignment mechanism that explores potential coordination opportunities for global interests. Extensive experiments indicate that our algorithm is able to produce team coordination behaviors in order to reduce overall cost even with our three relaxations.",
        "published": "2024-10-31 04:00:00",
        "id": "7a5e91e6-fe2b-45e6-ba30-697b2338fd76",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文研究了一种扩展版的图上团队协调问题（HPR - TCGRE），做出三种放宽假设，分析后将其分为两个子问题并开发算法，实验表明该算法能在这些放宽条件下降低整体成本。"
        },
        "tokens": 862
    },
    {
        "title": "Developing Convolutional Neural Networks using a Novel Lamarckian Co-Evolutionary Algorithm",
        "link": "https://arxiv.org/abs/2410.22487",
        "description": "arXiv:2410.22487v1 Announce Type: new \nAbstract: Neural Architecture Search (NAS) methods autonomously discover high-accuracy neural network architectures, outperforming manually crafted ones. However, The NAS methods require high computational costs due to the high dimension search space and the need to train multiple candidate solutions. This paper introduces LCoDeepNEAT, an instantiation of Lamarckian genetic algorithms, which extends the foundational principles of the CoDeepNEAT framework. LCoDeepNEAT co-evolves CNN architectures and their respective final layer weights. The evaluation process of LCoDeepNEAT entails a single epoch of SGD, followed by the transference of the acquired final layer weights to the genetic representation of the network. In addition, it expedites the process of evolving by imposing restrictions on the architecture search space, specifically targeting architectures comprising just two fully connected layers for classification. Our method yields a notable improvement in the classification accuracy of candidate solutions throughout the evolutionary process, ranging from 2% to 5.6%. This outcome underscores the efficacy and effectiveness of integrating gradient information and evolving the last layer of candidate solutions within LCoDeepNEAT. LCoDeepNEAT is assessed across six standard image classification datasets and benchmarked against eight leading NAS methods. Results demonstrate LCoDeepNEAT's ability to swiftly discover competitive CNN architectures with fewer parameters, conserving computational resources, and achieving superior classification accuracy compared to other approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "3a932761-1ebc-4ad0-b83a-56fd0d56275e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出LCoDeepNEAT（拉马克遗传算法实例）扩展CoDeepNEAT框架，共同进化CNN架构及其最终层权重，限制架构搜索空间加速进化过程，经六个标准图像分类数据集评估和与八个NAS方法对比，能快速发现有竞争力的CNN架构，节省计算资源并提高分类精度。"
        },
        "tokens": 913
    },
    {
        "title": "Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation",
        "link": "https://arxiv.org/abs/2410.22489",
        "description": "arXiv:2410.22489v1 Announce Type: new \nAbstract: Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code is available at https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot .",
        "published": "2024-10-31 04:00:00",
        "id": "4452fcd6-a28e-46b4-98fa-07d9b47b3adc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种无成本的多模态少样本3D点云语义分割设置，介绍多模态少样本分割网络MM - FSS，通过模块和技术利用多模态信息提升性能，在相关数据集上取得显著成果。"
        },
        "tokens": 923
    },
    {
        "title": "The PV-ALE Dataset: Enhancing Apple Leaf Disease Classification Through Transfer Learning with Convolutional Neural Networks",
        "link": "https://arxiv.org/abs/2410.22490",
        "description": "arXiv:2410.22490v1 Announce Type: new \nAbstract: As the global food security landscape continues to evolve, the need for accurate and reliable crop disease diagnosis has never been more pressing. To address global food security concerns, we extend the widely used PlantVillage dataset with additional apple leaf disease classes, enhancing diversity and complexity. Experimental evaluations on both original and extended datasets reveal that existing models struggle with the new additions, highlighting the need for more robust and generalizable computer vision models. Test F1 scores of 99.63% and 97.87% were obtained on the original and extended datasets, respectively. Our study provides a more challenging and diverse benchmark, paving the way for the development of accurate and reliable models for identifying apple leaf diseases under varying imaging conditions. The expanded dataset is available at https://www.kaggle.com/datasets/akinyemijoseph/apple-leaf-disease-dataset-6-classes-v2 enabling future research to build upon our findings.",
        "published": "2024-10-31 04:00:00",
        "id": "0fa5a514-af56-46e5-b307-0c513eee619f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "通过扩展PlantVillage数据集增加苹果叶病类别，实验评估发现现有模型应对新类别存在困难，新数据集可助力后续研究开发识别苹果叶病的准确可靠模型。"
        },
        "tokens": 797
    },
    {
        "title": "RealCQA-V2 : Visual Premise Proving",
        "link": "https://arxiv.org/abs/2410.22492",
        "description": "arXiv:2410.22492v1 Announce Type: new \nAbstract: We introduce Visual Premise Proving (VPP), a novel task tailored to refine the process of chart question answering by deconstructing it into a series of logical premises. Each of these premises represents an essential step in comprehending a chart's content and deriving logical conclusions, thereby providing a granular look at a model's reasoning abilities. This approach represents a departure from conventional accuracy-based evaluation methods, emphasizing the model's ability to sequentially validate each premise and ideally mimic human analytical processes. A model adept at reasoning is expected to demonstrate proficiency in both data retrieval and the structural understanding of charts, suggesting a synergy between these competencies. However, in our zero-shot study using the sophisticated MATCHA model on a scientific chart question answering dataset, an intriguing pattern emerged. The model showcased superior performance in chart reasoning (27\\%) over chart structure (19\\%) and data retrieval (14\\%). This performance gap suggests that models might more readily generalize reasoning capabilities across datasets, benefiting from consistent mathematical and linguistic semantics, even when challenged by changes in the visual domain that complicate structure comprehension and data retrieval. Furthermore, the efficacy of using accuracy of binary QA for evaluating chart reasoning comes into question if models can deduce correct answers without parsing chart data or structure. VPP highlights the importance of integrating reasoning with visual comprehension to enhance model performance in chart analysis, pushing for a balanced approach in evaluating visual data interpretation capabilities.",
        "published": "2024-10-31 04:00:00",
        "id": "89d2cc33-6425-4b4c-88d9-5d58e664642f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍视觉前提证明（VPP）任务，将图表问答过程分解为逻辑前提以细化该过程，零样本研究中MATCHA模型在科学图表问答数据集上推理表现优于结构理解和数据检索，这表明模型可能更易跨数据集推广推理能力，VPP强调将推理与视觉理解相结合对提升模型在图表分析中的性能的重要性。"
        },
        "tokens": 925
    },
    {
        "title": "Unlocking Point Processes through Point Set Diffusion",
        "link": "https://arxiv.org/abs/2410.22493",
        "description": "arXiv:2410.22493v1 Announce Type: new \nAbstract: Point processes model the distribution of random point sets in mathematical spaces, such as spatial and temporal domains, with applications in fields like seismology, neuroscience, and economics. Existing statistical and machine learning models for point processes are predominantly constrained by their reliance on the characteristic intensity function, introducing an inherent trade-off between efficiency and flexibility. In this paper, we introduce Point Set Diffusion, a diffusion-based latent variable model that can represent arbitrary point processes on general metric spaces without relying on the intensity function. By directly learning to stochastically interpolate between noise and data point sets, our approach enables efficient, parallel sampling and flexible generation for complex conditional tasks defined on the metric space. Experiments on synthetic and real-world datasets demonstrate that Point Set Diffusion achieves state-of-the-art performance in unconditional and conditional generation of spatial and spatiotemporal point processes while providing up to orders of magnitude faster sampling than autoregressive baselines.",
        "published": "2024-10-31 04:00:00",
        "id": "f0caaf4c-fce3-4638-9731-9715b5d650aa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Point Set Diffusion，一种基于扩散的潜变量模型，可在不依赖强度函数的情况下表示一般度量空间上的任意点过程，实验表明其在时空点过程的生成任务上性能达到先进水平且采样速度更快。"
        },
        "tokens": 793
    },
    {
        "title": "Anticipating Future with Large Language Model for Simultaneous Machine Translation",
        "link": "https://arxiv.org/abs/2410.22499",
        "description": "arXiv:2410.22499v1 Announce Type: new \nAbstract: Simultaneous machine translation (SMT) takes streaming input utterances and incrementally produces target text. Existing SMT methods only use the partial utterance that has already arrived at the input and the generated hypothesis. Motivated by human interpreters' technique to forecast future words before hearing them, we propose $\\textbf{T}$ranslation by $\\textbf{A}$nticipating $\\textbf{F}$uture (TAF), a method to improve translation quality while retraining low latency. Its core idea is to use a large language model (LLM) to predict future source words and opportunistically translate without introducing too much risk. We evaluate our TAF and multiple baselines of SMT on four language directions. Experiments show that TAF achieves the best translation quality-latency trade-off and outperforms the baselines by up to 5 BLEU points at the same latency (three words).",
        "published": "2024-10-31 04:00:00",
        "id": "a3090120-8d75-4c07-b34f-42f60c916604",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为TAF（Translation by Anticipating Future）的方法，利用大型语言模型预测未来源词以提高同步机器翻译质量并保持低延迟，在四个语言方向上的实验显示其在翻译质量 - 延迟权衡方面表现最佳。"
        },
        "tokens": 806
    },
    {
        "title": "AffectNet+: A Database for Enhancing Facial Expression Recognition with Soft-Labels",
        "link": "https://arxiv.org/abs/2410.22506",
        "description": "arXiv:2410.22506v1 Announce Type: new \nAbstract: Automated Facial Expression Recognition (FER) is challenging due to intra-class variations and inter-class similarities. FER can be especially difficult when facial expressions reflect a mixture of various emotions (aka compound expressions). Existing FER datasets, such as AffectNet, provide discrete emotion labels (hard-labels), where a single category of emotion is assigned to an expression. To alleviate inter- and intra-class challenges, as well as provide a better facial expression descriptor, we propose a new approach to create FER datasets through a labeling method in which an image is labeled with more than one emotion (called soft-labels), each with different confidences. Specifically, we introduce the notion of soft-labels for facial expression datasets, a new approach to affective computing for more realistic recognition of facial expressions. To achieve this goal, we propose a novel methodology to accurately calculate soft-labels: a vector representing the extent to which multiple categories of emotion are simultaneously present within a single facial expression. Finding smoother decision boundaries, enabling multi-labeling, and mitigating bias and imbalanced data are some of the advantages of our proposed method. Building upon AffectNet, we introduce AffectNet+, the next-generation facial expression dataset. This dataset contains soft-labels, three categories of data complexity subsets, and additional metadata such as age, gender, ethnicity, head pose, facial landmarks, valence, and arousal. AffectNet+ will be made publicly accessible to researchers.",
        "published": "2024-10-31 04:00:00",
        "id": "62a3640f-a0e0-465f-9894-d8dd65bd9a7d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出通过软标签创建面部表情识别(FER)数据集的新方法以解决类内和类间挑战，基于AffectNet构建含软标签、多种数据复杂度子集和附加元数据的AffectNet+数据集。"
        },
        "tokens": 896
    },
    {
        "title": "Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models",
        "link": "https://arxiv.org/abs/2410.22517",
        "description": "arXiv:2410.22517v1 Announce Type: new \nAbstract: We explore the internal mechanisms of how bias emerges in large language models (LLMs) when provided with ambiguous comparative prompts: inputs that compare or enforce choosing between two or more entities without providing clear context for preference. Most approaches for bias mitigation focus on either post-hoc analysis or data augmentation. However, these are transient solutions, without addressing the root cause: the model itself. Numerous prior works show the influence of the attention module towards steering generations. We believe that analyzing attention is also crucial for understanding bias, as it provides insight into how the LLM distributes its focus across different entities and how this contributes to biased decisions. To this end, we first introduce a metric to quantify the LLM's preference for one entity over another. We then propose $\\texttt{ATLAS}$ (Attention-based Targeted Layer Analysis and Scaling), a technique to localize bias to specific layers of the LLM by analyzing attention scores and then reduce bias by scaling attention in these biased layers. To evaluate our method, we conduct experiments across 3 datasets (BBQ, Crows-Pairs, and WinoGender) using $\\texttt{GPT-2 XL}$ (1.5B), $\\texttt{GPT-J}$ (6B), $\\texttt{LLaMA-2}$ (7B) and $\\texttt{LLaMA-3}$ (8B). Our experiments demonstrate that bias is concentrated in the later layers, typically around the last third. We also show how $\\texttt{ATLAS}$ effectively mitigates bias through targeted interventions without compromising downstream performance and an average increase of only 0.82% in perplexity when the intervention is applied. We see an average improvement of 0.28 points in the bias score across all the datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "c01cfb2f-a73f-43b2-be13-758007f72900",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究探索大语言模型在面对模糊比较提示时产生偏差的内部机制，提出量化LLM对实体偏好的指标和ATLAS技术以定位并减少偏差，经多数据集和模型实验，证明偏差集中于后层，ATLAS可有效缓解偏差且不影响下游性能"
        },
        "tokens": 987
    },
    {
        "title": "A Demonic Outcome Logic for Randomized Nondeterminism",
        "link": "https://arxiv.org/abs/2410.22540",
        "description": "arXiv:2410.22540v1 Announce Type: new \nAbstract: Programs increasingly rely on randomization in applications such as cryptography and machine learning. Analyzing randomized programs has been a fruitful research direction, but there is a gap when programs also exploit nondeterminism (for concurrency, efficiency, or algorithmic design). In this paper, we introduce Demonic Outcome Logic for reasoning about programs that exploit both randomization and nondeterminism. The logic includes several novel features, such as reasoning about multiple executions in tandem and manipulating pre- and postconditions using familiar equational laws -- including the distributive law of probabilistic choices over nondeterministic ones. We also give rules for loops that both establish termination and quantify the distribution of final outcomes from a single premise. We illustrate the reasoning capabilities of Demonic Outcome Logic through several case studies, including the Monty Hall problem, an adversarial protocol for simulating fair coins, and a heuristic based probabilistic SAT solver.",
        "published": "2024-10-31 04:00:00",
        "id": "73ff66ca-5aa4-41a9-862f-58461fbd075c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了用于推理同时利用随机化和非确定性的程序的恶魔结果逻辑，包含新特性，给出循环规则，通过案例说明其推理能力。"
        },
        "tokens": 780
    },
    {
        "title": "FairSkin: Fair Diffusion for Skin Disease Image Generation",
        "link": "https://arxiv.org/abs/2410.22551",
        "description": "arXiv:2410.22551v1 Announce Type: new \nAbstract: Image generation is a prevailing technique for clinical data augmentation for advancing diagnostic accuracy and reducing healthcare disparities. Diffusion Model (DM) has become a leading method in generating synthetic medical images, but it suffers from a critical twofold bias: (1) The quality of images generated for Caucasian individuals is significantly higher, as measured by the Frechet Inception Distance (FID). (2) The ability of the downstream-task learner to learn critical features from disease images varies across different skin tones. These biases pose significant risks, particularly in skin disease detection, where underrepresentation of certain skin tones can lead to misdiagnosis or neglect of specific conditions. To address these challenges, we propose FairSkin, a novel DM framework that mitigates these biases through a three-level resampling mechanism, ensuring fairer representation across racial and disease categories. Our approach significantly improves the diversity and quality of generated images, contributing to more equitable skin disease detection in clinical settings.",
        "published": "2024-10-31 04:00:00",
        "id": "6e08386f-f28b-451f-911d-95174544a93b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出FairSkin这一扩散模型框架，通过三级重采样机制减轻偏差，改善生成图像的多样性和质量，助力皮肤疾病检测更加公平。"
        },
        "tokens": 785
    },
    {
        "title": "Efficient Learned Query Execution over Text and Tables [Technical Report]",
        "link": "https://arxiv.org/abs/2410.22522",
        "description": "arXiv:2410.22522v1 Announce Type: new \nAbstract: In this paper, we present ELEET, a novel execution engine that allows one to seamlessly query and process text as a first-class citizen along with tables. To enable such a seamless integration of text and tables, ELEET leverages learned multi-modal operators (MMOps) such as joins and unions that seamlessly combine structured with unstructured textual data. While large language models (LLM) such as GPT-4 are interesting candidates to enable such learned multimodal operations, we deliberately do not follow this trend to enable MMOps, since it would result in high overhead at query runtime. Instead, to enable MMOps, ELEET comes with a more efficient small language model (SLM) that is targeted to extract structured data from text. Thanks to our novel architecture and pre-training procedure, the ELEET-model enables high-accuracy extraction with low overheads. In our evaluation, we compare query execution based on ELEET to baselines leveraging LLMs such as GPT-4 and show that ELEET can speed up multi-modal queries over tables and text by up to 575x without sacrificing accuracy.",
        "published": "2024-10-31 04:00:00",
        "id": "2d49221c-acbd-4d2e-876f-754ef3ffb3c6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "ELEET是一个新的执行引擎，它使用学习到的多模态算子无缝处理文本和表格数据，采用小型语言模型从文本中提取结构化数据，在评估中比GPT - 4等大语言模型执行多模态查询速度快很多且不牺牲准确性。"
        },
        "tokens": 852
    },
    {
        "title": "From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems",
        "link": "https://arxiv.org/abs/2410.22526",
        "description": "arXiv:2410.22526v1 Announce Type: new \nAbstract: To effectively address potential harms from AI systems, it is essential to identify and mitigate system-level hazards. Current analysis approaches focus on individual components of an AI system, like training data or models, in isolation, overlooking hazards from component interactions or how they are situated within a company's development process. To this end, we draw from the established field of system safety, which considers safety as an emergent property of the entire system, not just its components. In this work, we translate System Theoretic Process Analysis (STPA) - a recognized system safety framework - for analyzing AI operation and development processes. We focus on systems that rely on machine learning algorithms and conducted STPA on three case studies involving linear regression, reinforcement learning, and transformer-based generative models. Our analysis explored how STPA's control and system-theoretic perspectives apply to AI systems and whether unique AI traits - such as model opacity, capability uncertainty, and output complexity - necessitate significant modifications to the framework. We find that the key concepts and steps of conducting an STPA readily apply, albeit with a few adaptations tailored for AI systems. We present the Process-oriented Hazard Analysis for AI Systems (PHASE) as a guideline that adapts STPA concepts for AI, making STPA-based hazard analysis more accessible. PHASE enables four key affordances for analysts responsible for managing AI system harms: 1) detection of hazards at the systems level, including those from accumulation of disparate issues; 2) explicit acknowledgment of social factors contributing to experiences of algorithmic harms; 3) creation of traceable accountability chains between harms and those who can mitigate the harm; and 4) ongoing monitoring and mitigation of new hazards.",
        "published": "2024-10-31 04:00:00",
        "id": "ecabb6f6-3a51-4fb4-aa34-22b7265d8d87",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "为有效应对AI系统潜在危害，借鉴系统安全领域的系统理论过程分析（STPA）对AI运营和开发过程进行分析，提出面向过程的AI系统危害分析（PHASE）指南。"
        },
        "tokens": 945
    },
    {
        "title": "Multimodal Structure Preservation Learning",
        "link": "https://arxiv.org/abs/2410.22520",
        "description": "arXiv:2410.22520v1 Announce Type: new \nAbstract: When selecting data to build machine learning models in practical applications, factors such as availability, acquisition cost, and discriminatory power are crucial considerations. Different data modalities often capture unique aspects of the underlying phenomenon, making their utilities complementary. On the other hand, some sources of data host structural information that is key to their value. Hence, the utility of one data type can sometimes be enhanced by matching the structure of another. We propose Multimodal Structure Preservation Learning (MSPL) as a novel method of learning data representations that leverages the clustering structure provided by one data modality to enhance the utility of data from another modality. We demonstrate the effectiveness of MSPL in uncovering latent structures in synthetic time series data and recovering clusters from whole genome sequencing and antimicrobial resistance data using mass spectrometry data in support of epidemiology applications. The results show that MSPL can imbue the learned features with external structures and help reap the beneficial synergies occurring across disparate data modalities.",
        "published": "2024-10-31 04:00:00",
        "id": "9ad52ea3-b1ea-4b24-8a6e-aa9f9bc9fb45",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出多模态结构保留学习（MSPL）方法，利用一种数据模态的聚类结构提高另一种模态数据的效用，在合成时间序列数据、全基因组测序和抗菌耐药性数据等方面展示了有效性。"
        },
        "tokens": 790
    },
    {
        "title": "Lyapunov Characterization for ISS of Impulsive Switched Systems",
        "link": "https://arxiv.org/abs/2410.22521",
        "description": "arXiv:2410.22521v1 Announce Type: new \nAbstract: In this study, we investigate the ISS of impulsive switched systems that have modes with both stable and unstable flows. We assume that the switching signal satisfies mode-dependent average dwell and leave time conditions. To establish ISS conditions, we propose two types of time-varying ISS-Lyapunov functions: one that is non-decreasing and another one that is decreasing. Our research proves that the existence of either of these ISS-Lyapunov functions is a necessary and sufficient condition for ISS. We also present a technique for constructing a decreasing ISS-Lyapunov function from a non-decreasing one, which is useful for its own sake. Our findings also have added value to previous research that only studied sufficient conditions for ISS, as our results apply to a broader class of systems. This is because we impose less restrictive dwell and leave time constraints on the switching signal and our ISS-Lyapunov functions are time-varying with general nonlinear conditions imposed on them. Moreover, we provide a method to guarantee the ISS of a particular class of impulsive switched systems when the switching signal is unknown.",
        "published": "2024-10-31 04:00:00",
        "id": "5ded3418-2eb2-4d56-8847-e6504061785a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究了具有稳定和不稳定流模式的脉冲切换系统的输入到状态稳定性（ISS），提出两种时变ISS - Lyapunov函数，证明其存在性是ISS的充要条件，并给出相关构建方法和系统ISS的保证方法。"
        },
        "tokens": 834
    },
    {
        "title": "Auto-Intent: Automated Intent Discovery and Self-Exploration for Large Language Model Web Agents",
        "link": "https://arxiv.org/abs/2410.22552",
        "description": "arXiv:2410.22552v1 Announce Type: new \nAbstract: In this paper, we introduce Auto-Intent, a method to adapt a pre-trained large language model (LLM) as an agent for a target domain without direct fine-tuning, where we empirically focus on web navigation tasks. Our approach first discovers the underlying intents from target domain demonstrations unsupervisedly, in a highly compact form (up to three words). With the extracted intents, we train our intent predictor to predict the next intent given the agent's past observations and actions. In particular, we propose a self-exploration approach where top-k probable intent predictions are provided as a hint to the pre-trained LLM agent, which leads to enhanced decision-making capabilities. Auto-Intent substantially improves the performance of GPT-{3.5, 4} and Llama-3.1-{70B, 405B} agents on the large-scale real-website navigation benchmarks from Mind2Web and online navigation tasks from WebArena with its cross-benchmark generalization from Mind2Web.",
        "published": "2024-10-31 04:00:00",
        "id": "0123c7ff-3db9-4a7e-ab39-84ce1abf74ce",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Auto-Intent方法可在无直接微调的情况下将预训练大型语言模型适配为目标领域的代理（重点为网络导航任务），先无监督发现目标领域演示中的意图，再训练意图预测器，还提出自我探索方法提升决策能力，该方法提升了多个模型在网页导航任务中的性能。"
        },
        "tokens": 839
    },
    {
        "title": "ML Research Benchmark",
        "link": "https://arxiv.org/abs/2410.22553",
        "description": "arXiv:2410.22553v1 Announce Type: new \nAbstract: Artificial intelligence agents are increasingly capable of performing complex tasks across various domains. As these agents advance, there is a growing need to accurately measure and benchmark their capabilities, particularly in accelerating AI research and development. Current benchmarks focus on general machine learning tasks, but lack comprehensive evaluation methods for assessing AI agents' abilities in tackling research-level problems and competition-level challenges in the field of AI. We present the ML Research Benchmark (MLRB), comprising 7 competition-level tasks derived from recent machine learning conference tracks. These tasks span activities typically undertaken by AI researchers, including model training efficiency, pretraining on limited data, domain specific fine-tuning, and model compression. This paper introduces a novel benchmark and evaluates it using agent scaffolds powered by frontier models, including Claude-3 and GPT-4o. The results indicate that the Claude-3.5 Sonnet agent performs best across our benchmark, excelling in planning and developing machine learning models. However, both tested agents struggled to perform non-trivial research iterations. We observed significant performance variations across tasks, highlighting the complexity of AI development and the challenges in creating versatile agent scaffolds. While current AI agents can successfully navigate complex instructions and produce baseline results, they fall short of the capabilities required for advanced AI research. The ML Research Benchmark provides a valuable framework for assessing and comparing AI agents on tasks mirroring real-world AI research challenges.",
        "published": "2024-10-31 04:00:00",
        "id": "017c0a5d-33ac-4800-8f22-2c8e9b96737e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "提出ML研究基准（MLRB），包含7个竞赛级任务，用Claude - 3和GPT - 4o评估，结果表明Claude - 3.5 Sonnet在基准测试中表现最佳，但现有AI代理难以进行非平凡的研究迭代，该基准为评估和比较AI代理提供了框架。"
        },
        "tokens": 903
    },
    {
        "title": "Remote Sensing for Weed Detection and Control",
        "link": "https://arxiv.org/abs/2410.22554",
        "description": "arXiv:2410.22554v1 Announce Type: new \nAbstract: Italian ryegrass is a grass weed commonly found in winter wheat fields that are competitive with winter wheat for moisture and nutrients. Ryegrass can cause substantial reductions in yield and grain quality if not properly controlled with the use of herbicides. To control the cost and environmental impact we detect weeds in drone and satellite imagery. Satellite imagery is too coarse to be used for precision spraying, but can aid in planning drone flights and treatments. Drone images on the other hand have sufficiently good resolution for precision spraying. However, ryegrass is hard to distinguish from the crop and annotation requires expert knowledge. We used the Python segmentation models library to test more than 600 different neural network architectures for weed segmentation in drone images and we map accuracy versus the cost of the model prediction for these. Our best system applies herbicides to over 99% of the weeds while only spraying an area 30% larger than the annotated weed area. These models yield large savings if the weed covers a small part of the field.",
        "published": "2024-10-31 04:00:00",
        "id": "84b0a764-ac23-499b-b055-212d0e7b3eee",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为控制成本和环境影响，利用无人机和卫星图像检测杂草，用Python分割模型库测试600多种神经网络架构用于无人机图像中的杂草分割，最佳系统对99%以上杂草施药且喷洒面积仅比标注杂草面积大30%，杂草覆盖小部分田地时可节省大量成本。"
        },
        "tokens": 830
    },
    {
        "title": "Unsupervised Multimodal Fusion of In-process Sensor Data for Advanced Manufacturing Process Monitoring",
        "link": "https://arxiv.org/abs/2410.22558",
        "description": "arXiv:2410.22558v1 Announce Type: new \nAbstract: Effective monitoring of manufacturing processes is crucial for maintaining product quality and operational efficiency. Modern manufacturing environments generate vast amounts of multimodal data, including visual imagery from various perspectives and resolutions, hyperspectral data, and machine health monitoring information such as actuator positions, accelerometer readings, and temperature measurements. However, interpreting this complex, high-dimensional data presents significant challenges, particularly when labeled datasets are unavailable. This paper presents a novel approach to multimodal sensor data fusion in manufacturing processes, inspired by the Contrastive Language-Image Pre-training (CLIP) model. We leverage contrastive learning techniques to correlate different data modalities without the need for labeled data, developing encoders for five distinct modalities: visual imagery, audio signals, laser position (x and y coordinates), and laser power measurements. By compressing these high-dimensional datasets into low-dimensional representational spaces, our approach facilitates downstream tasks such as process control, anomaly detection, and quality assurance. We evaluate the effectiveness of our approach through experiments, demonstrating its potential to enhance process monitoring capabilities in advanced manufacturing systems. This research contributes to smart manufacturing by providing a flexible, scalable framework for multimodal data fusion that can adapt to diverse manufacturing environments and sensor configurations.",
        "published": "2024-10-31 04:00:00",
        "id": "387df474-421a-47b8-8dbb-c5378ba8c964",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受CLIP模型启发，提出一种制造过程中多模态传感器数据融合的新方法，利用对比学习技术关联不同数据模态，通过实验评估该方法对先进制造系统过程监控能力的有效性。"
        },
        "tokens": 848
    },
    {
        "title": "Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components",
        "link": "https://arxiv.org/abs/2410.22559",
        "description": "arXiv:2410.22559v1 Announce Type: new \nAbstract: Disentanglement, or identifying salient statistically independent factors of the data, is of interest in many areas of machine learning and statistics, with relevance to synthetic data generation with controlled properties, robust classification of features, parsimonious encoding, and a greater understanding of the generative process underlying the data. Disentanglement arises in several generative paradigms, including Variational Autoencoders (VAEs), Generative Adversarial Networks and diffusion models. Particular progress has recently been made in understanding disentanglement in VAEs, where the choice of diagonal posterior covariance matrices is shown to promote mutual orthogonality between columns of the decoder's Jacobian. We continue this thread to show how this linear independence translates to statistical independence, completing the chain in understanding how the VAE's objective identifies independent components of, or disentangles, the data.",
        "published": "2024-10-31 04:00:00",
        "id": "d070cf57-2e6f-47de-88cb-832205ecd6c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文 arXiv:2410.22559v1阐述了在机器学习和统计学的许多领域受关注的解缠（disentanglement）在变分自动编码器（VAEs）中的进展，即如何将线性独立转化为统计独立，完成对VAE目标如何识别数据独立成分或解缠数据的理解。"
        },
        "tokens": 815
    },
    {
        "title": "BBR Fairness Evaluation Using NS-3",
        "link": "https://arxiv.org/abs/2410.22560",
        "description": "arXiv:2410.22560v1 Announce Type: new \nAbstract: This paper evaluates the fairness of BBR congestion control using NS-3 simulator. While BBR improves performance over loss-based methods in single flows, unfairness issues emerge with competing BBR and BBR/Cubic flows. Unfairness correlates with factors like round-trip time and buffer size. The core reason is the lack of responding mechanisms for the flows to converge on fair bandwidth share.",
        "published": "2024-10-31 04:00:00",
        "id": "a66e566a-3e49-498e-8037-0f74c26c80d7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文使用NS - 3模拟器评估BBR拥塞控制的公平性，指出BBR在单流中性能虽优于基于丢失的方法，但在与其他流竞争时存在不公平问题且与往返时间和缓冲区大小等因素相关，核心原因是缺乏使流收敛于公平带宽份额的响应机制。"
        },
        "tokens": 708
    },
    {
        "title": "Vertical Federated Learning with Missing Features During Training and Inference",
        "link": "https://arxiv.org/abs/2410.22564",
        "description": "arXiv:2410.22564v1 Announce Type: new \nAbstract: Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely holds, as for many samples only a subset of the clients observe their partition. However, not utilizing incomplete samples during training harms generalization, and not supporting them during inference limits the utility of the model. Moreover, if any client leaves the federation after training, its partition becomes unavailable, rendering the learned model unusable. Missing feature blocks are therefore a key challenge limiting the applicability of vertical federated learning in real-world scenarios. To address this, we propose LASER-VFL, a vertical federated learning method for efficient training and inference of split neural network-based models that is capable of handling arbitrary sets of partitions. Our approach is simple yet effective, relying on the strategic sharing of model parameters and on task-sampling to train a family of predictors. We show that LASER-VFL achieves a $\\mathcal{O}({1}/{\\sqrt{T}})$ convergence rate for nonconvex objectives in general, $\\mathcal{O}({1}/{T})$ for sufficiently large batch sizes, and linear convergence under the Polyak-{\\L}ojasiewicz inequality. Numerical experiments show improved performance of LASER-VFL over the baselines. Remarkably, this is the case even in the absence of missing features. For example, for CIFAR-100, we see an improvement in accuracy of $21.4\\%$ when each of four feature blocks is observed with a probability of 0.5 and of $12.2\\%$ when all features are observed.",
        "published": "2024-10-31 04:00:00",
        "id": "feef5f30-4ba8-4faa-9bf7-2abe0cfe5690",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出LASER - VFL垂直联邦学习方法，可处理任意分区集以有效训练和推理基于分裂神经网络的模型，在非凸目标等情况下有收敛率，数值实验显示其性能优于基线。"
        },
        "tokens": 953
    },
    {
        "title": "Flow Matching for Posterior Inference with Simulator Feedback",
        "link": "https://arxiv.org/abs/2410.22573",
        "description": "arXiv:2410.22573v1 Announce Type: new \nAbstract: Flow-based generative modeling is a powerful tool for solving inverse problems in physical sciences that can be used for sampling and likelihood evaluation with much lower inference times than traditional methods. We propose to refine flows with additional control signals based on a simulator. Control signals can include gradients and a problem-specific cost function if the simulator is differentiable, or they can be fully learned from the simulator output. In our proposed method, we pretrain the flow network and include feedback from the simulator exclusively for finetuning, therefore requiring only a small amount of additional parameters and compute. We motivate our design choices on several benchmark problems for simulation-based inference and evaluate flow matching with simulator feedback against classical MCMC methods for modeling strong gravitational lens systems, a challenging inverse problem in astronomy. We demonstrate that including feedback from the simulator improves the accuracy by $53\\%$, making it competitive with traditional techniques while being up to $67$x faster for inference.",
        "published": "2024-10-31 04:00:00",
        "id": "239d46f8-a793-4f6b-bf44-5c26c6ad729d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于模拟器反馈的流匹配方法用于后验推理，通过预训练流网络并用模拟器反馈微调，在模拟推理基准问题上验证，在强引力透镜系统建模中提高53%的准确性且推理速度更快。"
        },
        "tokens": 796
    },
    {
        "title": "An eikonal model with re-excitability for fast simulations in cardiac electrophysiology",
        "link": "https://arxiv.org/abs/2410.22583",
        "description": "arXiv:2410.22583v1 Announce Type: new \nAbstract: Precision cardiology based on cardiac digital twins requires accurate simulations of cardiac arrhythmias. However, detailed models, such as the monodomain model, are computationally costly and have limited applicability in practice. Thus, it desirable to have fast models that can still represent the main physiological features presented during cardiac arrhythmias. The eikonal model is an approximation of the monodomain model that is widely used to describe the arrival times of the electrical wave. However, the standard eikonal model does not generalize to the complex re-entrant dynamics that characterize the cardiac arrhythmias. In this work, we propose an eikonal model that includes the tissue re-excitability, which allows to describe re-entries. The re-excitability properties are inferred from the monodomain model. Our eikonal model also handles the tissue anisotropy and heterogeneity. We compare the eikonal model to the monodomain model in various numerical experiments in the atria and the ventricles. The eikonal model is qualitatively accurate in the simulation of re-entries and can be potentially ran in real-time, opening the door to its clinical applicability.",
        "published": "2024-10-31 04:00:00",
        "id": "f71d6ee9-1395-49ec-9372-1643adf7301f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出一种包含组织再兴奋性的eikonal模型用于心脏电生理学快速模拟，可描述再入现象，在心房和心室的数值实验中与单域模型对比，定性准确且可能实时运行。"
        },
        "tokens": 848
    },
    {
        "title": "BENCHAGENTS: Automated Benchmark Creation with Agent Interaction",
        "link": "https://arxiv.org/abs/2410.22584",
        "description": "arXiv:2410.22584v1 Announce Type: new \nAbstract: Evaluations are limited by benchmark availability. As models evolve, there is a need to create benchmarks that can measure progress on new generative capabilities. However, creating new benchmarks through human annotations is slow and expensive, restricting comprehensive evaluations for any capability. We introduce BENCHAGENTS, a framework that methodically leverages large language models (LLMs) to automate benchmark creation for complex capabilities while inherently ensuring data and metric quality. BENCHAGENTS decomposes the benchmark creation process into planning, generation, data verification, and evaluation, each of which is executed by an LLM agent. These agents interact with each other and utilize human-in-the-loop feedback from benchmark developers to explicitly improve and flexibly control data diversity and quality. We use BENCHAGENTS to create benchmarks to evaluate capabilities related to planning and constraint satisfaction during text generation. We then use these benchmarks to study seven state-of-the-art models and extract new insights on common failure modes and model differences.",
        "published": "2024-10-31 04:00:00",
        "id": "694f6b41-214b-46ef-9175-0a0e414a8039",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "BENCHAGENTS框架利用大型语言模型自动化创建用于评估复杂能力的基准，通过代理交互、人工反馈来确保数据和指标质量，已创建评估文本生成规划和约束满足能力的基准并研究了七个先进模型。"
        },
        "tokens": 806
    },
    {
        "title": "Pre-Trained Vision Models as Perception Backbones for Safety Filters in Autonomous Driving",
        "link": "https://arxiv.org/abs/2410.22585",
        "description": "arXiv:2410.22585v1 Announce Type: new \nAbstract: End-to-end vision-based autonomous driving has achieved impressive success, but safety remains a major concern. The safe control problem has been addressed in low-dimensional settings using safety filters, e.g., those based on control barrier functions. Designing safety filters for vision-based controllers in the high-dimensional settings of autonomous driving can similarly alleviate the safety problem, but is significantly more challenging. In this paper, we address this challenge by using frozen pre-trained vision representation models as perception backbones to design vision-based safety filters, inspired by these models' success as backbones of robotic control policies. We empirically evaluate the offline performance of four common pre-trained vision models in this context. We try three existing methods for training safety filters for black-box dynamics, as the dynamics over representation spaces are not known. We use the DeepAccident dataset that consists of action-annotated videos from multiple cameras on vehicles in CARLA simulating real accident scenarios. Our results show that the filters resulting from our approach are competitive with the ones that are given the ground truth state of the ego vehicle and its environment.",
        "published": "2024-10-31 04:00:00",
        "id": "bfd593e7-c91d-4e73-a34a-4c5f23ced427",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "利用预训练视觉表征模型作为感知骨干设计基于视觉的安全过滤器以解决自动驾驶安全问题，通过DeepAccident数据集评估了四种常见预训练视觉模型在此情境下的离线性能。"
        },
        "tokens": 821
    },
    {
        "title": "Toxicity of the Commons: Curating Open-Source Pre-Training Data",
        "link": "https://arxiv.org/abs/2410.22587",
        "description": "arXiv:2410.22587v1 Announce Type: new \nAbstract: Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, there researchers are working to make language models safer. We propose a data curation pipeline to reduce harmful outputs by models trained on public domain data. There are unique challenges to working with public domain data, as these sources differ from web text in both form and content. Many sources are historical documents and are the result of Optical Character Recognition (OCR). Consequently, current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models. In this paper, we introduce a new fully open-source pipeline for open-data toxicity filtering. Our contributions are threefold. We create a custom training dataset, ToxicCommons, which is composed of texts which have been classified across five different dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence). We use this dataset to train a custom classifier, Celadon, that can be used to detect toxic content in open data more efficiently at a larger scale. Finally, we describe the balanced approach to content filtration that optimizes safety filtering with respect to the filtered data available for training.",
        "published": "2024-10-31 04:00:00",
        "id": "6e609fd5-ccd3-476e-973d-7f2c0cadbeab",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对开源大型语言模型训练数据中的毒性问题，论文提出一个数据处理管道，创建自定义训练数据集ToxicCommons，训练分类器Celadon，并描述了内容过滤的平衡方法。"
        },
        "tokens": 869
    },
    {
        "title": "FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness",
        "link": "https://arxiv.org/abs/2410.22591",
        "description": "arXiv:2410.22591v1 Announce Type: new \nAbstract: This paper introduces the first graph-based framework for generating group counterfactual explanations to audit model fairness, a crucial aspect of trustworthy machine learning. Counterfactual explanations are instrumental in understanding and mitigating unfairness by revealing how inputs should change to achieve a desired outcome. Our framework, named Feasible Group Counterfactual Explanations (FGCEs), captures real-world feasibility constraints and constructs subgroups with similar counterfactuals, setting it apart from existing methods. It also addresses key trade-offs in counterfactual generation, including the balance between the number of counterfactuals, their associated costs, and the breadth of coverage achieved. To evaluate these trade-offs and assess fairness, we propose measures tailored to group counterfactual generation. Our experimental results on benchmark datasets demonstrate the effectiveness of our approach in managing feasibility constraints and trade-offs, as well as the potential of our proposed metrics in identifying and quantifying fairness issues.",
        "published": "2024-10-31 04:00:00",
        "id": "10395785-1d36-4783-b0ad-52118baef965",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出首个基于图的框架FGCE生成群体反事实解释来审计模型公平性，提出相关衡量措施，实验结果证明其在管理约束和权衡方面的有效性及度量指标在识别量化公平性问题上的潜力。"
        },
        "tokens": 802
    },
    {
        "title": "GRADE: Quantifying Sample Diversity in Text-to-Image Models",
        "link": "https://arxiv.org/abs/2410.22592",
        "description": "arXiv:2410.22592v1 Announce Type: new \nAbstract: Text-to-image (T2I) models are remarkable at generating realistic images based on textual descriptions. However, textual prompts are inherently underspecified: they do not specify all possible attributes of the required image. This raises two key questions: Do T2I models generate diverse outputs on underspecified prompts? How can we automatically measure diversity? We propose GRADE: Granular Attribute Diversity Evaluation, an automatic method for quantifying sample diversity. GRADE leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ``shape'' and ``color'' for the concept ``cookie''). It then estimates frequency distributions of concepts and their attributes and quantifies diversity using (normalized) entropy. GRADE achieves over 90% human agreement while exhibiting weak correlation to commonly used diversity metrics. We use GRADE to measure the overall diversity of 12 T2I models using 400 concept-attribute pairs, revealing that all models display limited variation. Further, we find that these models often exhibit default behaviors, a phenomenon where the model consistently generates concepts with the same attributes (e.g., 98% of the cookies are round). Finally, we demonstrate that a key reason for low diversity is due to underspecified captions in training data. Our work proposes a modern, semantically-driven approach to measure sample diversity and highlights the stunning homogeneity in outputs by T2I models.",
        "published": "2024-10-31 04:00:00",
        "id": "db3f75d4-d68e-4303-89bc-9f95f8fd1f88",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出自动量化样本多样性的GRADE方法，发现12个T2I模型总体多样性有限、存在默认行为，低多样性源于训练数据说明不足，GRADE与常用多样性度量相关性弱且达成超90%人类认同。"
        },
        "tokens": 915
    },
    {
        "title": "Gaussian Derivative Change-point Detection for Early Warnings of Industrial System Failures",
        "link": "https://arxiv.org/abs/2410.22594",
        "description": "arXiv:2410.22594v1 Announce Type: new \nAbstract: An early warning of future system failure is essential for conducting predictive maintenance and enhancing system availability. This paper introduces a three-step framework for assessing system health to predict imminent system breakdowns. First, the Gaussian Derivative Change-Point Detection (GDCPD) algorithm is proposed for detecting changes in the high-dimensional feature space. GDCPD conducts a multivariate Change-Point Detection (CPD) by implementing Gaussian derivative processes for identifying change locations on critical system features, as these changes eventually will lead to system failure. To assess the significance of these changes, Weighted Mahalanobis Distance (WMD) is applied in both offline and online analyses. In the offline setting, WMD helps establish a threshold that determines significant system variations, while in the online setting, it facilitates real-time monitoring, issuing alarms for potential future system breakdowns. Utilizing the insights gained from the GDCPD and monitoring scheme, Long Short-Term Memory (LSTM) network is then employed to estimate the Remaining Useful Life (RUL) of the system. The experimental study of a real-world system demonstrates the effectiveness of the proposed methodology in accurately forecasting system failures well before they occur. By integrating CPD with real-time monitoring and RUL prediction, this methodology significantly advances system health monitoring and early warning capabilities.",
        "published": "2024-10-31 04:00:00",
        "id": "bc395766-1edd-4ee6-b0cf-bbf2d0e421c9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出三步框架评估系统健康状况以预测即将发生的系统故障，包括高斯导数变点检测算法、加权马氏距离的应用以及长短期记忆网络对系统剩余使用寿命的估计，实验证明其在故障发生前准确预测的有效性。"
        },
        "tokens": 874
    },
    {
        "title": "Symbolic Graph Inference for Compound Scene Understanding",
        "link": "https://arxiv.org/abs/2410.22626",
        "description": "arXiv:2410.22626v1 Announce Type: new \nAbstract: Scene understanding is a fundamental capability needed in many domains, ranging from question-answering to robotics. Unlike recent end-to-end approaches that must explicitly learn varying compositions of the same scene, our method reasons over their constituent objects and analyzes their arrangement to infer a scene's meaning. We propose a novel approach that reasons over a scene's scene- and knowledge-graph, capturing spatial information while being able to utilize general domain knowledge in a joint graph search. Empirically, we demonstrate the feasibility of our method on the ADE20K dataset and compare it to current scene understanding approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "c4cd32b5-a218-46e9-a767-e7b5222ef282",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种通过场景和知识图谱推理场景意义的新方法，经实证，在ADE20K数据集上证明了其可行性并与现有场景理解方法进行比较。"
        },
        "tokens": 712
    },
    {
        "title": "Are Large-Language Models Graph Algorithmic Reasoners?",
        "link": "https://arxiv.org/abs/2410.22597",
        "description": "arXiv:2410.22597v1 Announce Type: new \nAbstract: We seek to address a core challenge facing current Large Language Models (LLMs). LLMs have demonstrated superior performance in many tasks, yet continue to struggle with reasoning problems on explicit graphs that require multiple steps. To address this gap, we introduce a novel benchmark designed to evaluate LLM performance on classical algorithmic reasoning tasks on explicit graphs. Our benchmark encompasses five fundamental algorithms: Breadth-First Search (BFS) and Depth-First Search (DFS) for connectivity, Dijkstra's algorithm and Floyd-Warshall algorithm for all nodes shortest path, and Prim's Minimum Spanning Tree (MST-Prim's) algorithm. Through extensive experimentation, we assess the capabilities of state-of-the-art LLMs in executing these algorithms step-by-step and systematically evaluate their performance at each stage. Our findings highlight the persistent challenges LLMs face in this domain and underscore the necessity for advanced prompting techniques and algorithmic instruction to enhance their graph reasoning abilities. This work presents MAGMA, the first comprehensive benchmark focused on LLMs completing classical graph algorithms, and provides a critical step toward understanding and improving their structured problem-solving skills.",
        "published": "2024-10-31 04:00:00",
        "id": "e44b0d2d-612a-401c-b507-27325b75181d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究人员提出MAGMA基准，通过评估LLMs执行图算法的能力，揭示其在图推理方面面临的挑战并强调提升能力的必要性。"
        },
        "tokens": 822
    },
    {
        "title": "Solving Minimum-Cost Reach Avoid using Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.22600",
        "description": "arXiv:2410.22600v1 Announce Type: new \nAbstract: Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.",
        "published": "2024-10-31 04:00:00",
        "id": "88b8c6ca-bf98-427b-8d42-e87b1aa94f6e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种基于强化学习的RC - PPO方法解决最小成本到达 - 避免问题，实证结果显示该方法在Mujoco模拟器的基准测试中达成目标率与现有方法相当但累积成本更低。"
        },
        "tokens": 793
    },
    {
        "title": "A Cascade Approach for APT Campaign Attribution in System Event Logs: Technique Hunting and Subgraph Matching",
        "link": "https://arxiv.org/abs/2410.22602",
        "description": "arXiv:2410.22602v1 Announce Type: new \nAbstract: As Advanced Persistent Threats (APTs) grow increasingly sophisticated, the demand for effective detection methods has intensified. This study addresses the challenge of identifying APT campaign attacks through system event logs. A cascading approach, name SFM, combines Technique hunting and APT campaign attribution. Our approach assumes that real-world system event logs contain a vast majority of normal events interspersed with few suspiciously malicious ones and that these logs are annotated with Techniques of MITRE ATT&amp;CK framework for attack pattern recognition. Then, we attribute APT campaign attacks by aligning detected Techniques with known attack sequences to determine the most likely APT campaign. Evaluations on five real-world APT campaigns indicate that the proposed approach demonstrates reliable performance.",
        "published": "2024-10-31 04:00:00",
        "id": "c2a8c778-e016-4379-856c-eccd49c08f7e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为SFM的级联方法，结合技术搜索和APT活动归因，通过系统事件日志识别APT活动攻击，对五个真实APT活动的评估表明该方法性能可靠。"
        },
        "tokens": 758
    },
    {
        "title": "Testing Tensor Products of Algebraic Codes",
        "link": "https://arxiv.org/abs/2410.22606",
        "description": "arXiv:2410.22606v1 Announce Type: new \nAbstract: Motivated by recent advances in locally testable codes and quantum LDPCs based on robust testability of tensor product codes, we explore the local testability of tensor products of (an abstraction of) algebraic geometry codes. Such codes are parameterized by, in addition to standard parameters such as block length $n$ and dimension $k$, their genus $g$. We show that the tensor product of two algebraic geometry codes is robustly locally testable provided $n = \\Omega((k+g)^2)$. Apart from Reed-Solomon codes, this seems to be the first explicit family of codes of super-constant dual distance that is robustly locally testable.",
        "published": "2024-10-31 04:00:00",
        "id": "ac25aad3-24d9-4104-a1be-b0c23537187a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受局部可测试码和量子LDPC的进展推动，探索代数几何码张量积的局部可测试性，证明在一定条件下两个代数几何码的张量积可稳健地局部测试，这是超常数对偶距离码中第一个明确可稳健局部测试的族。"
        },
        "tokens": 750
    },
    {
        "title": "CoGS: Model Agnostic Causality Constrained Counterfactual Explanations using goal-directed ASP",
        "link": "https://arxiv.org/abs/2410.22615",
        "description": "arXiv:2410.22615v1 Announce Type: new \nAbstract: Machine learning models are increasingly used in critical areas such as loan approvals and hiring, yet they often function as black boxes, obscuring their decision-making processes. Transparency is crucial, as individuals need explanations to understand decisions, primarily if the decisions result in an undesired outcome. Our work introduces CoGS (Counterfactual Generation with s(CASP)), a model-agnostic framework capable of generating counterfactual explanations for classification models. CoGS leverages the goal-directed Answer Set Programming system s(CASP) to compute realistic and causally consistent modifications to feature values, accounting for causal dependencies between them. By using rule-based machine learning algorithms (RBML), notably the FOLD-SE algorithm, CoGS extracts the underlying logic of a statistical model to generate counterfactual solutions. By tracing a step-by-step path from an undesired outcome to a desired one, CoGS offers interpretable and actionable explanations of the changes required to achieve the desired outcome. We present details of the CoGS framework along with its evaluation.",
        "published": "2024-10-31 04:00:00",
        "id": "2bac959c-cc8d-4e46-b918-58684bcaf6ad",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了CoGS框架，这是一个模型无关框架，可利用s(CASP)为分类模型生成反事实解释，通过RBML算法提取统计模型的底层逻辑来生成解决方案，还提及对该框架的评估。"
        },
        "tokens": 822
    },
    {
        "title": "Controlling the Wireless Power Transfer Mechanism of the Both-Sides Retrodirective System",
        "link": "https://arxiv.org/abs/2410.22644",
        "description": "arXiv:2410.22644v1 Announce Type: new \nAbstract: To achieve efficient long-range wireless power transfer (WPT), large antenna systems are necessary spanning lengths of tens to thousands of meters in one dimension. This creates an array in the order of at least hundreds of thousands to billions of elements. This makes the implementation of beamforming control a challenge. Various works focus on iterative optimization or channel estimation to maintain high efficiency in a time-varying environment requiring complex processing capabilities. A simpler alternative is the both-sides retrodirective antenna array (BS-RDAA) system where iterative optimization or channel estimation is not required. In a previous study, it was observed that this system achieves maximum WPT efficiency if the system is marginally stable. Thus, there is a need to regulate the system to maintain marginal stability regardless of the transmission channel conditions. In this paper, we present a plant model for the system and design a control system to drive it to marginal stability. The result is a WPT implementation that is not dependent on complex processing capabilities present in other established high efficiency methods. We also confirmed the ability of the proposed design to maintain maximum efficiency in a dynamic environment through the results of an electromagnetic simulator and a time domain simulator.",
        "published": "2024-10-31 04:00:00",
        "id": "ce7c8701-b6c4-4c85-a619-c488ca08d4e4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为实现高效长距离无线电力传输，提出双边反向定向天线阵列系统的工厂模型并设计控制系统以保持边际稳定性，结果通过电磁模拟器和时域模拟器得到验证。"
        },
        "tokens": 840
    },
    {
        "title": "FISC: Federated Domain Generalization via Interpolative Style Transfer and Contrastive Learning",
        "link": "https://arxiv.org/abs/2410.22622",
        "description": "arXiv:2410.22622v1 Announce Type: new \nAbstract: Federated Learning (FL) shows promise in preserving privacy and enabling collaborative learning. However, most current solutions focus on private data collected from a single domain. A significant challenge arises when client data comes from diverse domains (i.e., domain shift), leading to poor performance on unseen domains. Existing Federated Domain Generalization approaches address this problem but assume each client holds data for an entire domain, limiting their practicality in real-world scenarios with domain-based heterogeneity and client sampling.\n  To overcome this, we introduce FISC, a novel FL domain generalization paradigm that handles more complex domain distributions across clients. FISC enables learning across domains by extracting an interpolative style from local styles and employing contrastive learning. This strategy gives clients multi-domain representations and unbiased convergent targets. Empirical results on multiple datasets, including PACS, Office-Home, and IWildCam, show FISC outperforms state-of-the-art (SOTA) methods. Our method achieves accuracy improvements ranging from 3.64% to 57.22% on unseen domains. Our code is available at https://anonymous.4open.science/r/FISC-AAAI-16107.",
        "published": "2024-10-31 04:00:00",
        "id": "111382f7-1e87-4c3d-babd-abaa2b4bcb4f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍了FISC这种新的联邦学习域泛化范式，通过从局部风格中提取插值风格和对比学习来处理跨客户端的复杂域分布，在多个数据集上优于现有方法。"
        },
        "tokens": 848
    },
    {
        "title": "PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation",
        "link": "https://arxiv.org/abs/2410.22623",
        "description": "arXiv:2410.22623v1 Announce Type: new \nAbstract: Video crime detection is a significant application of computer vision and artificial intelligence. However, existing datasets primarily focus on detecting severe crimes by analyzing entire video clips, often neglecting the precursor activities (i.e., privacy violations) that could potentially prevent these crimes. To address this limitation, we present PV-VTT (Privacy Violation Video To Text), a unique multimodal dataset aimed at identifying privacy violations. PV-VTT provides detailed annotations for both video and text in scenarios. To ensure the privacy of individuals in the videos, we only provide video feature vectors, avoiding the release of any raw video data. This privacy-focused approach allows researchers to use the dataset while protecting participant confidentiality. Recognizing that privacy violations are often ambiguous and context-dependent, we propose a Graph Neural Network (GNN)-based video description model. Our model generates a GNN-based prompt with image for Large Language Model (LLM), which deliver cost-effective and high-quality video descriptions. By leveraging a single video frame along with relevant text, our method reduces the number of input tokens required, maintaining descriptive quality while optimizing LLM API-usage. Extensive experiments validate the effectiveness and interpretability of our approach in video description tasks and flexibility of our PV-VTT dataset.",
        "published": "2024-10-31 04:00:00",
        "id": "c12b69c0-50e3-4d3a-a367-4a2b6a3621da",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决现有视频犯罪检测数据集的局限，提出PV - VTT数据集用于识别隐私侵犯，只提供视频特征向量保护隐私，还提出基于图神经网络的视频描述模型，实验验证了其有效性、可解释性和数据集的灵活性。"
        },
        "tokens": 870
    },
    {
        "title": "Systolic Array Data Flows for Efficient Matrix Multiplication in Deep Neural Networks",
        "link": "https://arxiv.org/abs/2410.22595",
        "description": "arXiv:2410.22595v1 Announce Type: new \nAbstract: The paper discusses how Systolic Arrays can improve matrix multiplication for deep neural networks (DNNs). With AI models like OpenAI's GPT now containing trillions of parameters, the need for efficient matrix multiplication is more critical than ever. In this paper, the three main systolic array data flows: Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS) are discussed. Each data flow's energy consumption and efficiency across various matrix sizes are calculated using the SCALE-Sim simulator. The results show that selecting the right data flow for specific matrix configurations can drastically reduce energy consumption. The conclusions provide helpful insights into optimizing hardware for AI and machine learning applications, offering potential improvements in designing energy-efficient DNN accelerators.",
        "published": "2024-10-31 04:00:00",
        "id": "edc5e406-9473-435a-975b-9ffa5a9b7d5f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨收缩阵列数据流动对深度神经网络中矩阵乘法效率的提升，通过计算不同数据流动在各种矩阵规模下的能耗和效率，得出为特定矩阵配置选择合适数据流动可大幅降低能耗的结论，有助于优化人工智能硬件。"
        },
        "tokens": 768
    },
    {
        "title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation",
        "link": "https://arxiv.org/abs/2410.22629",
        "description": "arXiv:2410.22629v1 Announce Type: new \nAbstract: The field of Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. Despite the substantial domain gaps in RS images that are characterized by variabilities such as location, wavelength, and sensor type, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies targeting the RSDG issue, especially for semantic segmentation tasks, where existing models are developed for specific unknown domains, struggling with issues of underfitting on other unknown scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 28 cross-domain settings across various regions, spectral bands, platforms, and climates, providing a comprehensive framework for testing the generalizability of future RSDG models. Extensive experiments on this benchmark demonstrate the superiority of CrossEarth over existing state-of-the-art methods.",
        "published": "2024-10-31 04:00:00",
        "id": "99fe8131-4b81-4a83-910b-564d9e99f748",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍用于遥感域泛化语义分割的视觉基础模型CrossEarth，阐述现有遥感域泛化研究的不足，CrossEarth通过特定管道展示强跨域泛化能力，还构建了RSDG基准，实验证明其优于现有方法。"
        },
        "tokens": 902
    },
    {
        "title": "Cops & Robber on Periodic Temporal Graphs",
        "link": "https://arxiv.org/abs/2410.22618",
        "description": "arXiv:2410.22618v1 Announce Type: new \nAbstract: We consider the Cops and Robber pursuit-evasion game when the edge-set of the graph is allowed to change in time, possibly at every round. Specifically, the game is played on an infinite periodic sequence $\\mathcal{G} = (G_0, \\dots, G_{p-1})^*$ of graphs on the same set $V$ of $n$ vertices: in round $t$, the topology of $\\mathcal{G}$ is $G_i=(V,E_i)$ where $i\\equiv t \\pmod{p}$.\n  Concentrating on the case of a single cop, we provide a characterization of copwin periodic temporal graphs, establishing several basic properties on their nature, and extending to the temporal domain classical C&amp;R concepts such as covers and corners. Based on these results, we design an efficient algorithm for determining if a periodic temporal graph is copwin.\n  We also consider the case of $k>1$ cops. By shifting from a representation in terms of directed graphs to one in terms of directed multi-hypergraphs, we prove that all the fundamental properties established for $k=1$ continue to hold, providing a characterization of $k$-copwin periodic graphs, as well as a general strategy to determine if a periodic graph is $k$-copwin.\n  Our results do not rely on any assumption on properties such as connectivity, symmetry, reflexivity held by the individual graphs in the sequence. They are established for a unified version of the game that includes the standard games studied in the literature, both for undirected and directed graphs, and both when the players are fully active and when they are not. They hold also for a variety of settings not considered in the literature.",
        "published": "2024-10-31 04:00:00",
        "id": "c9a07193-57fe-4d56-82b7-229e70e89a62",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "论文考虑了边集随时间变化时的警察与强盗追逃游戏，针对单警和多警情况分别进行研究并得出相关结论，包括给出了单警获胜的周期时态图特征并设计了判定算法，通过转换表示方式证明多警情况基本性质仍然成立并给出了判定策略。"
        },
        "tokens": 987
    },
    {
        "title": "Consistency Diffusion Bridge Models",
        "link": "https://arxiv.org/abs/2410.22637",
        "description": "arXiv:2410.22637v1 Announce Type: new \nAbstract: Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample $4\\times$ to $50\\times$ faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from $64 \\times 64$ to $256 \\times 256$, as well as supporting downstream tasks such as semantic interpolation in the data space.",
        "published": "2024-10-31 04:00:00",
        "id": "94c025bb-ac02-4123-a905-ff54a9507d84",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "扩散去噪桥模型（DDBMs）在耦合数据分布任务中取得成功但采样计算需求高，受一致性模型启发，通过学习其概率流常微分方程的一致性函数提出两种范式，实验结果表明新方法采样更快且视觉质量更好，还支持下游任务。"
        },
        "tokens": 910
    },
    {
        "title": "Prove Your Point!: Bringing Proof-Enhancement Principles to Argumentative Essay Generation",
        "link": "https://arxiv.org/abs/2410.22642",
        "description": "arXiv:2410.22642v1 Announce Type: new \nAbstract: Argumentative essay generation (AEG) aims to generate complete texts on specific controversial topics or debates. Although current AEG methods can generate individual opinions, they often overlook the high-level connections between these opinions. This often leads to the generated results being mired in logical confusion, unable to proof their own arguments effectively. The generated essay may present evidence that contradicts the claims or they may fail to assemble the claims into logical flow. In this paper, we present a unified two-stage framework: Proof-Enhancement and Self-Annotation (PESA) for AEG with a focus on logical enhancement. Specifically, we first construct pseudo-labels for logical information,claims and grounds, using a large language model. We then propose a tree planning approach that introduces proof principles and ensures logical consistency. Extensive experimental results show that, benefiting from proof principle guidance, PESA generates argumentative essays with better logical validity and persuasiveness than strong baseline models.",
        "published": "2024-10-31 04:00:00",
        "id": "9d4a7e11-c496-41d7-9570-14ac23ebd524",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一个名为Proof - Enhancement and Self - Annotation (PESA)的两阶段框架用于论点性文章生成，实验结果表明该框架受证明原理指导，比强基线模型生成的文章更具逻辑有效性和说服力。"
        },
        "tokens": 805
    },
    {
        "title": "FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation",
        "link": "https://arxiv.org/abs/2410.22651",
        "description": "arXiv:2410.22651v1 Announce Type: new \nAbstract: Training data privacy has been a top concern in AI modeling. While methods like differentiated private learning allow data contributors to quantify acceptable privacy loss, model utility is often significantly damaged. In practice, controlled data access remains a mainstream method for protecting data privacy in many industrial and research environments. In controlled data access, authorized model builders work in a restricted environment to access sensitive data, which can fully preserve data utility with reduced risk of data leak. However, unlike differential privacy, there is no quantitative measure for individual data contributors to tell their privacy risk before participating in a machine learning task. We developed the demo prototype FT-PrivacyScore to show that it's possible to efficiently and quantitatively estimate the privacy risk of participating in a model fine-tuning task. The demo source code will be available at \\url{https://github.com/RhincodonE/demo_privacy_scoring}.",
        "published": "2024-10-31 04:00:00",
        "id": "f7e1b765-8a8f-4e85-b6d4-47b25baa8aab",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "FT - PrivacyScore原型旨在有效量化参与模型微调任务的隐私风险，其相关代码将公开，在人工智能建模中训练数据隐私受关注，当前保护数据隐私方法存在问题，该原型提供了新的隐私风险量化可能。"
        },
        "tokens": 791
    },
    {
        "title": "DECRL: A Deep Evolutionary Clustering Jointed Temporal Knowledge Graph Representation Learning Approach",
        "link": "https://arxiv.org/abs/2410.22631",
        "description": "arXiv:2410.22631v1 Announce Type: new \nAbstract: Temporal Knowledge Graph (TKG) representation learning aims to map temporal evolving entities and relations to embedded representations in a continuous low-dimensional vector space. However, existing approaches cannot capture the temporal evolution of high-order correlations in TKGs. To this end, we propose a Deep Evolutionary Clustering jointed temporal knowledge graph Representation Learning approach (DECRL). Specifically, a deep evolutionary clustering module is proposed to capture the temporal evolution of high-order correlations among entities. Furthermore, a cluster-aware unsupervised alignment mechanism is introduced to ensure the precise one-to-one alignment of soft overlapping clusters across timestamps, thereby maintaining the temporal smoothness of clusters. In addition, an implicit correlation encoder is introduced to capture latent correlations between any pair of clusters under the guidance of a global graph. Extensive experiments on seven real-world datasets demonstrate that DECRL achieves the state-of-the-art performances, outperforming the best baseline by an average of 9.53%, 12.98%, 10.42%, and 14.68% in MRR, Hits@1, Hits@3, and Hits@10, respectively.",
        "published": "2024-10-31 04:00:00",
        "id": "3f47fb8f-30a4-4275-b7f2-5813119f9e5e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种名为DECRL的深度进化聚类联合时态知识图谱表示学习方法，该方法能捕捉时态知识图谱中高阶相关性的时态演变等，在七个真实数据集的实验中达到了最先进的性能。"
        },
        "tokens": 850
    },
    {
        "title": "SimpsonsVQA: Enhancing Inquiry-Based Learning with a Tailored Dataset",
        "link": "https://arxiv.org/abs/2410.22648",
        "description": "arXiv:2410.22648v1 Announce Type: new \nAbstract: Visual Question Answering (VQA) has emerged as a promising area of research to develop AI-based systems for enabling interactive and immersive learning. Numerous VQA datasets have been introduced to facilitate various tasks, such as answering questions or identifying unanswerable ones. However, most of these datasets are constructed using real-world images, leaving the performance of existing models on cartoon images largely unexplored. Hence, in this paper, we present \"SimpsonsVQA\", a novel dataset for VQA derived from The Simpsons TV show, designed to promote inquiry-based learning. Our dataset is specifically designed to address not only the traditional VQA task but also to identify irrelevant questions related to images, as well as the reverse scenario where a user provides an answer to a question that the system must evaluate (e.g., as correct, incorrect, or ambiguous). It aims to cater to various visual applications, harnessing the visual content of \"The Simpsons\" to create engaging and informative interactive systems. SimpsonsVQA contains approximately 23K images, 166K QA pairs, and 500K judgments (https://simpsonsvqa.org). Our experiments show that current large vision-language models like ChatGPT4o underperform in zero-shot settings across all three tasks, highlighting the dataset's value for improving model performance on cartoon images. We anticipate that SimpsonsVQA will inspire further research, innovation, and advancements in inquiry-based learning VQA.",
        "published": "2024-10-31 04:00:00",
        "id": "481ea56d-99dd-4a99-bd23-d1981499f66b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了名为SimpsonsVQA的新数据集，源于《辛普森一家》，用于视觉问答以促进探究式学习，包含约23K图像、166K问答对和500K判断，实验表明现有大模型在零样本设置下表现不佳。"
        },
        "tokens": 923
    },
    {
        "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting",
        "link": "https://arxiv.org/abs/2410.22649",
        "description": "arXiv:2410.22649v1 Announce Type: new \nAbstract: In recent years, Transformer-based models (Transformers) have achieved significant success in multivariate time series forecasting (MTSF). However, previous works focus on extracting features either from the time domain or the frequency domain, which inadequately captures the trends and periodic characteristics. To address this issue, we propose a wavelet learning framework to model complex temporal dependencies of the time series data. The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales. Additionally, the Softmax self-attention mechanism used by Transformers has quadratic complexity, which leads to excessive computational costs when capturing long-term dependencies. Therefore, we propose a novel attention mechanism: Rotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens $r$ to aggregate information from the $KV$ matrices and redistribute it to the $Q$ matrix, offering linear complexity. We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain. We conduct extensive experiments on eight real-world datasets. The results indicate that WaveRoRA outperforms existing state-of-the-art models while maintaining lower computational costs.",
        "published": "2024-10-31 04:00:00",
        "id": "5863408e-d419-418a-8b1a-ac471a8485b8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出WaveRoRA模型，其包含的RoRA注意力机制可在小波域捕获多变量时间序列数据的复杂时间依赖关系，在八个真实数据集上的实验表明该模型性能优于现有模型且计算成本更低。"
        },
        "tokens": 868
    },
    {
        "title": "An Overtaking Trajectory Planning Framework Based on Spatio-temporal Topology and Reachable Set Analysis Ensuring Time Efficiency",
        "link": "https://arxiv.org/abs/2410.22643",
        "description": "arXiv:2410.22643v1 Announce Type: new \nAbstract: Generating overtaking trajectories in high-speed scenarios presents significant challenges and is typically addressed through hierarchical planning methods. However, this method has two primary drawbacks. First, heuristic algorithms can only provide a single initial solution, which may lead to local optima and consequently diminish the quality of the solution. Second, the time efficiency of trajectory refinement based on numerical optimization is insufficient. To overcome these limitations, this paper proposes an overtaking trajectory planning framework based on spatio-temporal topology and reachable set analysis (SROP), to improve trajectory quality and time efficiency. Specifically, this paper introduces topological classes to describe trajectories representing different overtaking behaviors, which support the spatio-temporal topological search method employed by the upper-layer planner to identify diverse initial paths. This approach helps prevent getting stuck in local optima, enhancing the overall solution quality by considering multiple initial solutions from distinct topologies. Moreover, the reachable set method is integrated into the lower-layer planner for parallel trajectory evaluation. This method enhances planning efficiency by decoupling vehicle model constraints from the optimization process, enabling parallel computation while ensuring control feasibility. Simulation results show that the proposed method improves the smoothness of generated trajectories by 66.8% compared to state-of-the-art methods, highlighting its effectiveness in enhancing trajectory quality. Additionally, this method reduces computation time by 62.9%, demonstrating its efficiency.",
        "published": "2024-10-31 04:00:00",
        "id": "dd3bc91b-25e4-492b-b603-11ff1c8657eb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对高速场景中超车轨迹规划的挑战，提出基于时空拓扑和可达集分析（SROP）的框架，上层用时空拓扑搜索法找不同初始路径避免局部最优，下层用可达集方法并行评估轨迹提高效率，该方法提升轨迹质量并减少计算时间。"
        },
        "tokens": 906
    },
    {
        "title": "Development of a Python-Based Software for Calculating the Jones Polynomial: Insights into the Behavior of Polymers and Biopolymers",
        "link": "https://arxiv.org/abs/2410.22652",
        "description": "arXiv:2410.22652v1 Announce Type: new \nAbstract: This thesis details a Python-based software designed to calculate the Jones polynomial, a vital mathematical tool from Knot Theory used for characterizing the topological and geometrical complexity of curves in \\( \\mathbb{R}^3 \\), which is essential in understanding physical systems of filaments, including the behavior of polymers and biopolymers. The Jones polynomial serves as a topological invariant capable of distinguishing between different knot structures. This capability is fundamental to characterizing the architecture of molecular chains, such as proteins and DNA. Traditional computational methods for deriving the Jones polynomial have been limited by closure-schemes and high execution costs, which can be impractical for complex structures like those that appear in real life. This software implements methods that significantly reduce calculation times, allowing for more efficient and practical applications in the study of biological polymers. It utilizes a divide-and-conquer approach combined with parallel computing and applies recursive Reidemeister moves to optimize the computation, transitioning from an exponential to a near-linear runtime for specific configurations. This thesis provides an overview of the software's functions, detailed performance evaluations using protein structures as test cases, and a discussion of the implications for future research and potential algorithmic improvements.",
        "published": "2024-10-31 04:00:00",
        "id": "8d1f4389-bf1f-4ea3-8740-360541d6ca02",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文阐述一款用于计算琼斯多项式的Python软件，它采用特定计算方法优化运行时间，文中以蛋白质结构为测试用例进行性能评估并讨论未来研究意义。"
        },
        "tokens": 840
    },
    {
        "title": "FlowDCN: Exploring DCN-like Architectures for Fast Image Generation with Arbitrary Resolution",
        "link": "https://arxiv.org/abs/2410.22655",
        "description": "arXiv:2410.22655v1 Announce Type: new \nAbstract: Arbitrary-resolution image generation still remains a challenging task in AIGC, as it requires handling varying resolutions and aspect ratios while maintaining high visual quality. Existing transformer-based diffusion methods suffer from quadratic computation cost and limited resolution extrapolation capabilities, making them less effective for this task. In this paper, we propose FlowDCN, a purely convolution-based generative model with linear time and memory complexity, that can efficiently generate high-quality images at arbitrary resolutions. Equipped with a new design of learnable group-wise deformable convolution block, our FlowDCN yields higher flexibility and capability to handle different resolutions with a single model. FlowDCN achieves the state-of-the-art 4.30 sFID on $256\\times256$ ImageNet Benchmark and comparable resolution extrapolation results, surpassing transformer-based counterparts in terms of convergence speed (only $\\frac{1}{5}$ images), visual quality, parameters ($8\\%$ reduction) and FLOPs ($20\\%$ reduction). We believe FlowDCN offers a promising solution to scalable and flexible image synthesis.",
        "published": "2024-10-31 04:00:00",
        "id": "44e0d193-6a25-4ed6-9a0c-2b2f572ab3dc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "FlowDCN是一种基于卷积的生成模型，具有线性时间和内存复杂度，可高效生成任意分辨率的高质量图像，在256×256 ImageNet基准测试中达到4.30 sFID，在收敛速度、视觉质量、参数和FLOP方面优于基于变压器的模型。"
        },
        "tokens": 859
    },
    {
        "title": "Reweighting Local Mimina with Tilted SAM",
        "link": "https://arxiv.org/abs/2410.22656",
        "description": "arXiv:2410.22656v1 Announce Type: new \nAbstract: Sharpness-Aware Minimization (SAM) has been demonstrated to improve the generalization performance of overparameterized models by seeking flat minima on the loss landscape through optimizing model parameters that incur the largest loss within a neighborhood. Nevertheless, such min-max formulations are computationally challenging especially when the problem is highly non-convex. Additionally, focusing only on the worst-case local solution while ignoring potentially many other local solutions may be suboptimal when searching for flat minima. In this work, we propose Tilted SAM (TSAM), a generalization of SAM inspired by exponential tilting that effectively assigns higher priority to local solutions that are flatter and that incur larger losses. TSAM is parameterized by a tilt hyperparameter t and reduces to SAM as t approaches infinity. We prove that (1) the TSAM objective is smoother than SAM and thus easier to optimize; and (2) TSAM explicitly favors flatter minima as t increases. This is desirable as flatter minima could have better generalization properties for certain tasks. We develop algorithms motivated by the discretization of Hamiltonian dynamics to solve TSAM. Empirically, TSAM arrives at flatter local minima and results in superior test performance than the baselines of SAM and ERM across a range of image and text tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "d127ba3a-ad32-4a12-8d05-f0d0b5ab1c72",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出一种受指数倾斜启发的Tilted SAM（TSAM），它是SAM的推广，被证明比SAM目标更平滑且更易优化、明确倾向更平坦的最小值，通过离散化哈密顿动力学开发算法求解TSAM，在图像和文本任务中比SAM和ERM有更好的测试性能。"
        },
        "tokens": 883
    },
    {
        "title": "Automatic programming via large language models with population self-evolution for dynamic job shop scheduling problem",
        "link": "https://arxiv.org/abs/2410.22657",
        "description": "arXiv:2410.22657v1 Announce Type: new \nAbstract: Heuristic dispatching rules (HDRs) are widely regarded as effective methods for solving dynamic job shop scheduling problems (DJSSP) in real-world production environments. However, their performance is highly scenario-dependent, often requiring expert customization. To address this, genetic programming (GP) and gene expression programming (GEP) have been extensively used for automatic algorithm design. Nevertheless, these approaches often face challenges due to high randomness in the search process and limited generalization ability, hindering the application of trained dispatching rules to new scenarios or dynamic environments. Recently, the integration of large language models (LLMs) with evolutionary algorithms has opened new avenues for prompt engineering and automatic algorithm design. To enhance the capabilities of LLMs in automatic HDRs design, this paper proposes a novel population self-evolutionary (SeEvo) method, a general search framework inspired by the self-reflective design strategies of human experts. The SeEvo method accelerates the search process and enhances exploration capabilities. Experimental results show that the proposed SeEvo method outperforms GP, GEP, end-to-end deep reinforcement learning methods, and more than 10 common HDRs from the literature, particularly in unseen and dynamic scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "7ff3efc0-abbe-48fd-9290-486c9da3dd91",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对动态作业车间调度问题，提出一种新的种群自进化方法，该方法结合大型语言模型，实验结果显示在多种场景下优于其他算法。"
        },
        "tokens": 841
    },
    {
        "title": "Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models",
        "link": "https://arxiv.org/abs/2410.22660",
        "description": "arXiv:2410.22660v1 Announce Type: new \nAbstract: Code-switching, the phenomenon of alternating between two or more languages in a single conversation, presents unique challenges for Natural Language Processing (NLP). Most existing research focuses on either syntactic constraints or neural generation, with few efforts to integrate linguistic theory with large language models (LLMs) for generating natural code-switched text. In this paper, we introduce EZSwitch, a novel framework that combines Equivalence Constraint Theory (ECT) with LLMs to produce linguistically valid and fluent code-switched text. We evaluate our method using both human judgments and automatic metrics, demonstrating a significant improvement in the quality of generated code-switching sentences compared to baseline LLMs. To address the lack of suitable evaluation metrics, we conduct a comprehensive correlation study of various automatic metrics against human scores, revealing that current metrics often fail to capture the nuanced fluency of code-switched text. Additionally, we create CSPref, a human preference dataset based on human ratings and analyze model performance across ``hard`` and ``easy`` examples. Our findings indicate that incorporating linguistic constraints into LLMs leads to more robust and human-aligned generation, paving the way for scalable code-switching text generation across diverse language pairs.",
        "published": "2024-10-31 04:00:00",
        "id": "f5c51e68-db17-48c2-9539-850434ab7554",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出EZSwitch框架将等价约束理论与大型语言模型结合以生成语言有效的语码转换文本，通过人类评判和自动指标评估方法，还研究自动指标与人类评分的相关性，创建人类偏好数据集并分析模型性能。"
        },
        "tokens": 865
    },
    {
        "title": "Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers",
        "link": "https://arxiv.org/abs/2410.22663",
        "description": "arXiv:2410.22663v1 Announce Type: new \nAbstract: Machine learning (ML) for text classification has been widely used in various domains, such as toxicity detection, chatbot consulting, and review analysis. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Several studies indicate that traditional metrics, such as model confidence and accuracy, are insufficient to build human trust in ML models. These models often learn spurious correlations during training and predict based on them during inference. In the real world, where such correlations are absent, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are reasonable. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. Due to the lack of automated trustworthiness oracles, the assessment requires manual validation of the decision process disclosed by explanation methods, which is time-consuming and not scalable. We propose TOKI, the first automated trustworthiness oracle generation method for text classifiers, which automatically checks whether the prediction-contributing words are related to the predicted class using explanation methods and word embeddings. To demonstrate its practical usefulness, we introduce a novel adversarial attack method targeting trustworthiness issues identified by TOKI. We compare TOKI with a naive baseline based solely on model confidence using human-created ground truths of 6,000 predictions. We also compare TOKI-guided adversarial attack method with A2T, a SOTA adversarial attack method. Results show that relying on prediction uncertainty cannot distinguish between trustworthy and untrustworthy predictions, TOKI achieves 142% higher accuracy than the naive baseline, and TOKI-guided adversarial attack method is more effective with fewer perturbations than A2T.",
        "published": "2024-10-31 04:00:00",
        "id": "12a34bec-c79c-42f6-9c00-ac344e3c3d58",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出针对文本分类器的自动可信度预言机生成方法TOKI，通过实验证明TOKI比仅基于模型置信度的方法准确率更高，TOKI引导的对抗攻击方法更有效。"
        },
        "tokens": 948
    },
    {
        "title": "A Walsh Hadamard Derived Linear Vector Symbolic Architecture",
        "link": "https://arxiv.org/abs/2410.22669",
        "description": "arXiv:2410.22669v1 Announce Type: new \nAbstract: Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\mathbb{R}^d$ are `bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems. Code is available at https://github.com/FutureComputing4AI/Hadamard-derived-Linear-Binding",
        "published": "2024-10-31 04:00:00",
        "id": "b3f56fb2-c176-4fb5-be1c-87616fdb555c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍一种新的向量符号架构Hadamard - derived线性绑定（HLB），它有良好计算效率，在经典VSA任务中有功效且在可微系统中表现良好，代码可在指定网址获取。"
        },
        "tokens": 775
    },
    {
        "title": "IM-GIV: an effective integrity monitoring scheme for tightly-coupled GNSS/INS/Vision integration based on factor graph optimization",
        "link": "https://arxiv.org/abs/2410.22672",
        "description": "arXiv:2410.22672v1 Announce Type: new \nAbstract: Global Navigation Satellite System/Inertial Navigation System (GNSS/INS)/Vision integration based on factor graph optimization (FGO) has recently attracted extensive attention in navigation and robotics community. Integrity monitoring (IM) capability is required when FGO-based integrated navigation system is used for safety-critical applications. However, traditional researches on IM of integrated navigation system are mostly based on Kalman filter. It is urgent to develop effective IM scheme for FGO-based GNSS/INS/Vision integration. In this contribution, the position error bounding formula to ensure the integrity of the GNSS/INS/Vision integration based on FGO is designed and validated for the first time. It can be calculated by the linearized equations from the residuals of GNSS pseudo-range, IMU pre-integration and visual measurements. The specific position error bounding is given in the case of GNSS, INS and visual measurement faults. Field experiments were conducted to evaluate and validate the performance of the proposed position error bounding. Experimental results demonstrate that the proposed position error bounding for the GNSS/INS/Vision integration based on FGO can correctly fit the position error against different fault modes, and the availability of integrity in six fault modes is 100% after correct and timely fault exclusion.",
        "published": "2024-10-31 04:00:00",
        "id": "cd27db45-834a-41d0-a7e3-b5f688c23239",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出并验证了基于因子图优化的GNSS/INS/视觉集成的位置误差界定公式，以确保其完整性，实验表明该公式可针对不同故障模式正确拟合位置误差并在排除故障后保证完整性的可用性。"
        },
        "tokens": 877
    },
    {
        "title": "Calibrating Practical Privacy Risks for Differentially Private Machine Learning",
        "link": "https://arxiv.org/abs/2410.22673",
        "description": "arXiv:2410.22673v1 Announce Type: new \nAbstract: Differential privacy quantifies privacy through the privacy budget $\\epsilon$, yet its practical interpretation is complicated by variations across models and datasets. Recent research on differentially private machine learning and membership inference has highlighted that with the same theoretical $\\epsilon$ setting, the likelihood-ratio-based membership inference (LiRA) attacking success rate (ASR) may vary according to specific datasets and models, which might be a better indicator for evaluating real-world privacy risks. Inspired by this practical privacy measure, we study the approaches that can lower the attacking success rate to allow for more flexible privacy budget settings in model training. We find that by selectively suppressing privacy-sensitive features, we can achieve lower ASR values without compromising application-specific data utility. We use the SHAP and LIME model explainer to evaluate feature sensitivities and develop feature-masking strategies. Our findings demonstrate that the LiRA $ASR^M$ on model $M$ can properly indicate the inherent privacy risk of a dataset for modeling, and it's possible to modify datasets to enable the use of larger theoretical $\\epsilon$ settings to achieve equivalent practical privacy protection. We have conducted extensive experiments to show the inherent link between ASR and the dataset's privacy risk. By carefully selecting features to mask, we can preserve more data utility with equivalent practical privacy protection and relaxed $\\epsilon$ settings. The implementation details are shared online at the provided GitHub URL \\url{https://anonymous.4open.science/r/On-sensitive-features-and-empirical-epsilon-lower-bounds-BF67/}.",
        "published": "2024-10-31 04:00:00",
        "id": "e8cbba4f-bd5f-413c-8c6d-e680cd32c558",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究受差分隐私机器学习和成员推理启发，发现通过选择性抑制隐私敏感特征可降低攻击成功率，在不损害数据效用前提下实现更灵活的隐私预算设置，还通过实验展示了攻击成功率与数据集隐私风险的内在联系。"
        },
        "tokens": 927
    },
    {
        "title": "Backdoor Attack Against Vision Transformers via Attention Gradient-Based Image Erosion",
        "link": "https://arxiv.org/abs/2410.22678",
        "description": "arXiv:2410.22678v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Networks (CNN) across various computer vision tasks. However, akin to CNN, ViTs are vulnerable to backdoor attacks, where the adversary embeds the backdoor into the victim model, causing it to make wrong predictions about testing samples containing a specific trigger. Existing backdoor attacks against ViTs have the limitation of failing to strike an optimal balance between attack stealthiness and attack effectiveness.\n  In this work, we propose an Attention Gradient-based Erosion Backdoor (AGEB) targeted at ViTs. Considering the attention mechanism of ViTs, AGEB selectively erodes pixels in areas of maximal attention gradient, embedding a covert backdoor trigger. Unlike previous backdoor attacks against ViTs, AGEB achieves an optimal balance between attack stealthiness and attack effectiveness, ensuring the trigger remains invisible to human detection while preserving the model's accuracy on clean samples. Extensive experimental evaluations across various ViT architectures and datasets confirm the effectiveness of AGEB, achieving a remarkable Attack Success Rate (ASR) without diminishing Clean Data Accuracy (CDA). Furthermore, the stealthiness of AGEB is rigorously validated, demonstrating minimal visual discrepancies between the clean and the triggered images.",
        "published": "2024-10-31 04:00:00",
        "id": "d3ec5d84-221b-43c7-a4cb-fbb7f014bfc8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出针对视觉变换器（ViTs）的基于注意力梯度的侵蚀后门（AGEB）攻击，能在攻击隐蔽性和有效性间取得平衡，经多种架构和数据集验证有效。"
        },
        "tokens": 853
    },
    {
        "title": "Practical and Accurate Reconstruction of an Illuminant's Spectral Power Distribution for Inverse Rendering Pipelines",
        "link": "https://arxiv.org/abs/2410.22679",
        "description": "arXiv:2410.22679v1 Announce Type: new \nAbstract: Inverse rendering pipelines are gaining prominence in realizing photo-realistic reconstruction of real-world objects for emulating them in virtual reality scenes. Apart from material reflectances, spectral rendering and in-scene illuminants' spectral power distributions (SPDs) play important roles in producing photo-realistic images. We present a simple, low-cost technique to capture and reconstruct the SPD of uniform illuminants. Instead of requiring a costly spectrometer for such measurements, our method uses a diffractive compact disk (CD-ROM) and a machine learning approach for accurate estimation. We show our method to work well with spotlights under simulations and few real-world examples. Presented results clearly demonstrate the reliability of our approach through quantitative and qualitative evaluations, especially in spectral rendering of iridescent materials.",
        "published": "2024-10-31 04:00:00",
        "id": "22c8c25d-b6a2-484e-8c46-bc43978678cf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种简单低成本的均匀光源光谱功率分布的捕获和重建技术，使用衍射光盘和机器学习方法准确估计，通过模拟和实例展示方法的可靠性。"
        },
        "tokens": 762
    },
    {
        "title": "Byzantine-Robust Federated Learning: An Overview With Focus on Developing Sybil-based Attacks to Backdoor Augmented Secure Aggregation Protocols",
        "link": "https://arxiv.org/abs/2410.22680",
        "description": "arXiv:2410.22680v1 Announce Type: new \nAbstract: Federated Learning (FL) paradigms enable large numbers of clients to collaboratively train Machine Learning models on private data. However, due to their multi-party nature, traditional FL schemes are left vulnerable to Byzantine attacks that attempt to hurt model performance by injecting malicious backdoors. A wide variety of prevention methods have been proposed to protect frameworks from such attacks. This paper provides a exhaustive and updated taxonomy of existing methods and frameworks, before zooming in and conducting an in-depth analysis of the strengths and weaknesses of the Robustness of Federated Learning (RoFL) protocol. From there, we propose two novel Sybil-based attacks that take advantage of vulnerabilities in RoFL. Finally, we conclude with comprehensive proposals for future testing, describe and detail implementation of the proposed attacks, and offer direction for improvements in the RoFL protocol as well as Byzantine-robust frameworks as a whole.",
        "published": "2024-10-31 04:00:00",
        "id": "a7d3a454-6f7f-47ad-bf31-4053071b511a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文概述拜占庭稳健联邦学习，分析联邦学习稳健性协议的优缺点并提出两种攻击，还对未来测试提出建议。"
        },
        "tokens": 785
    },
    {
        "title": "Choice between Partial Trajectories",
        "link": "https://arxiv.org/abs/2410.22690",
        "description": "arXiv:2410.22690v1 Announce Type: new \nAbstract: As AI agents generate increasingly sophisticated behaviors, manually encoding human preferences to guide these agents becomes more challenging. To address this, it has been suggested that agents instead learn preferences from human choice data. This approach requires a model of choice behavior that the agent can use to interpret the data. For choices between partial trajectories of states and actions, previous models assume choice probabilities to be determined by the partial return or the cumulative advantage.\n  We consider an alternative model based instead on the bootstrapped return, which adds to the partial return an estimate of the future return. Benefits of the bootstrapped return model stem from its treatment of human beliefs. Unlike partial return, choices based on bootstrapped return reflect human beliefs about the environment. Further, while recovering the reward function from choices based on cumulative advantage requires that those beliefs are correct, doing so from choices based on bootstrapped return does not.\n  To motivate the bootstrapped return model, we formulate axioms and prove an Alignment Theorem. This result formalizes how, for a general class of human preferences, such models are able to disentangle goals from beliefs. This ensures recovery of an aligned reward function when learning from choices based on bootstrapped return.\n  The bootstrapped return model also affords greater robustness to choice behavior. Even when choices are based on partial return, learning via a bootstrapped return model recovers an aligned reward function. The same holds with choices based on the cumulative advantage if the human and the agent both adhere to correct and consistent beliefs about the environment. On the other hand, if choices are based on bootstrapped return, learning via partial return or cumulative advantage models does not generally produce an aligned reward function.",
        "published": "2024-10-31 04:00:00",
        "id": "1df1c21b-c3cf-4050-bc54-4c3acee7acc6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出基于自举回报的模型用于解释人类选择数据以引导AI，阐述其在处理人类信念方面的优势、能使目标与信念分离的原理，以及在不同选择基础下的鲁棒性。"
        },
        "tokens": 942
    },
    {
        "title": "Permutation Invariant Learning with High-Dimensional Particle Filters",
        "link": "https://arxiv.org/abs/2410.22695",
        "description": "arXiv:2410.22695v1 Announce Type: new \nAbstract: Sequential learning in deep models often suffers from challenges such as catastrophic forgetting and loss of plasticity, largely due to the permutation dependence of gradient-based algorithms, where the order of training data impacts the learning outcome. In this work, we introduce a novel permutation-invariant learning framework based on high-dimensional particle filters. We theoretically demonstrate that particle filters are invariant to the sequential ordering of training minibatches or tasks, offering a principled solution to mitigate catastrophic forgetting and loss-of-plasticity. We develop an efficient particle filter for optimizing high-dimensional models, combining the strengths of Bayesian methods with gradient-based optimization. Through extensive experiments on continual supervised and reinforcement learning benchmarks, including SplitMNIST, SplitCIFAR100, and ProcGen, we empirically show that our method consistently improves performance, while reducing variance compared to standard baselines.",
        "published": "2024-10-31 04:00:00",
        "id": "96bef44f-0129-4af3-86d1-5d0afb267042",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章介绍一种基于高维粒子滤波器的置换不变学习框架，理论上可解决深度模型顺序学习中的问题，经实验验证其能提升性能并减少方差。"
        },
        "tokens": 767
    },
    {
        "title": "MiniTac: An Ultra-Compact 8 mm Vision-Based Tactile Sensor for Enhanced Palpation in Robot-Assisted Minimally Invasive Surgery",
        "link": "https://arxiv.org/abs/2410.22691",
        "description": "arXiv:2410.22691v1 Announce Type: new \nAbstract: Robot-assisted minimally invasive surgery (RAMIS) provides substantial benefits over traditional open and laparoscopic methods. However, a significant limitation of RAMIS is the surgeon's inability to palpate tissues, a crucial technique for examining tissue properties and detecting abnormalities, restricting the widespread adoption of RAMIS. To overcome this obstacle, we introduce MiniTac, a novel vision-based tactile sensor with an ultra-compact cross-sectional diameter of 8 mm, designed for seamless integration into mainstream RAMIS devices, particularly the Da Vinci surgical systems. MiniTac features a novel mechanoresponsive photonic elastomer membrane that changes color distribution under varying contact pressures. This color change is captured by an embedded miniature camera, allowing MiniTac to detect tumors both on the tissue surface and in deeper layers typically obscured from endoscopic view. MiniTac's efficacy has been rigorously tested on both phantoms and ex-vivo tissues. By leveraging advanced mechanoresponsive photonic materials, MiniTac represents a significant advancement in integrating tactile sensing into RAMIS, potentially expanding its applicability to a wider array of clinical scenarios that currently rely on traditional surgical approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "23733edf-7aa1-4efc-b5f5-ee1ed4280eac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一种用于机器人辅助微创手术的超紧凑视觉触觉传感器MiniTac，阐述其设计、原理、测试情况及其在该领域的意义。"
        },
        "tokens": 836
    },
    {
        "title": "Multi-Task Interactive Robot Fleet Learning with Visual World Models",
        "link": "https://arxiv.org/abs/2410.22689",
        "description": "arXiv:2410.22689v1 Announce Type: new \nAbstract: Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a multi-task interactive robot fleet learning framework to address these challenges. Sirius-Fleet monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate Sirius-Fleet's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website: https://ut-austin-rpl.github.io/sirius-fleet",
        "published": "2024-10-31 04:00:00",
        "id": "018e18b5-169a-4873-9792-e2936fc18b10",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Sirius - Fleet多任务交互式机器人集群学习框架，用视觉世界模型预测行动结果和异常，通过人类干预提升机器人自主性，在大规模基准测试中验证其有效性。"
        },
        "tokens": 828
    },
    {
        "title": "Persistent Homology for MCI Classification: A Comparative Analysis between Graph and Vietoris-Rips Filtrations",
        "link": "https://arxiv.org/abs/2410.22681",
        "description": "arXiv:2410.22681v1 Announce Type: new \nAbstract: Mild cognitive impairment (MCI), often linked to early neurodegeneration, is characterized by subtle cognitive declines and disruptions in brain connectivity. The present study offers a detailed analysis of topological changes associated with MCI, focusing on two subtypes: Early MCI and Late MCI. This analysis utilizes fMRI time series data from two distinct populations: the publicly available ADNI dataset (Western cohort) and the in-house TLSA dataset (Indian Urban cohort). Persistent Homology, a topological data analysis method, is employed with two distinct filtration techniques - Vietoris-Rips and graph filtration-for classifying MCI subtypes. For Vietoris-Rips filtration, inter-ROI Wasserstein distance matrices between persistent diagrams are used for classification, while graph filtration relies on the top ten most persistent homology features. Comparative analysis shows that the Vietoris-Rips filtration significantly outperforms graph filtration, capturing subtle variations in brain connectivity with greater accuracy. The Vietoris-Rips filtration method achieved the highest classification accuracy of 85.7\\% for distinguishing between age and gender matched healthy controls and MCI, whereas graph filtration reached a maximum accuracy of 71.4\\% for the same task. This superior performance highlights the sensitivity of Vietoris-Rips filtration in detecting intricate topological features associated with neurodegeneration. The findings underscore the potential of persistent homology, particularly when combined with the Wasserstein distance, as a powerful tool for early diagnosis and precise classification of cognitive impairments, offering valuable insights into brain connectivity changes in MCI.",
        "published": "2024-10-31 04:00:00",
        "id": "7b5b3d49-f7fd-46fe-a589-698f149d0d1e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "利用fMRI时间序列数据，通过持久同调的两种过滤技术（Vietoris - Rips和图过滤）对MCI亚型进行分类，对比发现Vietoris - Rips过滤技术在区分健康对照组和MCI方面准确性更高。"
        },
        "tokens": 928
    },
    {
        "title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings",
        "link": "https://arxiv.org/abs/2410.22685",
        "description": "arXiv:2410.22685v1 Announce Type: new \nAbstract: Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the-art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional entailment criteria between multiple generated responses and also depend on sequence likelihoods. While effective, these approaches often overestimate uncertainty due to their sensitivity to minor wording differences, additional correct information, and non-important words in the sequence. We propose a novel approach that leverages semantic embeddings to achieve smoother and more robust estimation of semantic uncertainty in LLMs. By capturing semantic similarities without depending on sequence likelihoods, our method inherently reduces any biases introduced by irrelevant words in the answers. Furthermore, we introduce an amortised version of our approach by explicitly modelling semantics as latent variables in a joint probabilistic model. This allows for uncertainty estimation in the embedding space with a single forward pass, significantly reducing computational overhead compared to existing multi-pass methods. Experiments across multiple question-answering datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate and nuanced uncertainty quantification than traditional approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "97f6c303-a53c-4c3d-a9e2-98049c2ad20e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种利用语义嵌入改进大型语言模型语义不确定性量化的新方法，实验表明该方法比传统方法能更准确细致地量化不确定性。"
        },
        "tokens": 819
    },
    {
        "title": "An optimal parallel-in-time preconditioner for parabolic optimal control problems",
        "link": "https://arxiv.org/abs/2410.22686",
        "description": "arXiv:2410.22686v1 Announce Type: new \nAbstract: In this work, we propose a novel diagonalization-based preconditioner for the all-at-once linear system arising from the optimal control problem of parabolic equations. The proposed preconditioner is constructed based on an $\\epsilon$-circulant modification to the rotated block diagonal (RBD) preconditioning technique, which can be efficiently diagonalized by fast Fourier transforms in a parallel-in-time fashion. \\textcolor{black}{To our knowledge, this marks the first application of the $\\epsilon$-circulant modification to RBD preconditioning. Before our work, the studies of PinT preconditioning techniques for the optimal control problem are mainly focused on $\\epsilon$-circulant modification to Schur complement based preconditioners, which involves multiplication of forward and backward evolutionary processes and thus square the condition number. Compared with those Schur complement based preconditioning techniques in the literature, the advantage of the proposed $\\epsilon$-circulant modified RBD preconditioning is that it does not involve the multiplication of forward and backward evolutionary processes. When the generalized minimal residual method is deployed on the preconditioned system, we prove that when choosing $\\epsilon=\\mathcal{O}(\\sqrt{\\tau})$ with $\\tau$ being the temporal step-size , the convergence rate of the preconditioned GMRES solver is independent of the matrix size and the regularization parameter. Such restriction on $\\epsilon$ is more relax than the assumptions on $\\epsilon$ from other works related to $\\epsilon$-circulant based preconditioning techniques for the optimal control problem. Numerical results are provided to demonstrate the effectiveness of our proposed solvers.",
        "published": "2024-10-31 04:00:00",
        "id": "76ede6c0-9d03-4733-bfc2-3e696d8fe111",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出一种基于对角化的预处理器用于抛物型方程最优控制问题产生的线性系统，证明了在特定条件下预条件GMRES求解器收敛率的性质，并通过数值结果展示其有效性。"
        },
        "tokens": 944
    },
    {
        "title": "MassiveGNN: Efficient Training via Prefetching for Massively Connected Distributed Graphs",
        "link": "https://arxiv.org/abs/2410.22697",
        "description": "arXiv:2410.22697v1 Announce Type: new \nAbstract: Graph Neural Networks (GNN) are indispensable in learning from graph-structured data, yet their rising computational costs, especially on massively connected graphs, pose significant challenges in terms of execution performance. To tackle this, distributed-memory solutions such as partitioning the graph to concurrently train multiple replicas of GNNs are in practice. However, approaches requiring a partitioned graph usually suffer from communication overhead and load imbalance, even under optimal partitioning and communication strategies due to irregularities in the neighborhood minibatch sampling.\n  This paper proposes practical trade-offs for improving the sampling and communication overheads for representation learning on distributed graphs (using popular GraphSAGE architecture) by developing a parameterized continuous prefetch and eviction scheme on top of the state-of-the-art Amazon DistDGL distributed GNN framework, demonstrating about 15-40% improvement in end-to-end training performance on the National Energy Research Scientific Computing Center's (NERSC) Perlmutter supercomputer for various OGB datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "4c8c8f61-b195-40d3-8d85-a65054298407",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出MassiveGNN，通过在亚马逊DistDGL分布式GNN框架上开发参数化连续预取和驱逐方案，改善分布式图表示学习的采样和通信开销，在NERSC的Perlmutter超级计算机上对各种OGB数据集的端到端训练性能有15 - 40%的提升。"
        },
        "tokens": 838
    },
    {
        "title": "An Iterative Algorithm for Regularized Non-negative Matrix Factorizations",
        "link": "https://arxiv.org/abs/2410.22698",
        "description": "arXiv:2410.22698v1 Announce Type: new \nAbstract: We generalize the non-negative matrix factorization algorithm of Lee and Seung to accept a weighted norm, and to support ridge and Lasso regularization. We recast the Lee and Seung multiplicative update as an additive update which does not get stuck on zero values. We apply the companion R package rnnmf to the problem of finding a reduced rank representation of a database of cocktails.",
        "published": "2024-10-31 04:00:00",
        "id": "35e9e5a0-584d-413c-afa7-b0b6a1bc02cd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种正则化非负矩阵分解的迭代算法，将Lee和Seung的乘法更新重铸为加法更新，并应用相关R包解决鸡尾酒数据库的降秩表示问题。"
        },
        "tokens": 674
    },
    {
        "title": "Exactly Minimax-Optimal Locally Differentially Private Sampling",
        "link": "https://arxiv.org/abs/2410.22699",
        "description": "arXiv:2410.22699v1 Announce Type: new \nAbstract: The sampling problem under local differential privacy has recently been studied with potential applications to generative models, but a fundamental analysis of its privacy-utility trade-off (PUT) remains incomplete. In this work, we define the fundamental PUT of private sampling in the minimax sense, using the f-divergence between original and sampling distributions as the utility measure. We characterize the exact PUT for both finite and continuous data spaces under some mild conditions on the data distributions, and propose sampling mechanisms that are universally optimal for all f-divergences. Our numerical experiments demonstrate the superiority of our mechanisms over baselines, in terms of theoretical utilities for finite data space and of empirical utilities for continuous data space.",
        "published": "2024-10-31 04:00:00",
        "id": "b8fd010a-2464-4ab5-84e2-9894e10c36cd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究定义了局部差分隐私下私人采样在极小极大意义上的基本隐私 - 效用权衡，刻画了有限和连续数据空间下的精确权衡，并提出通用最优采样机制，数值实验证明其优越性。"
        },
        "tokens": 748
    },
    {
        "title": "Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images",
        "link": "https://arxiv.org/abs/2410.22705",
        "description": "arXiv:2410.22705v1 Announce Type: new \nAbstract: Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed \"geometry cloaks\", into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak. Our project is available at https://qsong2001.github.io/geometry_cloak.",
        "published": "2024-10-31 04:00:00",
        "id": "a9616345-53b3-4ba2-8ed6-ec9f6a23956a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为防止单视图3D重建方法TGS被用于从版权图像创建未经授权的3D模型，提出在图像中嵌入不可见的几何扰动（几何斗篷）的图像保护方法，实验验证了其有效性。"
        },
        "tokens": 835
    },
    {
        "title": "Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization",
        "link": "https://arxiv.org/abs/2410.22707",
        "description": "arXiv:2410.22707v1 Announce Type: new \nAbstract: State recognition of the environment and objects, such as the open/closed state of doors and the on/off of lights, is indispensable for robots that perform daily life support and security tasks. Until now, state recognition methods have been based on training neural networks from manual annotations, preparing special sensors for the recognition, or manually programming to extract features from point clouds or raw images. In contrast, we propose a robotic state recognition method using a pre-trained vision-language model, which is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several kinds of language prompts in advance, calculate the similarity between these prompts and the current image by ITR, and perform state recognition. By applying the optimal weighting to each prompt using black-box optimization, state recognition can be performed with higher accuracy. Experiments show that this theory enables a variety of state recognitions by simply preparing multiple prompts without retraining neural networks or manual programming. In addition, since only prompts and their weights need to be prepared for each recognizer, there is no need to prepare multiple models, which facilitates resource management. It is possible to recognize the open/closed state of transparent doors, the state of whether water is running or not from a faucet, and even the qualitative state of whether a kitchen is clean or not, which have been challenging so far, through language.",
        "published": "2024-10-31 04:00:00",
        "id": "e9471884-2f37-4737-8207-362455a451d2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种使用预训练视觉 - 语言模型的机器人状态识别方法，通过图像到文本检索任务和黑盒优化，利用提前准备的语言提示进行状态识别，实验表明该方法无需重新训练神经网络或手动编程即可实现多种状态识别且便于资源管理。"
        },
        "tokens": 904
    },
    {
        "title": "FilterViT and DropoutViT: Lightweight Vision Transformer Models for Efficient Attention Mechanisms",
        "link": "https://arxiv.org/abs/2410.22709",
        "description": "arXiv:2410.22709v1 Announce Type: new \nAbstract: In this study, we introduce FilterViT, an enhanced version of MobileViT, which leverages an attention-based mechanism for early-stage downsampling. Traditional QKV operations on high-resolution feature maps are computationally intensive due to the abundance of tokens. To address this, we propose a filter attention mechanism using a convolutional neural network (CNN) to generate an importance mask, focusing attention on key image regions. The method significantly reduces computational complexity while maintaining interpretability, as it highlights essential image areas. Experimental results show that FilterViT achieves substantial gains in both efficiency and accuracy compared to other models. We also introduce DropoutViT, a variant that uses a stochastic approach for pixel selection, further enhancing robustness.",
        "published": "2024-10-31 04:00:00",
        "id": "3f628e9d-3732-485e-a3ff-4bd484b5a165",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "FilterViT是MobileViT的增强版，采用基于注意力的机制进行早期下采样，通过卷积神经网络生成重要性掩码以减少计算复杂度并保持可解释性，DropoutViT是其变体，实验表明FilterViT在效率和准确性上有很大提升。"
        },
        "tokens": 773
    },
    {
        "title": "LoFLAT: Local Feature Matching using Focused Linear Attention Transformer",
        "link": "https://arxiv.org/abs/2410.22710",
        "description": "arXiv:2410.22710v1 Announce Type: new \nAbstract: Local feature matching is an essential technique in image matching and plays a critical role in a wide range of vision-based applications. However, existing Transformer-based detector-free local feature matching methods encounter challenges due to the quadratic computational complexity of attention mechanisms, especially at high resolutions. However, while existing Transformer-based detector-free local feature matching methods have reduced computational costs using linear attention mechanisms, they still struggle to capture detailed local interactions, which affects the accuracy and robustness of precise local correspondences. In order to enhance representations of attention mechanisms while preserving low computational complexity, we propose the LoFLAT, a novel Local Feature matching using Focused Linear Attention Transformer in this paper. Our LoFLAT consists of three main modules: the Feature Extraction Module, the Feature Transformer Module, and the Matching Module. Specifically, the Feature Extraction Module firstly uses ResNet and a Feature Pyramid Network to extract hierarchical features. The Feature Transformer Module further employs the Focused Linear Attention to refine attention distribution with a focused mapping function and to enhance feature diversity with a depth-wise convolution. Finally, the Matching Module predicts accurate and robust matches through a coarse-to-fine strategy. Extensive experimental evaluations demonstrate that the proposed LoFLAT outperforms the LoFTR method in terms of both efficiency and accuracy.",
        "published": "2024-10-31 04:00:00",
        "id": "b0598703-fad1-43d5-8053-53be4657a78e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出了一种名为LoFLAT的新型局部特征匹配方法，由特征提取、特征转换和匹配三个主要模块构成，实验证明其在效率和准确性上优于LoFTR方法。"
        },
        "tokens": 864
    },
    {
        "title": "Offline Behavior Distillation",
        "link": "https://arxiv.org/abs/2410.22728",
        "description": "arXiv:2410.22728v1 Announce Type: new \nAbstract: Massive reinforcement learning (RL) data are typically collected to train policies offline without the need for interactions, but the large data volume can cause training inefficiencies. To tackle this issue, we formulate offline behavior distillation (OBD), which synthesizes limited expert behavioral data from sub-optimal RL data, enabling rapid policy learning. We propose two naive OBD objectives, DBC and PBC, which measure distillation performance via the decision difference between policies trained on distilled data and either offline data or a near-expert policy. Due to intractable bi-level optimization, the OBD objective is difficult to minimize to small values, which deteriorates PBC by its distillation performance guarantee with quadratic discount complexity $\\mathcal{O}(1/(1-\\gamma)^2)$. We theoretically establish the equivalence between the policy performance and action-value weighted decision difference, and introduce action-value weighted PBC (Av-PBC) as a more effective OBD objective. By optimizing the weighted decision difference, Av-PBC achieves a superior distillation guarantee with linear discount complexity $\\mathcal{O}(1/(1-\\gamma))$. Extensive experiments on multiple D4RL datasets reveal that Av-PBC offers significant improvements in OBD performance, fast distillation convergence speed, and robust cross-architecture/optimizer generalization.",
        "published": "2024-10-31 04:00:00",
        "id": "8e3be76c-e687-401a-8aae-d28ac76db64c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决大规模强化学习离线训练数据量大导致效率低的问题，提出离线行为蒸馏（OBD），介绍两种目标DBC和PBC并指出PBC的问题，理论上建立策略性能和动作 - 价值加权决策差异的等价性，引入Av - PBC目标，实验表明其在OBD性能、收敛速度和泛化能力上有显著提升。"
        },
        "tokens": 897
    },
    {
        "title": "Exposing Cross-Platform Coordinated Inauthentic Activity in the Run-Up to the 2024 U.S. Election",
        "link": "https://arxiv.org/abs/2410.22716",
        "description": "arXiv:2410.22716v1 Announce Type: new \nAbstract: Coordinated information operations remain a persistent challenge on social media, despite platform efforts to curb them. While previous research has primarily focused on identifying these operations within individual platforms, this study shows that coordination frequently transcends platform boundaries. Leveraging newly collected data of online conversations related to the 2024 U.S. Election across $\\mathbb{X}$ (formerly, Twitter), Facebook, and Telegram, we construct similarity networks to detect coordinated communities exhibiting suspicious sharing behaviors within and across platforms. Proposing an advanced coordination detection model, we reveal evidence of potential foreign interference, with Russian-affiliated media being systematically promoted across Telegram and $\\mathbb{X}$. Our analysis also uncovers substantial intra- and cross-platform coordinated inauthentic activity, driving the spread of highly partisan, low-credibility, and conspiratorial content. These findings highlight the urgent need for regulatory measures that extend beyond individual platforms to effectively address the growing challenge of cross-platform coordinated influence campaigns.",
        "published": "2024-10-31 04:00:00",
        "id": "d6ab6e3d-542f-4888-9366-f47306885569",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究利用新收集的与2024年美国大选相关的跨平台数据构建相似性网络，检测协调社区，发现潜在外国干涉证据、跨平台不真实活动在传播特定不良内容，凸显跨平台监管的紧迫性。"
        },
        "tokens": 822
    },
    {
        "title": "MIXAD: Memory-Induced Explainable Time Series Anomaly Detection",
        "link": "https://arxiv.org/abs/2410.22735",
        "description": "arXiv:2410.22735v1 Announce Type: new \nAbstract: For modern industrial applications, accurately detecting and diagnosing anomalies in multivariate time series data is essential. Despite such need, most state-of-the-art methods often prioritize detection performance over model interpretability. Addressing this gap, we introduce MIXAD (Memory-Induced Explainable Time Series Anomaly Detection), a model designed for interpretable anomaly detection. MIXAD leverages a memory network alongside spatiotemporal processing units to understand the intricate dynamics and topological structures inherent in sensor relationships. We also introduce a novel anomaly scoring method that detects significant shifts in memory activation patterns during anomalies. Our approach not only ensures decent detection performance but also outperforms state-of-the-art baselines by 34.30% and 34.51% in interpretability metrics.",
        "published": "2024-10-31 04:00:00",
        "id": "5e3de6e8-552d-444d-b1f0-b4331e3fd25a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "MIXAD是一种用于可解释异常检测的模型，利用记忆网络和时空处理单元理解传感器关系中的动力学和拓扑结构，新的异常评分方法可检测异常期间记忆激活模式的显著变化，检测性能不错且可解释性指标优于现有基准。"
        },
        "tokens": 775
    },
    {
        "title": "One Prompt to Verify Your Models: Black-Box Text-to-Image Models Verification via Non-Transferable Adversarial Attacks",
        "link": "https://arxiv.org/abs/2410.22725",
        "description": "arXiv:2410.22725v1 Announce Type: new \nAbstract: Recently, the success of Text-to-Image (T2I) models has led to the rise of numerous third-party platforms, which claim to provide cheaper API services and more flexibility in model options. However, this also raises a new security concern: Are these third-party services truly offering the models they claim? To address this problem, we propose the first T2I model verification method named Text-to-Image Model Verification via Non-Transferable Adversarial Attacks (TVN). The non-transferability of adversarial examples means that these examples are only effective on a target model and ineffective on other models, thereby allowing for the verification of the target model. TVN utilizes the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to optimize the cosine similarity of a prompt's text encoding, generating non-transferable adversarial prompts. By calculating the CLIP-text scores between the non-transferable adversarial prompts without perturbations and the images, we can verify if the model matches the claimed target model, based on a 3-sigma threshold. The experiments showed that TVN performed well in both closed-set and open-set scenarios, achieving a verification accuracy of over 90\\%. Moreover, the adversarial prompts generated by TVN significantly reduced the CLIP-text scores of the target model, while having little effect on other models.",
        "published": "2024-10-31 04:00:00",
        "id": "575617cd-49bf-40cb-bfa0-4fb4d70a64fa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对第三方文本到图像（T2I）模型服务是否如其声称提供相应模型的安全担忧，提出名为TVN的T2I模型验证方法，通过NSGA - II算法生成不可迁移的对抗性提示，计算CLIP - text分数验证模型，实验显示其在不同场景下验证准确率超90%。"
        },
        "tokens": 910
    },
    {
        "title": "SCRREAM : SCan, Register, REnder And Map:A Framework for Annotating Accurate and Dense 3D Indoor Scenes with a Benchmark",
        "link": "https://arxiv.org/abs/2410.22715",
        "description": "arXiv:2410.22715v1 Announce Type: new \nAbstract: Traditionally, 3d indoor datasets have generally prioritized scale over ground-truth accuracy in order to obtain improved generalization. However, using these datasets to evaluate dense geometry tasks, such as depth rendering, can be problematic as the meshes of the dataset are often incomplete and may produce wrong ground truth to evaluate the details. In this paper, we propose SCRREAM, a dataset annotation framework that allows annotation of fully dense meshes of objects in the scene and registers camera poses on the real image sequence, which can produce accurate ground truth for both sparse 3D as well as dense 3D tasks. We show the details of the dataset annotation pipeline and showcase four possible variants of datasets that can be obtained from our framework with example scenes, such as indoor reconstruction and SLAM, scene editing & object removal, human reconstruction and 6d pose estimation. Recent pipelines for indoor reconstruction and SLAM serve as new benchmarks. In contrast to previous indoor dataset, our design allows to evaluate dense geometry tasks on eleven sample scenes against accurately rendered ground truth depth maps.",
        "published": "2024-10-31 04:00:00",
        "id": "8b361f27-f431-4165-b1ee-fa3d1103f65c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出SCRREAM数据集注释框架，可对场景中物体全密集网格进行注释并在真实图像序列上注册相机姿态，以产生准确的地面真值，展示了数据集注释管道细节和四种可能的数据集变体，并可用于评估密集几何任务。"
        },
        "tokens": 850
    },
    {
        "title": "Extensional Properties of Recurrent Neural Networks",
        "link": "https://arxiv.org/abs/2410.22730",
        "description": "arXiv:2410.22730v1 Announce Type: new \nAbstract: A property of a recurrent neural network (RNN) is called \\emph{extensional} if, loosely speaking, it is a property of the function computed by the RNN rather than a property of the RNN algorithm. Many properties of interest in RNNs are extensional, for example, robustness against small changes of input or good clustering of inputs. Given an RNN, it is natural to ask whether it has such a property. We give a negative answer to the general question about testing extensional properties of RNNs. Namely, we prove a version of Rice's theorem for RNNs: any nontrivial extensional property of RNNs is undecidable.",
        "published": "2024-10-31 04:00:00",
        "id": "3d096d9e-74b4-487b-aa98-6207d9af52ac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究递归神经网络（RNN）的外延属性，证明了RNN的非平凡外延属性是不可判定的，即回答了测试RNN外延属性的一般性问题。"
        },
        "tokens": 734
    },
    {
        "title": "Community search signatures as foundation features for human-centered geospatial modeling",
        "link": "https://arxiv.org/abs/2410.22721",
        "description": "arXiv:2410.22721v1 Announce Type: new \nAbstract: Aggregated relative search frequencies offer a unique composite signal reflecting people's habits, concerns, interests, intents, and general information needs, which are not found in other readily available datasets. Temporal search trends have been successfully used in time series modeling across a variety of domains such as infectious diseases, unemployment rates, and retail sales. However, most existing applications require curating specialized datasets of individual keywords, queries, or query clusters, and the search data need to be temporally aligned with the outcome variable of interest. We propose a novel approach for generating an aggregated and anonymized representation of search interest as foundation features at the community level for geospatial modeling. We benchmark these features using spatial datasets across multiple domains. In zip codes with a population greater than 3000 that cover over 95% of the contiguous US population, our models for predicting missing values in a 20% set of holdout counties achieve an average $R^2$ score of 0.74 across 21 health variables, and 0.80 across 6 demographic and environmental variables. Our results demonstrate that these search features can be used for spatial predictions without strict temporal alignment, and that the resulting models outperform spatial interpolation and state of the art methods using satellite imagery features.",
        "published": "2024-10-31 04:00:00",
        "id": "dc8ef33a-2add-423c-bf23-1b4f9efc70fd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新方法生成搜索兴趣的聚合和匿名表示作为社区层面地理空间建模的基础特征，通过多领域空间数据集进行基准测试并展示良好效果。"
        },
        "tokens": 855
    },
    {
        "title": "Constructing Multimodal Datasets from Scratch for Rapid Development of a Japanese Visual Language Model",
        "link": "https://arxiv.org/abs/2410.22736",
        "description": "arXiv:2410.22736v1 Announce Type: new \nAbstract: To develop high-performing Visual Language Models (VLMs), it is essential to prepare multimodal resources, such as image-text pairs, interleaved data, and instruction data. While multimodal resources for English are abundant, there is a significant lack of corresponding resources for non-English languages, such as Japanese. To address this problem, we take Japanese as a non-English language and propose a method for rapidly creating Japanese multimodal datasets from scratch. We collect Japanese image-text pairs and interleaved data from web archives and generate Japanese instruction data directly from images using an existing VLM. Our experimental results show that a VLM trained on these native datasets outperforms those relying on machine-translated content.",
        "published": "2024-10-31 04:00:00",
        "id": "2480660a-b149-4d66-bcc1-c28976b17b1f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种从零开始快速创建日本多模态数据集的方法，实验表明基于这些原生数据集训练的视觉语言模型性能优于依赖机器翻译内容的模型。"
        },
        "tokens": 742
    },
    {
        "title": "Toward Designing Accessible and Meaningful Software for Cancer Survivors",
        "link": "https://arxiv.org/abs/2410.22740",
        "description": "arXiv:2410.22740v1 Announce Type: new \nAbstract: Cancer survivors experience a wide range of impairments arising from cancer or its treatment, such as chemo brain, visual impairments, and physical impairments. These impairments degrade their quality of life and potentially make software use more challenging for them. However, there has been limited research on designing accessible software for cancer survivors. To bridge this research gap, we conducted a formative study including a survey (n=46), semi-structured interviews (n=20), and a diary study (n=10) with cancer survivors. Our results revealed a wide range of impairments experienced by cancer survivors, including chemo brain, neuropathy, and visual impairments. Cancer survivors heavily relied on software for socialization, health purposes, and cancer advocacy, but their impairments made software use more challenging for them. Based on the results, we offer a set of accessibility guidelines that software designers can utilize when creating applications for cancer survivors. Further, we suggest design features for inclusion, such as health resources, socialization tools, and games, tailored to the needs of cancer survivors. This research aims to spotlight cancer survivors' software accessibility challenges and software needs and invite more research in this important yet under-investigated domain.",
        "published": "2024-10-31 04:00:00",
        "id": "7183eb0f-0664-4419-b46f-2ab4ad88fd2b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对癌症幸存者因癌症或治疗产生多种功能障碍导致软件使用困难，研究人员进行了包括调查、访谈和日记研究的形成性研究，揭示了相关功能障碍情况、幸存者对软件的依赖，提供了软件设计的无障碍指南并建议了一些设计功能。"
        },
        "tokens": 856
    },
    {
        "title": "Designing AI Personalities: Enhancing Human-Agent Interaction Through Thoughtful Persona Design",
        "link": "https://arxiv.org/abs/2410.22744",
        "description": "arXiv:2410.22744v1 Announce Type: new \nAbstract: In the rapidly evolving field of artificial intelligence (AI) agents, designing the agent's characteristics is crucial for shaping user experience. This workshop aims to establish a research community focused on AI agent persona design for various contexts, such as in-car assistants, educational tools, and smart home environments. We will explore critical aspects of persona design, such as voice, embodiment, and demographics, and their impact on user satisfaction and engagement. Through discussions and hands-on activities, we aim to propose practices and standards that enhance the ecological validity of agent personas. Topics include the design of conversational interfaces, the influence of agent personas on user experience, and approaches for creating contextually appropriate AI agents. This workshop will provide a platform for building a community dedicated to developing AI agent personas that better fit diverse, everyday interactions.",
        "published": "2024-10-31 04:00:00",
        "id": "3722de44-37bd-4d71-aef2-e2c223e6bd8a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 3
            },
            "keyFacts": "一篇关于人工智能领域中AI智能体角色设计的研究，旨在建立专注于不同场景下AI智能体角色设计的研究社区，探索角色设计的关键方面及其对用户满意度和参与度的影响，并通过讨论和实践活动提出提升角色生态有效性的实践和标准。"
        },
        "tokens": 784
    },
    {
        "title": "Analysis of Classifier Training on Synthetic Data for Cross-Domain Datasets",
        "link": "https://arxiv.org/abs/2410.22748",
        "description": "arXiv:2410.22748v1 Announce Type: new \nAbstract: A major challenges of deep learning (DL) is the necessity to collect huge amounts of training data. Often, the lack of a sufficiently large dataset discourages the use of DL in certain applications. Typically, acquiring the required amounts of data costs considerable time, material and effort. To mitigate this problem, the use of synthetic images combined with real data is a popular approach, widely adopted in the scientific community to effectively train various detectors. In this study, we examined the potential of synthetic data-based training in the field of intelligent transportation systems. Our focus is on camera-based traffic sign recognition applications for advanced driver assistance systems and autonomous driving. The proposed augmentation pipeline of synthetic datasets includes novel augmentation processes such as structured shadows and gaussian specular highlights. A well-known DL model was trained with different datasets to compare the performance of synthetic and real image-based trained models. Additionally, a new, detailed method to objectively compare these models is proposed. Synthetic images are generated using a semi-supervised errors-guide method which is also described. Our experiments showed that a synthetic image-based approach outperforms in most cases real image-based training when applied to cross-domain test datasets (+10% precision for GTSRB dataset) and consequently, the generalization of the model is improved decreasing the cost of acquiring images.",
        "published": "2024-10-31 04:00:00",
        "id": "2a86b106-f3ea-4e3e-92bb-c0adf166a961",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究检验了智能交通系统领域基于合成数据训练的潜力，采用包含新增强过程的合成数据集增强管道，用半监督误差引导方法生成合成图像，用不同数据集训练深度学习模型并提出新的比较方法，实验表明合成图像方法在跨域测试数据集上大多优于真实图像训练，提高模型泛化能力并降低图像获取成本。"
        },
        "tokens": 895
    },
    {
        "title": "Understanding Aggregations of Proper Learners in Multiclass Classification",
        "link": "https://arxiv.org/abs/2410.22749",
        "description": "arXiv:2410.22749v1 Announce Type: new \nAbstract: Multiclass learnability is known to exhibit a properness barrier: there are learnable classes which cannot be learned by any proper learner. Binary classification faces no such barrier for learnability, but a similar one for optimal learning, which can in general only be achieved by improper learners. Fortunately, recent advances in binary classification have demonstrated that this requirement can be satisfied using aggregations of proper learners, some of which are strikingly simple. This raises a natural question: to what extent can simple aggregations of proper learners overcome the properness barrier in multiclass classification?\n  We give a positive answer to this question for classes which have finite Graph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners of Hanneke, Larsen, and Aden-Ali et al. (appropriately generalized to the multiclass setting) achieve sample complexity $O\\left(\\frac{d_G + \\ln(1 / \\delta)}{\\epsilon}\\right)$. This forms a strict improvement upon the sample complexity of ERM. We complement this with a lower bound demonstrating that for certain classes of Graph dimension $d_G$, majorities of ERM learners require $\\Omega \\left( \\frac{d_G + \\ln(1 / \\delta)}{\\epsilon}\\right)$ samples. Furthermore, we show that a single ERM requires $\\Omega \\left(\\frac{d_G \\ln(1 / \\epsilon) + \\ln(1 / \\delta)}{\\epsilon}\\right)$ samples on such classes, exceeding the lower bound of Daniely et al. (2015) by a factor of $\\ln(1 / \\epsilon)$. For multiclass learning in full generality -- i.e., for classes of finite DS dimension but possibly infinite Graph dimension -- we give a strong refutation to these learning strategies, by exhibiting a learnable class which cannot be learned to constant error by any aggregation of a finite number of proper learners.",
        "published": "2024-10-31 04:00:00",
        "id": "430bdb38-3c2a-404c-aba6-e7fc80567191",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究多类分类中简单的合适学习者聚合能在多大程度上克服合适性障碍，对有限图维度的类给出积极答案，也给出下限证明和对一般多类学习的反驳。"
        },
        "tokens": 999
    },
    {
        "title": "SoftCTRL: Soft conservative KL-control of Transformer Reinforcement Learning for Autonomous Driving",
        "link": "https://arxiv.org/abs/2410.22752",
        "description": "arXiv:2410.22752v1 Announce Type: new \nAbstract: In recent years, motion planning for urban self-driving cars (SDV) has become a popular problem due to its complex interaction of road components. To tackle this, many methods have relied on large-scale, human-sampled data processed through Imitation learning (IL). Although effective, IL alone cannot adequately handle safety and reliability concerns. Combining IL with Reinforcement learning (RL) by adding KL divergence between RL and IL policy to the RL loss can alleviate IL's weakness but suffer from over-conservation caused by covariate shift of IL. To address this limitation, we introduce a method that combines IL with RL using an implicit entropy-KL control that offers a simple way to reduce the over-conservation characteristic. In particular, we validate different challenging simulated urban scenarios from the unseen dataset, indicating that although IL can perform well in imitation tasks, our proposed method significantly improves robustness (over 17\\% reduction in failures) and generates human-like driving behavior.",
        "published": "2024-10-31 04:00:00",
        "id": "983fe06e-13a7-46fe-9c19-db20e2496b92",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种将模仿学习与强化学习相结合的方法SoftCTRL，通过隐式熵 - KL控制减少过度保守性，在未见过的数据集的模拟城市场景中验证，能显著提高鲁棒性并生成类人驾驶行为。"
        },
        "tokens": 813
    },
    {
        "title": "Synthesis of Timeline-Based Planning Strategies Avoiding Determinization",
        "link": "https://arxiv.org/abs/2410.22757",
        "description": "arXiv:2410.22757v1 Announce Type: new \nAbstract: Qualitative timeline-based planning models domains as sets of independent, but interacting, components whose behaviors over time, the timelines, are governed by sets of qualitative temporal constraints (ordering relations), called synchronization rules. Its plan-existence problem has been shown to be PSPACE-complete; in particular, PSPACE-membership has been proved via reduction to the nonemptiness problem for nondeterministic finite automata. However, nondeterministic automata cannot be directly used to synthesize planning strategies as a costly determinization step is needed. In this paper, we identify a large fragment of qualitative timeline-based planning whose plan-existence problem can be directly mapped into the nonemptiness problem of deterministic finite automata, which can then be exploited to synthesize strategies. In addition, we identify a  maximal subset of Allen's relations that fits into such a deterministic fragment.",
        "published": "2024-10-31 04:00:00",
        "id": "985f4345-b3f0-4766-b5b9-37d87367e8e3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨定性基于时间线的规划，确定其计划存在问题可映射到确定性有限自动机的非空性问题的一个大分支，还确定了符合该确定性分支的Allen关系的最大子集。"
        },
        "tokens": 782
    },
    {
        "title": "Reactive Synthesis for Expected Impacts",
        "link": "https://arxiv.org/abs/2410.22760",
        "description": "arXiv:2410.22760v1 Announce Type: new \nAbstract: As business processes become increasingly complex,  effectively modeling decision points, their likelihood,  and resource consumption is crucial for optimizing operations.  To address this challenge, this paper introduces a formal  extension of the Business Process Model and Notation (BPMN)  that incorporates choices, probabilities, and impacts,  referred to as BPMN+CPI. This extension is motivated  by the growing emphasis on precise control within  business process management, where carefully  selecting decision pathways in repeated instances  is crucial for conforming to certain standards of multiple resource consumption and environmental impacts.  In this context we deal with the problem of synthesizing a  strategy (if any) that guarantees that the expected impacts on repeated execution of the input process  are below a given threshold.  We show that this problem belongs to   PSPACE complexity class; moreover we provide an effective procedure  for computing a strategy (if present).",
        "published": "2024-10-31 04:00:00",
        "id": "775dcca9-80ff-49c1-8338-73f176ad7ce4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为应对业务流程日益复杂的挑战，本文引入BPMN+CPI扩展，解决合成策略问题，使输入流程重复执行的预期影响低于给定阈值，证明该问题属于PSPACE复杂度类并提供计算策略的有效程序。"
        },
        "tokens": 786
    },
    {
        "title": "Deterministic Suffix-reading Automata",
        "link": "https://arxiv.org/abs/2410.22761",
        "description": "arXiv:2410.22761v1 Announce Type: new \nAbstract: We introduce deterministic suffix-reading automata (DSA), a new automaton model over finite words. Transitions in a DSA are labeled with words. From a state, a DSA triggers an outgoing transition on seeing a word ending with the transition's label. Therefore, rather than moving along an input word letter by letter, a DSA can jump along blocks of letters, with each block ending in a suitable suffix. This feature allows DSAs to recognize regular languages more concisely, compared to DFAs. In this work, we focus on questions around finding a \"minimal\" DSA for a regular language. The number of states is not a faithful measure of the size of a DSA, since the transition-labels contain strings of arbitrary length. Hence, we consider total-size (number of states + number of edges + total length of transition-labels) as the size measure of DSAs.\n  We start by formally defining the model and providing a DSA-to-DFA conversion that allows to compare the expressiveness and succinctness of DSA with related automata models.  Our main technical contribution is a method to derive DSAs from a given DFA: a DFA-to-DSA conversion. We make a surprising observation that the smallest DSA derived from the canonical DFA of a regular language L need not be a minimal DSA for L. This observation leads to a fundamental bottleneck in deriving a minimal DSA for a regular language. In fact, we prove that given a DFA and a number k, the problem of deciding if there exists an equivalent DSA of total-size at most k is NP-complete.",
        "published": "2024-10-31 04:00:00",
        "id": "089cd41a-6dea-4188-a6ce-210d9de4735a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了确定性后缀读取自动机（DSA）这一有限单词的自动机模型，包括其定义、DSA - DFA转换、从DFA导出DSA的方法、最小DSA相关的观察与证明等内容。"
        },
        "tokens": 942
    },
    {
        "title": "FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple Reference Images",
        "link": "https://arxiv.org/abs/2410.22771",
        "description": "arXiv:2410.22771v1 Announce Type: new \nAbstract: Facial parts swapping aims to selectively transfer regions of interest from the source image onto the target image while maintaining the rest of the target image unchanged. Most studies on face swapping designed specifically for full-face swapping, are either unable or significantly limited when it comes to swapping individual facial parts, which hinders fine-grained and customized character designs. However, designing such an approach specifically for facial parts swapping is challenged by a reasonable multiple reference feature fusion, which needs to be both efficient and effective. To overcome this challenge, FuseAnyPart is proposed to facilitate the seamless \"fuse-any-part\" customization of the face. In FuseAnyPart, facial parts from different people are assembled into a complete face in latent space within the Mask-based Fusion Module. Subsequently, the consolidated feature is dispatched to the Addition-based Injection Module for fusion within the UNet of the diffusion model to create novel characters. Extensive experiments qualitatively and quantitatively validate the superiority and robustness of FuseAnyPart. Source codes are available at https://github.com/Thomas-wyh/FuseAnyPart.",
        "published": "2024-10-31 04:00:00",
        "id": "e7ca34ff-54c8-4011-b300-be0c79cb8869",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "FuseAnyPart提出通过多参考图像实现扩散驱动的面部局部交换，包括基于掩码的融合模块和基于加法的注入模块，实验验证了其优越性和鲁棒性且代码已开源。"
        },
        "tokens": 820
    },
    {
        "title": "Self-Driving Car Racing: Application of Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.22766",
        "description": "arXiv:2410.22766v1 Announce Type: new \nAbstract: This paper explores the application of deep reinforcement learning (RL) techniques in the domain of autonomous self-driving car racing. Motivated by the rise of AI-driven mobility and autonomous racing events, the project aims to develop an AI agent that efficiently drives a simulated car in the OpenAI Gymnasium CarRacing environment. We investigate various RL algorithms, including Deep Q-Network (DQN), Proximal Policy Optimization (PPO), and novel adaptations that incorporate transfer learning and recurrent neural networks (RNNs) for enhanced performance. The project demonstrates that while DQN provides a strong baseline for policy learning, integrating ResNet and LSTM models significantly improves the agent's ability to capture complex spatial and temporal dynamics. PPO, particularly in continuous action spaces, shows promising results for fine control, although challenges such as policy collapse remain. We compare the performance of these approaches and outline future research directions focused on improving computational efficiency and addressing model stability. Our findings contribute to the ongoing development of AI systems in autonomous driving and related control tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "44243168-5137-4df1-a56a-bcb43bb0dca8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨深度强化学习技术在自动驾驶赛车领域的应用，研究多种强化学习算法，包括DQN、PPO及其改进版，比较它们的性能并指出未来研究方向。"
        },
        "tokens": 807
    },
    {
        "title": "Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot",
        "link": "https://arxiv.org/abs/2410.22767",
        "description": "arXiv:2410.22767v1 Announce Type: new \nAbstract: Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontology-less DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots.",
        "published": "2024-10-31 04:00:00",
        "id": "ac31aeff-e70f-4192-bb77-c95387129bf6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新方法提升对话状态跟踪（DST）性能，无需预定义本体，在无本体DST模型中达到先进水平并适用于开放域对话。"
        },
        "tokens": 814
    },
    {
        "title": "InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models",
        "link": "https://arxiv.org/abs/2410.22770",
        "description": "arXiv:2410.22770v1 Announce Type: new \nAbstract: Prompt injection attacks pose a critical threat to large language models (LLMs), enabling goal hijacking and data leakage. Prompt guard models, though effective in defense, suffer from over-defense -- falsely flagging benign inputs as malicious due to trigger word bias. To address this issue, we introduce NotInject, an evaluation dataset that systematically measures over-defense across various prompt guard models. NotInject contains 339 benign samples enriched with trigger words common in prompt injection attacks, enabling fine-grained evaluation. Our results show that state-of-the-art models suffer from over-defense issues, with accuracy dropping close to random guessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt guard model that incorporates a new training strategy, Mitigating Over-defense for Free (MOF), which significantly reduces the bias on trigger words. InjecGuard demonstrates state-of-the-art performance on diverse benchmarks including NotInject, surpassing the existing best model by 30.8%, offering a robust and open-source solution for detecting prompt injection attacks. The code and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard.",
        "published": "2024-10-31 04:00:00",
        "id": "d9b0f930-96fd-4ad3-b0c5-47fc6ba4f9a6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决提示防护模型中的过度防御问题，引入评估数据集NotInject，提出InjecGuard模型并采用新训练策略MOF，其性能超越现有最佳模型30.8%且代码和数据集已开源。"
        },
        "tokens": 852
    },
    {
        "title": "A Game-Theoretic Approach for Security Control Selection",
        "link": "https://arxiv.org/abs/2410.22762",
        "description": "arXiv:2410.22762v1 Announce Type: new \nAbstract: Selecting the combination of security controls that will most effectively protect a system's assets is a difficult task. If the wrong controls are selected, the system may be left vulnerable to cyber-attacks that can impact the confidentiality, integrity and availability of critical data and services. In practical settings, it is not possible to select and implement every control possible. Instead considerations, such as budget, effectiveness, and dependencies among various controls, must be considered to choose a combination of security controls that best achieve a set of system security objectives. In this paper, we propose a game-theoretic approach for selecting effective combinations of security controls based on expected attacker profiles and a set budget. The control selection problem is set up as a two-person zero-sum one-shot game. Valid control combinations for selection are generated using an algebraic formalism to account for dependencies among selected controls. We demonstrate the proposed approach on an illustrative financial system used in government departments under four different scenarios. The results illustrate how a security analyst can use the proposed approach to guide and support decision-making in the control selection activity when developing secure systems.",
        "published": "2024-10-31 04:00:00",
        "id": "e5b84c0d-2e35-4cd4-8cac-05817abcfcc7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于预期攻击者特征和设定预算选择有效安全控制组合的博弈论方法，用代数形式主义生成有效控制组合，通过政府部门金融系统的四个场景进行演示。"
        },
        "tokens": 813
    },
    {
        "title": "Reliability Assessment of Information Sources Based on Random Permutation Set",
        "link": "https://arxiv.org/abs/2410.22772",
        "description": "arXiv:2410.22772v1 Announce Type: new \nAbstract: In pattern recognition, handling uncertainty is a critical challenge that significantly affects decision-making and classification accuracy. Dempster-Shafer Theory (DST) is an effective reasoning framework for addressing uncertainty, and the Random Permutation Set (RPS) extends DST by additionally considering the internal order of elements, forming a more ordered extension of DST. However, there is a lack of a transformation method based on permutation order between RPS and DST, as well as a sequence-based probability transformation method for RPS. Moreover, the reliability of RPS sources remains an issue that requires attention. To address these challenges, this paper proposes an RPS transformation approach and a probability transformation method tailored for RPS. On this basis, a reliability computation method for RPS sources, based on the RPS probability transformation, is introduced and applied to pattern recognition. Experimental results demonstrate that the proposed approach effectively bridges the gap between DST and RPS and achieves superior recognition accuracy in classification problems.",
        "published": "2024-10-31 04:00:00",
        "id": "69d71d99-608a-4d91-9c27-966146b08b13",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出RPS变换方法和概率变换方法，并基于RPS概率变换给出RPS源可靠性计算方法，应用于模式识别，以解决DST与RPS之间转换及RPS源可靠性等问题并提高分类识别准确率。"
        },
        "tokens": 815
    },
    {
        "title": "Diffusion Beats Autoregressive: An Evaluation of Compositional Generation in Text-to-Image Models",
        "link": "https://arxiv.org/abs/2410.22775",
        "description": "arXiv:2410.22775v1 Announce Type: new \nAbstract: Text-to-image (T2I) generative models, such as Stable Diffusion and DALL-E, have shown remarkable proficiency in producing high-quality, realistic, and natural images from textual descriptions. However, these models sometimes fail to accurately capture all the details specified in the input prompts, particularly concerning entities, attributes, and spatial relationships. This issue becomes more pronounced when the prompt contains novel or complex compositions, leading to what are known as compositional generation failure modes. Recently, a new open-source diffusion-based T2I model, FLUX, has been introduced, demonstrating strong performance in high-quality image generation. Additionally, autoregressive T2I models like LlamaGen have claimed competitive visual quality performance compared to diffusion-based models. In this study, we evaluate the compositional generation capabilities of these newly introduced models against established models using the T2I-CompBench benchmark. Our findings reveal that LlamaGen, as a vanilla autoregressive model, is not yet on par with state-of-the-art diffusion models for compositional generation tasks under the same criteria, such as model size and inference time. On the other hand, the open-source diffusion-based model FLUX exhibits compositional generation capabilities comparable to the state-of-the-art closed-source model DALL-E3.",
        "published": "2024-10-31 04:00:00",
        "id": "436d2978-7d64-4105-bc82-62553788bece",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文对比了新推出的文本到图像模型（如FLUX、LlamaGen）与已有的模型（如Stable Diffusion、DALL - E）在组合生成能力方面的表现，发现LlamaGen在组合生成任务上不如现有扩散模型，而FLUX表现可与DALL - E3相媲美。"
        },
        "tokens": 896
    },
    {
        "title": "Conflux-PSRO: Effectively Leveraging Collective Advantages in Policy Space Response Oracles",
        "link": "https://arxiv.org/abs/2410.22776",
        "description": "arXiv:2410.22776v1 Announce Type: new \nAbstract: Policy Space Response Oracle (PSRO) with policy population construction has been demonstrated as an effective method for approximating Nash Equilibrium (NE) in zero-sum games. Existing studies have attempted to improve diversity in policy space, primarily by incorporating diversity regularization into the Best Response (BR). However, these methods cause the BR to deviate from maximizing rewards, easily resulting in a population that favors diversity over performance, even when diversity is not always necessary. Consequently, exploitability is difficult to reduce until policies are fully explored, especially in complex games. In this paper, we propose Conflux-PSRO, which fully exploits the diversity of the population by adaptively selecting and training policies at state-level. Specifically, Conflux-PSRO identifies useful policies from the existing population and employs a routing policy to select the most appropriate policies at each decision point, while simultaneously training them to enhance their effectiveness. Compared to the single-policy BR of traditional PSRO and its diversity-improved variants, the BR generated by Conflux-PSRO not only leverages the specialized expertise of diverse policies but also synergistically enhances overall performance. Our experiments on various environments demonstrate that Conflux-PSRO significantly improves the utility of BRs and reduces exploitability compared to existing methods.",
        "published": "2024-10-31 04:00:00",
        "id": "0577fe4a-361e-4852-8399-5cafdf0dd016",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出Conflux - PSRO，它通过在状态层面自适应选择和训练策略来充分利用策略群体的多样性，实验表明其比现有方法能显著提高最佳响应的效用并降低可利用性。"
        },
        "tokens": 867
    },
    {
        "title": "Bregman implementation of Meyer's $G-$norm for cartoon + textures decomposition",
        "link": "https://arxiv.org/abs/2410.22777",
        "description": "arXiv:2410.22777v1 Announce Type: new \nAbstract: In this paper, we design a very simple algorithm based on Split Bregman iterations to numerically solve the cartoon + textures decomposition model of Meyer. This results in a significant gain in speed compared to Chambolle's nonlinear projectors.",
        "published": "2024-10-31 04:00:00",
        "id": "49467dd1-2b87-4fd8-9924-f223e8e7a170",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "计算机科学",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "设计了基于分裂布雷格曼迭代的简单算法来数值求解迈耶卡通+纹理分解模型，相比尚博勒的非线性投影仪速度显著提升。"
        },
        "tokens": 647
    },
    {
        "title": "Signal Processing via Cross-Dimensional Projection",
        "link": "https://arxiv.org/abs/2410.22779",
        "description": "arXiv:2410.22779v1 Announce Type: new \nAbstract: Using projection between Euclidian spaces of different dimensions, the signal compression and decompression become straightforward. This encoding/decoding technique requires no preassigned measuring matrix as in compressed sensing. Moreover, in application there is no dimension or size restrictions. General formulas for encoding/decoding of any finite dimensional signals are provided. Their main properties are revealed. Particularly, it is shown that under the equivalence assumption the technique provides the best approximation with least square error.",
        "published": "2024-10-31 04:00:00",
        "id": "e4b5fe27-6a54-49c1-9949-b2dfad09e6d0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "通过不同维度欧几里得空间之间的投影实现信号压缩和解压缩，无需预分配测量矩阵，给出编码/解码通用公式并揭示主要特性。"
        },
        "tokens": 682
    },
    {
        "title": "MALoRA: Mixture of Asymmetric Low-Rank Adaptation for Enhanced Multi-Task Learning",
        "link": "https://arxiv.org/abs/2410.22782",
        "description": "arXiv:2410.22782v1 Announce Type: new \nAbstract: Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA have significantly improved the adaptation of LLMs to downstream tasks in a resource-efficient manner. However, in multi-task scenarios, challenges such as training imbalance and the seesaw effect frequently emerge. Mixture-of-LoRA (MoLoRA), which combines LoRA with sparse Mixture-of-Experts, mitigates some of these issues by promoting task-specific learning across experts. Despite this, MoLoRA remains inefficient in terms of training speed, parameter utilization, and overall multi-task performance. In this paper, we propose Mixture of Asymmetric Low-Rank Adaptaion (MALoRA), a flexible fine-tuning framework that leverages asymmetric optimization across LoRA experts. MALoRA reduces the number of trainable parameters by 30% to 48%, increases training speed by 1.2x, and matches the computational efficiency of single-task LoRA models. Additionally, MALoRA addresses overfitting issues commonly seen in high-rank configurations, enhancing performance stability. Extensive experiments across diverse multi-task learning scenarios demonstrate that MALoRA consistently outperforms all baseline methods in both inter-domain and intra-domain tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "8c93d554-410b-4e9b-808e-ae081564d587",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出MALoRA这一微调框架，可减少可训练参数、提高训练速度、解决过拟合问题并在多任务学习场景中优于基线方法。"
        },
        "tokens": 855
    },
    {
        "title": "Contrastive Learning and Adversarial Disentanglement for Privacy-Preserving Task-Oriented Semantic Communications",
        "link": "https://arxiv.org/abs/2410.22784",
        "description": "arXiv:2410.22784v1 Announce Type: new \nAbstract: Task-oriented semantic communication systems have emerged as a promising approach to achieving efficient and intelligent data transmission, where only information relevant to a specific task is communicated. However, existing methods struggle to fully disentangle task-relevant and task-irrelevant information, leading to privacy concerns and subpar performance. To address this, we propose an information-bottleneck method, named CLAD (contrastive learning and adversarial disentanglement). CLAD leverages contrastive learning to effectively capture task-relevant features while employing adversarial disentanglement to discard task-irrelevant information. Additionally, due to the lack of reliable and reproducible methods to gain insight into the informativeness and minimality of the encoded feature vectors, we introduce a new technique to compute the information retention index (IRI), a comparative metric used as a proxy for the mutual information between the encoded features and the input, reflecting the minimality of the encoded features. The IRI quantifies the minimality and informativeness of the encoded feature vectors across different task-oriented communication techniques. Our extensive experiments demonstrate that CLAD outperforms state-of-the-art baselines in terms of task performance, privacy preservation, and IRI. CLAD achieves a predictive performance improvement of around 2.5-3%, along with a 77-90% reduction in IRI and a 57-76% decrease in adversarial accuracy.",
        "published": "2024-10-31 04:00:00",
        "id": "a69c3ed5-54d7-46f8-a451-b56c673899c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为CLAD的信息瓶颈方法，通过对比学习和对抗解缠解决任务导向语义通信系统中任务相关和无关信息难以完全解缠的问题，引入计算信息保留指数的新技术，实验表明CLAD在任务性能、隐私保护和信息保留指数方面优于现有基准。"
        },
        "tokens": 919
    },
    {
        "title": "Theoretical Investigations and Practical Enhancements on Tail Task Risk Minimization in Meta Learning",
        "link": "https://arxiv.org/abs/2410.22788",
        "description": "arXiv:2410.22788v1 Announce Type: new \nAbstract: Meta learning is a promising paradigm in the era of large models and task distributional robustness has become an indispensable consideration in real-world scenarios. Recent advances have examined the effectiveness of tail task risk minimization in fast adaptation robustness improvement \\citep{wang2023simple}. This work contributes to more theoretical investigations and practical enhancements in the field. Specifically, we reduce the distributionally robust strategy to a max-min optimization problem, constitute the Stackelberg equilibrium as the solution concept, and estimate the convergence rate. In the presence of tail risk, we further derive the generalization bound, establish connections with estimated quantiles, and practically improve the studied strategy. Accordingly, extensive evaluations demonstrate the significance of our proposal and its scalability to multimodal large models in boosting robustness.",
        "published": "2024-10-31 04:00:00",
        "id": "01ff892b-0941-4f1b-b47b-8220e1ba1c8e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文对元学习中的尾任务风险最小化进行更多理论研究和实践改进，将分布鲁棒策略简化为最大 - 最小优化问题并估计收敛率，在尾风险存在时推导泛化界并改进策略，评估显示其对多模态大模型提升鲁棒性的意义。"
        },
        "tokens": 783
    },
    {
        "title": "Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation",
        "link": "https://arxiv.org/abs/2410.22790",
        "description": "arXiv:2410.22790v1 Announce Type: new \nAbstract: Sequential recommender systems (SRSs) aim to predict the subsequent items which may interest users via comprehensively modeling users' complex preference embedded in the sequence of user-item interactions. However, most of existing SRSs often model users' single low-level preference based on item ID information while ignoring the high-level preference revealed by item attribute information, such as item category. Furthermore, they often utilize limited sequence context information to predict the next item while overlooking richer inter-item semantic relations. To this end, in this paper, we proposed a novel hierarchical preference modeling framework to substantially model the complex low- and high-level preference dynamics for accurate sequential recommendation. Specifically, in the framework, a novel dual-transformer module and a novel dual contrastive learning scheme have been designed to discriminatively learn users' low- and high-level preference and to effectively enhance both low- and high-level preference learning respectively. In addition, a novel semantics-enhanced context embedding module has been devised to generate more informative context embedding for further improving the recommendation performance. Extensive experiments on six real-world datasets have demonstrated both the superiority of our proposed method over the state-of-the-art ones and the rationality of our design.",
        "published": "2024-10-31 04:00:00",
        "id": "ced3f03e-0e10-47c0-a7c4-de0101b6c9c4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种分层偏好建模框架，包含双变压器模块、双对比学习方案和语义增强上下文嵌入模块，经六个真实数据集实验证明该方法在顺序推荐方面的优越性。"
        },
        "tokens": 842
    },
    {
        "title": "DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection",
        "link": "https://arxiv.org/abs/2410.22803",
        "description": "arXiv:2410.22803v1 Announce Type: new \nAbstract: This paper describes sound event localization and detection (SELD) for spatial audio recordings captured by firstorder ambisonics (FOA) microphones. In this task, one may train a deep neural network (DNN) using FOA data annotated with the classes and directions of arrival (DOAs) of sound events. However, the performance of this approach is severely bounded by the amount of annotated data. To overcome this limitation, we propose a novel method of pretraining the feature extraction part of the DNN in a self-supervised manner. We use spatial audio-visual recordings abundantly available as virtual reality contents. Assuming that sound objects are concurrently observed by the FOA microphones and the omni-directional camera, we jointly train audio and visual encoders with contrastive learning such that the audio and visual embeddings of the same recording and DOA are made close. A key feature of our method is that the DOA-wise audio embeddings are jointly extracted from the raw audio data, while the DOA-wise visual embeddings are separately extracted from the local visual crops centered on the corresponding DOA. This encourages the latent features of the audio encoder to represent both the classes and DOAs of sound events. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows non-annotated audio-visual recordings of 100 hours reduced the error score of SELD from 36.4 pts to 34.9 pts.",
        "published": "2024-10-31 04:00:00",
        "id": "27b0a3ed-bdfc-4bc2-9268-9861179aa9ff",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种自监督预训练神经网络特征提取部分的新方法用于声音事件定位和检测（SELD），使用视听记录联合训练音频和视觉编码器，实验表明利用100小时无注释视听记录可降低SELD的错误分数。"
        },
        "tokens": 919
    },
    {
        "title": "Less is More: DocString Compression in Code Generation",
        "link": "https://arxiv.org/abs/2410.22793",
        "description": "arXiv:2410.22793v1 Announce Type: new \nAbstract: The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings which capture user re quirements for the code and used as the prompt for LLMs, often contains redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our extensive experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25-40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this research is to improve efficiency and reduce the cost while maintaining the quality of the generated code, especially when calling third-party APIs, and is able to reduce the token processing cost by 25-40%.",
        "published": "2024-10-31 04:00:00",
        "id": "636ef4d6-27fc-4841-a0a8-df2e7e9758d7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出一种新的DocString压缩方法ShortenDoc用于代码生成，在六个代码生成数据集、五个开源和一个闭源大语言模型上实验，可实现25 - 40%压缩且保持代码质量，提高效率并降低成本。"
        },
        "tokens": 886
    },
    {
        "title": "Solving Differential Equations with Constrained Learning",
        "link": "https://arxiv.org/abs/2410.22796",
        "description": "arXiv:2410.22796v1 Announce Type: new \nAbstract: (Partial) differential equations (PDEs) are fundamental tools for describing natural phenomena, making their solution crucial in science and engineering. While traditional methods, such as the finite element method, provide reliable solutions, their accuracy is often tied to the use of computationally intensive fine meshes. Moreover, they do not naturally account for measurements or prior solutions, and any change in the problem parameters requires results to be fully recomputed. Neural network-based approaches, such as physics-informed neural networks and neural operators, offer a mesh-free alternative by directly fitting those models to the PDE solution. They can also integrate prior knowledge and tackle entire families of PDEs by simply aggregating additional training losses. Nevertheless, they are highly sensitive to hyperparameters such as collocation points and the weights associated with each loss. This paper addresses these challenges by developing a science-constrained learning (SCL) framework. It demonstrates that finding a (weak) solution of a PDE is equivalent to solving a constrained learning problem with worst-case losses. This explains the limitations of previous methods that minimize the expected value of aggregated losses. SCL also organically integrates structural constraints (e.g., invariances) and (partial) measurements or known solutions. The resulting constrained learning problems can be tackled using a practical algorithm that yields accurate solutions across a variety of PDEs, neural network architectures, and prior knowledge levels without extensive hyperparameter tuning and sometimes even at a lower computational cost.",
        "published": "2024-10-31 04:00:00",
        "id": "23ff5302-3a06-4953-95c3-fc5b4b9203a2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出科学约束学习(SCL)框架解决偏微分方程，该框架将求解偏微分方程等同于解决带最坏情况损失的约束学习问题，能整合结构约束、测量值或已知解，有实用算法可得出准确解且无需大量超参数调整、有时计算成本更低。"
        },
        "tokens": 910
    },
    {
        "title": "Wavelet Burst Accumulation for turbulence mitigation",
        "link": "https://arxiv.org/abs/2410.22802",
        "description": "arXiv:2410.22802v1 Announce Type: new \nAbstract: In this paper, we investigate the extension of the recently proposed weighted Fourier burst accumulation (FBA) method into the wavelet domain. The purpose of FBA is to reconstruct a clean and sharp image from a sequence of blurred frames. This concept lies in the construction of weights to amplify dominant frequencies in the Fourier spectrum of each frame. The reconstructed image is then obtained by taking the inverse Fourier transform of the average of all processed spectra. In this paper, we first suggest to replace the rigid registration step used in the original algorithm by a non-rigid registration in order to be able to process sequences acquired through atmospheric turbulence. Second, we propose to work in a wavelet domain instead of the Fourier one. This leads us to the construction of two types of algorithms. Finally, we propose an alternative approach to replace the weighting idea by an approach promoting the sparsity in the used space. Several experiments are provided to illustrate the efficiency of the proposed methods.",
        "published": "2024-10-31 04:00:00",
        "id": "0d76ab74-5a48-4622-b801-177783062897",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文研究将加权傅里叶突发累积方法扩展到小波域以重建清晰图像，提出用非刚性配准代替原算法中的刚性配准，构建两种算法，还提出用促进稀疏性的方法替代加权思想并进行实验。"
        },
        "tokens": 803
    },
    {
        "title": "Open Turbulent Image Set (OTIS)",
        "link": "https://arxiv.org/abs/2410.22791",
        "description": "arXiv:2410.22791v1 Announce Type: new \nAbstract: Long distance imaging is subject to the impact of the turbulent atmosphere. This results into geometric distortions and some blur effect in the observed frames. Despite the existence of several turbulence mitigation algorithms in the literature, no common dataset exists to objectively evaluate their efficiency. In this paper, we describe a new dataset called OTIS (Open Turbulent Images Set) which contains several sequences (either static or dynamic) acquired through the turbulent atmosphere. For almost all sequences, we provide the corresponding groundtruth in order to make the comparison between algorithms easier. We also discuss possible metrics to perform such comparisons.",
        "published": "2024-10-31 04:00:00",
        "id": "0004d792-6faa-4143-be3e-c6aaa94342d7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文介绍了一个名为OTIS的新数据集，包含经湍流大气获取的序列并大多提供对应真值，旨在客观评估湍流缓解算法效率并讨论相关比较指标。"
        },
        "tokens": 709
    },
    {
        "title": "Run-Time Adaptation of Neural Beamforming for Robust Speech Dereverberation and Denoising",
        "link": "https://arxiv.org/abs/2410.22805",
        "description": "arXiv:2410.22805v1 Announce Type: new \nAbstract: This paper describes speech enhancement for realtime automatic speech recognition (ASR) in real environments. A standard approach to this task is to use neural beamforming that can work efficiently in an online manner. It estimates the masks of clean dry speech from a noisy echoic mixture spectrogram with a deep neural network (DNN) and then computes a enhancement filter used for beamforming. The performance of such a supervised approach, however, is drastically degraded under mismatched conditions. This calls for run-time adaptation of the DNN. Although the ground-truth speech spectrogram required for adaptation is not available at run time, blind dereverberation and separation methods such as weighted prediction error (WPE) and fast multichannel nonnegative matrix factorization (FastMNMF) can be used for generating pseudo groundtruth data from a mixture. Based on this idea, a prior work proposed a dual-process system based on a cascade of WPE and minimum variance distortionless response (MVDR) beamforming asynchronously fine-tuned by block-online FastMNMF. To integrate the dereverberation capability into neural beamforming and make it fine-tunable at run time, we propose to use weighted power minimization distortionless response (WPD) beamforming, a unified version of WPE and minimum power distortionless response (MPDR), whose joint dereverberation and denoising filter is estimated using a DNN. We evaluated the impact of run-time adaptation under various conditions with different numbers of speakers, reverberation times, and signal-to-noise ratios (SNRs).",
        "published": "2024-10-31 04:00:00",
        "id": "0b770392-cfb9-4673-b0d7-dd22e20a4be1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出将加权功率最小化无失真响应（WPD）波束形成用于实时自适应神经网络波束形成以实现语音增强，在不同说话人数、混响时间和信噪比条件下评估了其运行时自适应的影响。"
        },
        "tokens": 933
    },
    {
        "title": "MILP-StuDio: MILP Instance Generation via Block Structure Decomposition",
        "link": "https://arxiv.org/abs/2410.22806",
        "description": "arXiv:2410.22806v1 Announce Type: new \nAbstract: Mixed-integer linear programming (MILP) is one of the most popular mathematical formulations with numerous applications. In practice, improving the performance of MILP solvers often requires a large amount of high-quality data, which can be challenging to collect. Researchers thus turn to generation techniques to generate additional MILP instances. However, existing approaches do not take into account specific block structures -- which are closely related to the problem formulations -- in the constraint coefficient matrices (CCMs) of MILPs. Consequently, they are prone to generate computationally trivial or infeasible instances due to the disruptions of block structures and thus problem formulations. To address this challenge, we propose a novel MILP generation framework, called Block Structure Decomposition (MILP-StuDio), to generate high-quality instances by preserving the block structures. Specifically, MILP-StuDio begins by identifying the blocks in CCMs and decomposing the instances into block units, which serve as the building blocks of MILP instances. We then design three operators to construct new instances by removing, substituting, and appending block units in the original instances, enabling us to generate instances with flexible sizes. An appealing feature of MILP-StuDio is its strong ability to preserve the feasibility and computational hardness of the generated instances. Experiments on the commonly-used benchmarks demonstrate that using instances generated by MILP-StuDio is able to significantly reduce over 10% of the solving time for learning-based solvers.",
        "published": "2024-10-31 04:00:00",
        "id": "625d236b-c884-4987-9dc4-7c42b1e662e1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为MILP - StuDio的MILP生成框架，通过分解约束系数矩阵中的块结构生成高质量实例，实验表明其能显著减少求解时间。"
        },
        "tokens": 905
    },
    {
        "title": "Causality-Enhanced Behavior Sequence Modeling in LLMs for Personalized Recommendation",
        "link": "https://arxiv.org/abs/2410.22809",
        "description": "arXiv:2410.22809v1 Announce Type: new \nAbstract: Recent advancements in recommender systems have focused on leveraging Large Language Models (LLMs) to improve user preference modeling, yielding promising outcomes. However, current LLM-based approaches struggle to fully leverage user behavior sequences, resulting in suboptimal preference modeling for personalized recommendations. In this study, we propose a novel Counterfactual Fine-Tuning (CFT) method to address this issue by explicitly emphasizing the role of behavior sequences when generating recommendations. Specifically, we employ counterfactual reasoning to identify the causal effects of behavior sequences on model output and introduce a task that directly fits the ground-truth labels based on these effects, achieving the goal of explicit emphasis. Additionally, we develop a token-level weighting mechanism to adjust the emphasis strength for different item tokens, reflecting the diminishing influence of behavior sequences from earlier to later tokens during predicting an item. Extensive experiments on real-world datasets demonstrate that CFT effectively improves behavior sequence modeling. Our codes are available at https://github.com/itsmeyjt/CFT.",
        "published": "2024-10-31 04:00:00",
        "id": "30b34c05-46ba-45f4-b20d-60fe614ac80e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为改善个性化推荐中基于大语言模型的偏好建模不佳问题，提出反事实微调方法，还开发了令牌级加权机制，实验证明其可有效改进行为序列建模并公开了代码。"
        },
        "tokens": 809
    },
    {
        "title": "Adaptive Multi Scale Document Binarisation Using Vision Mamba",
        "link": "https://arxiv.org/abs/2410.22811",
        "description": "arXiv:2410.22811v1 Announce Type: new \nAbstract: Enhancing and preserving the readability of document images, particularly historical ones, is crucial for effective document image analysis. Numerous models have been proposed for this task, including convolutional-based, transformer-based, and hybrid convolutional-transformer architectures. While hybrid models address the limitations of purely convolutional or transformer-based methods, they often suffer from issues like quadratic time complexity. In this work, we propose a Mamba-based architecture for document binarisation, which efficiently handles long sequences by scaling linearly and optimizing memory usage. Additionally, we introduce novel modifications to the skip connections by incorporating Difference of Gaussians (DoG) features, inspired by conventional signal processing techniques. These multiscale high-frequency features enable the model to produce high-quality, detailed outputs.",
        "published": "2024-10-31 04:00:00",
        "id": "892e1b56-e5e9-4dc8-882b-32574fffe8e4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于Mamba的文档二值化架构，通过线性缩放和优化内存使用来有效处理长序列，且对跳跃连接进行新的修改以产生高质量、详细的输出。"
        },
        "tokens": 751
    },
    {
        "title": "Universality of the $\\pi^2/6$ Pathway in Avoiding Model Collapse",
        "link": "https://arxiv.org/abs/2410.22812",
        "description": "arXiv:2410.22812v1 Announce Type: new \nAbstract: Researchers in empirical machine learning recently spotlighted their fears of so-called Model Collapse. They imagined a discard workflow, where an initial generative model is trained with real data, after which the real data are discarded, and subsequently, the model generates synthetic data on which a new model is trained. They came to the conclusion that models degenerate as model-fitting generations proceed. However, other researchers considered an augment workflow, where the original real data continue to be used in each generation of training, augmented by synthetic data from models fit in all earlier generations. Empirical results on canonical datasets and learning procedures confirmed the occurrence of model collapse under the discard workflow and avoidance of model collapse under the augment workflow. Under the augment workflow, theoretical evidence also confirmed avoidance in particular instances; specifically, Gerstgrasser et al. (2024) found that for classical Linear Regression, test risk at any later generation is bounded by a moderate multiple, viz. pi-squared-over-6 of the test risk of training with the original real data alone. Some commentators questioned the generality of theoretical conclusions based on the generative model assumed in Gerstgrasser et al. (2024): could similar conclusions be reached for other task/model pairings? In this work, we demonstrate the universality of the pi-squared-over-6 augment risk bound across a large family of canonical statistical models, offering key insights into exactly why collapse happens under the discard workflow and is avoided under the augment workflow. In the process, we provide a framework that is able to accommodate a large variety of workflows (beyond discard and augment), thereby enabling an experimenter to judge the comparative merits of multiple different workflows by simulating a simple Gaussian process.",
        "published": "2024-10-31 04:00:00",
        "id": "e71352e6-972f-43d5-ac34-baffebd7ac63",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究者探讨模型崩溃问题，实证结果显示在丢弃工作流下会发生模型崩溃，在增强工作流下可避免，本文证明了pi - squared - over - 6增强风险界限在多种经典统计模型中的普遍性，还提供了能容纳多种工作流的框架"
        },
        "tokens": 971
    },
    {
        "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients",
        "link": "https://arxiv.org/abs/2410.22815",
        "description": "arXiv:2410.22815v1 Announce Type: new \nAbstract: Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A2 maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.",
        "published": "2024-10-31 04:00:00",
        "id": "34a831e9-8ee8-44bd-8206-5d694dfaccf6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出LoRA - A2解决联邦学习中低秩适应的聚合不一致问题，在低秩和高数据异质性条件下表现出鲁棒性，可大幅减少上传参数且不影响性能，有助于LLMs在资源受限环境的部署。"
        },
        "tokens": 807
    },
    {
        "title": "Enhancing Tool Manipulation of An Aerial Vehicle with A Dynamically Displacing Center-of-Mass",
        "link": "https://arxiv.org/abs/2410.22816",
        "description": "arXiv:2410.22816v1 Announce Type: new \nAbstract: As aerial robots gain traction in industrial applications, there is growing interest in enhancing their physical interaction capabilities. Pushing tasks performed by aerial manipulators have been successfully demonstrated in contact-based inspections. However, more complex industrial applications require these systems to support higher-DoF (Degree of Freedom) manipulators and generate larger forces while pushing (e.g., drilling, grinding). This paper builds on our previous work, where we introduced an aerial vehicle with a dynamically displacing CoM (Center of Mass) to improve force exertion during interactions. We propose a novel approach to further enhance this system's force generation by optimizing its CoM location during interactions. Additionally, we study the case of this aerial vehicle equipped with a 2-DoF manipulation arm to extend the system's functionality in tool-based tasks. The effectiveness of the proposed methods is validated through simulations, demonstrating the potential of this system for advanced aerial manipulation in practical settings.",
        "published": "2024-10-31 04:00:00",
        "id": "83bf0b4c-cb7d-446c-b1c0-be16619ad1a3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文基于之前工作提出新方法，通过优化空中机器人在交互中的质心位置来增强其力的产生，并研究配备2自由度操作臂的情况以扩展其在工具任务中的功能，经模拟验证了有效性。"
        },
        "tokens": 802
    },
    {
        "title": "Epipolar-Free 3D Gaussian Splatting for Generalizable Novel View Synthesis",
        "link": "https://arxiv.org/abs/2410.22817",
        "description": "arXiv:2410.22817v1 Announce Type: new \nAbstract: Generalizable 3D Gaussian splitting (3DGS) can reconstruct new scenes from sparse-view observations in a feed-forward inference manner, eliminating the need for scene-specific retraining required in conventional 3DGS. However, existing methods rely heavily on epipolar priors, which can be unreliable in complex realworld scenes, particularly in non-overlapping and occluded regions. In this paper, we propose eFreeSplat, an efficient feed-forward 3DGS-based model for generalizable novel view synthesis that operates independently of epipolar line constraints. To enhance multiview feature extraction with 3D perception, we employ a selfsupervised Vision Transformer (ViT) with cross-view completion pre-training on large-scale datasets. Additionally, we introduce an Iterative Cross-view Gaussians Alignment method to ensure consistent depth scales across different views. Our eFreeSplat represents an innovative approach for generalizable novel view synthesis. Different from the existing pure geometry-free methods, eFreeSplat focuses more on achieving epipolar-free feature matching and encoding by providing 3D priors through cross-view pretraining. We evaluate eFreeSplat on wide-baseline novel view synthesis tasks using the RealEstate10K and ACID datasets. Extensive experiments demonstrate that eFreeSplat surpasses state-of-the-art baselines that rely on epipolar priors, achieving superior geometry reconstruction and novel view synthesis quality. Project page: https://tatakai1.github.io/efreesplat/.",
        "published": "2024-10-31 04:00:00",
        "id": "762a343e-a874-4fd5-89a0-61f9abd6b1cf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出eFreeSplat，一种基于3DGS且独立于极线约束的通用新视图合成前馈模型，采用自监督Vision Transformer并引入迭代跨视图高斯对齐方法，在宽基线新视图合成任务上超越依赖极先验的基线。"
        },
        "tokens": 926
    },
    {
        "title": "A test-free semantic mistakes localization framework in Neural Code Translation",
        "link": "https://arxiv.org/abs/2410.22818",
        "description": "arXiv:2410.22818v1 Announce Type: new \nAbstract: In the task of code translation, neural network-based models have been shown to frequently produce semantically erroneous code that deviates from the original logic of the source code. This issue persists even with advanced large models. Although a recent approach proposed using test cases to identify these semantic errors, it relies heavily on the quality of the test cases and is not applicable to code snippets without test cases in real-world scenarios. Therefore, We present EISP, a static analysis framework based on the Large Language Model (LLM).First, the framework generates a semantic mapping between source code and translated code. Next, each sub-code fragment is identified by recursively traversing the abstract syntax tree of the source code, and its corresponding translated code fragment is found through the semantic mapping. Finally, EISP connects each pair of sub-code fragments with fine-grained knowledge hints through an AI chain to assist LLMs in discovering semantic mistakes in the translated code. In our benchmark evaluation, the EISP framework, based on GPT-4o mini, achieved an accuracy of 82.3\\%, representing a 20.3\\% improvement over baseline methods using the same base model, and a 7.4\\% improvement compared to dynamic analysis methods that require test cases and manual intervention. To our knowledge, EISP is the first tool to locate semantic errors in translated code without test cases or compilable code. This innovative tool provides the software engineering community with a new way to deal with code fragments without test cases.",
        "published": "2024-10-31 04:00:00",
        "id": "8a19f099-ded6-4645-86a8-d98ca068ccdd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了EISP这一基于大型语言模型的静态分析框架，用于在无测试用例的神经代码翻译中定位语义错误，EISP在基准评估中表现优于基线方法和动态分析方法。"
        },
        "tokens": 906
    },
    {
        "title": "An invariance principle based concentration result for large-scale stochastic pairwise interaction network systems",
        "link": "https://arxiv.org/abs/2410.22820",
        "description": "arXiv:2410.22820v1 Announce Type: new \nAbstract: We study stochastic pairwise interaction network systems whereby a finite population of agents, identified with the nodes of a graph, update their states in response to both individual mutations and pairwise interactions with their neighbors. The considered class of systems include the main epidemic models -such as the SIS, SIR, and SIRS models-, certain social dynamics models -such as the voter and anti-voter models-, as well as evolutionary dynamics on graphs. Since these stochastic systems fall into the class of finite-state Markov chains, they always admit stationary distributions. We analyze the asymptotic behavior of these stationary distributions in the limit as the population size grows large while the interaction network maintains certain mixing properties. Our approach relies on the use of Lyapunov-type functions to obtain concentration results on these stationary distributions. Notably, our results are not limited to fully mixed population models, as they do apply to a much broader spectrum of interaction network structures, including, e.g., Erd\\\"oos-R\\'enyi random graphs.",
        "published": "2024-10-31 04:00:00",
        "id": "9c145db9-2d25-4883-b957-1783f6aeda32",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究人员分析了随机成对交互网络系统的渐近行为，当种群规模增大且交互网络保持特定混合特性时，使用李雅普诺夫型函数得到平稳分布的集中结果，其结果适用于多种交互网络结构。"
        },
        "tokens": 811
    },
    {
        "title": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations",
        "link": "https://arxiv.org/abs/2410.22821",
        "description": "arXiv:2410.22821v1 Announce Type: new \nAbstract: How to evaluate Large Language Models (LLMs) in code generation remains an open question. Existing benchmarks have two limitations - data leakage and lack of domain-specific evaluation. The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains. To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: (1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories. (2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. (3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. EvoCodeBench has been released.",
        "published": "2024-10-31 04:00:00",
        "id": "ba6eeb63-e7f2-4e25-9d7e-715697806c8e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为EvoCodeBench的新基准，具有动态更新数据、编程领域分类标签和特定领域评估功能，用于评估8种流行的大型语言模型在代码生成方面的能力并得出一些见解。"
        },
        "tokens": 1013
    },
    {
        "title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models",
        "link": "https://arxiv.org/abs/2410.22832",
        "description": "arXiv:2410.22832v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also introduces potential security risks. In this work, we reveal a novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which enables attackers to manipulate the retrieval mechanisms of RAG systems by injecting malicious texts into the knowledge database. When the RAG system encounters target questions, it generates the attacker's pre-determined answers instead of the correct ones, undermining the integrity and trustworthiness of the system. We formalize HijackRAG as an optimization problem and propose both black-box and white-box attack strategies tailored to different levels of the attacker's knowledge. Extensive experiments on multiple benchmark datasets show that HijackRAG consistently achieves high attack success rates, outperforming existing baseline attacks. Furthermore, we demonstrate that the attack is transferable across different retriever models, underscoring the widespread risk it poses to RAG systems. Lastly, our exploration of various defense mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing the urgent need for more robust security measures to protect RAG systems in real-world deployments.",
        "published": "2024-10-31 04:00:00",
        "id": "79213eea-2edc-4547-ad9f-a50912776603",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究揭示针对检索增强型大型语言模型（RAG）的新漏洞HijackRAG，攻击者可通过向知识库注入恶意文本操纵其检索机制，相关攻击策略效果良好且可跨模型转移，现有防御机制难以应对。"
        },
        "tokens": 871
    },
    {
        "title": "Grasping Force Estimation for Markerless Visuotactile Sensors",
        "link": "https://arxiv.org/abs/2410.22825",
        "description": "arXiv:2410.22825v1 Announce Type: new \nAbstract: Tactile sensors have been used for force estimation in the past, especially Vision-Based Tactile Sensors (VBTS) have recently become a new trend due to their high spatial resolution and low cost. In this work, we have designed and implemented several approaches to estimate the normal grasping force using different types of markerless visuotactile representations obtained from VBTS. Our main goal is to determine the most appropriate visuotactile representation, based on a performance analysis during robotic grasping tasks. Our proposal has been tested on the dataset generated with our DIGIT sensors and another one obtained using GelSight Mini sensors from another state-of-the-art work. We have also tested the generalization capabilities of our best approach, called RGBmod. The results led to two main conclusions. First, the RGB visuotactile representation is a better input option than the depth image or a combination of the two for estimating normal grasping forces. Second, RGBmod achieved a good performance when tested on 10 unseen everyday objects in real-world scenarios, achieving an average relative error of 0.125 +- 0.153. Furthermore, we show that our proposal outperforms other works in the literature that use RGB and depth information for the same task.",
        "published": "2024-10-31 04:00:00",
        "id": "b3c3504e-3099-4405-9c21-505af17775fa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究人员设计并实现多种方法，用从视觉型触觉传感器（VBTS）获取的不同无标记视觉触觉表征来估计正常抓取力，测试得出RGB视觉触觉表征更适合，且RGBmod性能良好并优于其他同类研究。"
        },
        "tokens": 872
    },
    {
        "title": "How Well Do Large Language Models Disambiguate Swedish Words?",
        "link": "https://arxiv.org/abs/2410.22827",
        "description": "arXiv:2410.22827v1 Announce Type: new \nAbstract: We evaluate a battery of recent large language models on two benchmarks for word sense disambiguation in Swedish. At present, all current models are less accurate than the best supervised disambiguators in cases where a training set is available, but most models outperform graph-based unsupervised systems. Different prompting approaches are compared, with a focus on how to express the set of possible senses in a given context. The best accuracies are achieved when human-written definitions of the senses are included in the prompts.",
        "published": "2024-10-31 04:00:00",
        "id": "99f18776-85fa-4e1e-a877-8a12d8ae6117",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究评估一系列大型语言模型在瑞典语词义消歧两个基准上的表现，比较不同提示方法，发现包含人工编写的词义定义时准确率最高，当前模型在有训练集可用的情况下准确性不如最佳监督消歧器，但大多优于无监督系统。"
        },
        "tokens": 719
    },
    {
        "title": "Situational Scene Graph for Structured Human-centric Situation Understanding",
        "link": "https://arxiv.org/abs/2410.22829",
        "description": "arXiv:2410.22829v1 Announce Type: new \nAbstract: Graph based representation has been widely used in modelling spatio-temporal relationships in video understanding. Although effective, existing graph-based approaches focus on capturing the human-object relationships while ignoring fine-grained semantic properties of the action components. These semantic properties are crucial for understanding the current situation, such as where does the action takes place, what tools are used and functional properties of the objects. In this work, we propose a graph-based representation called Situational Scene Graph (SSG) to encode both human-object relationships and the corresponding semantic properties. The semantic details are represented as predefined roles and values inspired by situation frame, which is originally designed to represent a single action. Based on our proposed representation, we introduce the task of situational scene graph generation and propose a multi-stage pipeline Interactive and Complementary Network (InComNet) to address the task. Given that the existing datasets are not applicable to the task, we further introduce a SSG dataset whose annotations consist of semantic role-value frames for human, objects and verb predicates of human-object relations. Finally, we demonstrate the effectiveness of our proposed SSG representation by testing on different downstream tasks. Experimental results show that the unified representation can not only benefit predicate classification and semantic role-value classification, but also benefit reasoning tasks on human-centric situation understanding. We will release the code and the dataset soon.",
        "published": "2024-10-31 04:00:00",
        "id": "39dd3631-a86d-4880-a316-dea6d88e1ea5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于图的情境场景图（SSG）表示，用于编码人与对象关系和语义属性，介绍SSG生成任务与多阶段管道InComNet，创建SSG数据集，实验表明该表示对下游任务有效。"
        },
        "tokens": 874
    },
    {
        "title": "The Reconstruction of the Space-Dependent Thermal Conductivity from Sparse Temperature Measurements",
        "link": "https://arxiv.org/abs/2410.22822",
        "description": "arXiv:2410.22822v1 Announce Type: new \nAbstract: We present a novel method for reconstructing the thermal conductivity coefficient in 1D and 2D heat equations using moving sensors that dynamically traverse the domain to record sparse and noisy temperature measurements. We significantly reduce the computational cost associated with forward PDE evaluations by employing automatic differentiation, enabling a more efficient and scalable reconstruction process. This allows the inverse problem to be solved with fewer sensors and observations. Specifically, we demonstrate the successful reconstruction of thermal conductivity on the 1D circle and 2D torus, using one and four moving sensors, respectively, with their positions recorded over time. Our method incorporates sampling algorithms to compute confidence intervals for the reconstructed conductivity, improving robustness against measurement noise. Extensive numerical simulations of heat dynamics validate the efficacy of our approach, confirming both the accuracy and stability of the reconstructed thermal conductivity. Additionally, the method is thoroughly tested using large datasets from machine learning, allowing us to evaluate its performance across various scenarios and ensure its reliability. This approach provides a cost-effective and flexible solution for conductivity reconstruction from sparse measurements, making it a robust tool for solving inverse problems in complex domains.",
        "published": "2024-10-31 04:00:00",
        "id": "f9079d2d-20a4-4137-bf39-0ad6369e0df1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出一种利用移动传感器记录稀疏和噪声温度测量值来重建1D和2D热方程中热传导系数的新方法，采用自动微分降低计算成本，可在传感器和观测较少情况下求解反问题，还可计算置信区间提高鲁棒性，经数值模拟和大数据集测试验证了该方法的有效性。"
        },
        "tokens": 855
    },
    {
        "title": "SFDFusion: An Efficient Spatial-Frequency Domain Fusion Network for Infrared and Visible Image Fusion",
        "link": "https://arxiv.org/abs/2410.22837",
        "description": "arXiv:2410.22837v1 Announce Type: new \nAbstract: Infrared and visible image fusion aims to utilize the complementary information from two modalities to generate fused images with prominent targets and rich texture details. Most existing algorithms only perform pixel-level or feature-level fusion from different modalities in the spatial domain. They usually overlook the information in the frequency domain, and some of them suffer from inefficiency due to excessively complex structures. To tackle these challenges, this paper proposes an efficient Spatial-Frequency Domain Fusion (SFDFusion) network for infrared and visible image fusion. First, we propose a Dual-Modality Refinement Module (DMRM) to extract complementary information. This module extracts useful information from both the infrared and visible modalities in the spatial domain and enhances fine-grained spatial details. Next, to introduce frequency domain information, we construct a Frequency Domain Fusion Module (FDFM) that transforms the spatial domain to the frequency domain through Fast Fourier Transform (FFT) and then integrates frequency domain information. Additionally, we design a frequency domain fusion loss to provide guidance for the fusion process. Extensive experiments on public datasets demonstrate that our method produces fused images with significant advantages in various fusion metrics and visual effects. Furthermore, our method demonstrates high efficiency in image fusion and good performance on downstream detection tasks, thereby satisfying the real-time demands of advanced visual tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "be0f88be-a086-4012-8ba4-25d8617a01b4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种用于红外和可见光图像融合的高效空间 - 频域融合（SFDFusion）网络，包括双模态细化模块（DMRM）和频域融合模块（FDFM），实验表明该方法在融合指标、视觉效果、效率和下游检测任务方面表现良好。"
        },
        "tokens": 891
    },
    {
        "title": "Danoliteracy of Generative, Large Language Models",
        "link": "https://arxiv.org/abs/2410.22839",
        "description": "arXiv:2410.22839v1 Announce Type: new \nAbstract: The language technology moonshot moment of Generative, Large Language Models (GLLMs) was not limited to English: These models brought a surge of technological applications, investments and hype to low-resource languages as well. However, the capabilities of these models in languages such as Danish were until recently difficult to verify beyond qualitative demonstrations due to a lack of applicable evaluation corpora. We present a GLLM benchmark to evaluate Danoliteracy, a measure of Danish language and cultural competency, across eight diverse scenarios such Danish citizenship tests and abstractive social media question answering. This limited-size benchmark is found to produce a robust ranking that correlates to human feedback at $\\rho \\sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings. Analyzing these model results across scenarios, we find one strong underlying factor explaining $95\\%$ of scenario performance variance for GLLMs in Danish, suggesting a $g$ factor of model consistency in language adaption.",
        "published": "2024-10-31 04:00:00",
        "id": "3b2fd71c-8b1e-4eb7-a7c1-73e2638ee474",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一个GLLM基准来评估Danoliteracy（丹麦语和文化能力的衡量标准），该基准在八个不同场景中产生的排名与人类反馈相关，还分析了模型结果，发现一个解释丹麦语中GLLMs场景性能差异的强因素。"
        },
        "tokens": 822
    },
    {
        "title": "Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation",
        "link": "https://arxiv.org/abs/2410.22844",
        "description": "arXiv:2410.22844v1 Announce Type: new \nAbstract: Adversarial Collaborative Filtering (ACF), which typically applies adversarial perturbations at user and item embeddings through adversarial training, is widely recognized as an effective strategy for enhancing the robustness of Collaborative Filtering (CF) recommender systems against poisoning attacks. Besides, numerous studies have empirically shown that ACF can also improve recommendation performance compared to traditional CF. Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear. To bridge this gap, in this paper, we first theoretically show that ACF can achieve a lower recommendation error compared to traditional CF with the same training epochs in both clean and poisoned data contexts. Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness. Building on these theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.",
        "published": "2024-10-31 04:00:00",
        "id": "a8821ea1-5213-4e5a-b008-f8030e9154d1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文从理论上表明对抗性协同过滤（ACF）在推荐误差方面优于传统协同过滤（CF），基于理论提出个性化幅度对抗协同过滤（PamaCF），实验证明其能抵御中毒攻击并提升推荐性能。"
        },
        "tokens": 837
    },
    {
        "title": "Two-Way One-Counter Nets Revisited",
        "link": "https://arxiv.org/abs/2410.22845",
        "description": "arXiv:2410.22845v1 Announce Type: new \nAbstract: One Counter Nets (OCNs) are finite-state automata equipped with a counter that cannot become negative, but cannot be explicitly tested for zero. Their close connection to various other models (e.g., PDAs, Vector Addition Systems, and Counter Automata) make them an attractive modeling tool. The two-way variant of OCNs (2-OCNs) was introduced in the 1980's and shown to be more expressive than OCNs, so much so that the emptiness problem is undecidable already in the deterministic model (2-DOCNs). In a first part, we study the emptiness problem of natural restrictions of 2-OCNs, under the light of modern results about Vector Addition System with States (VASS). We show that emptiness is decidable for 2-OCNs over \\emph{bounded languages} i.e., languages contained in $a_1^*a_2^*\\cdots a_k^*$), and decidable and Ackermann-complete for \\emph{sweeping} 2-OCNs, where the head direction only changes at the end-markers. Both decidability results revolve around reducing the problem to VASS reachability, but they rely on strikingly different approaches.\n  In a second part, we study the expressive power of 2-OCNs, showing an array of connections between bounded languages, sweeping 2-OCNs, and semilinear languages. Most noteworthy among these connections, is that the bounded languages recognized by sweeping 2-OCNs are precisely those that are semilinear. Finally, we establish an intricate pumping lemma for 2-DOCNs and use it to show that there are OCN languages that are not 2-DOCN recognizable, improving on the known result that there are such 2-OCN languages.",
        "published": "2024-10-31 04:00:00",
        "id": "42673f8b-7ad3-4193-933f-205ce571060e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究Two - Way One - Counter Nets（2 - OCNs）的空集问题和表达能力，包括在有界语言、扫描2 - OCNs中的空集判定结果，以及2 - OCNs与半线性语言的关系、2 - DOCNs的泵引理等内容。"
        },
        "tokens": 996
    },
    {
        "title": "Knowledge Graph Based Visual Search Application",
        "link": "https://arxiv.org/abs/2410.22846",
        "description": "arXiv:2410.22846v1 Announce Type: new \nAbstract: The FAIR data principles advocate for making scientific and research datasets 'Findable' and 'Accessible'. Yet, the sheer volume and diversity of these datasets present significant challenges. Despite advancements in data search technologies, techniques for representing search results are still traditional and inadequate, often returning extraneous results. To address these issues, we developed a knowledge graph based visual search application designed to enhance data search for Earth System Scientists. This application utilizes various chart widgets and a knowledge graph at the backend, connecting two disparate data repositories.",
        "published": "2024-10-31 04:00:00",
        "id": "26b7f699-3d24-48bc-b338-f98ff7387c4a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决数据搜索结果表示传统且不充分等问题，开发了基于知识图谱的视觉搜索应用以提升地球系统科学家的数据搜索能力，应用使用多种图表组件并连接两个不同数据仓库。"
        },
        "tokens": 701
    },
    {
        "title": "Centimeter-level Geometry Reconstruction and Material Identification in 300 GHz Monostatic Sensing",
        "link": "https://arxiv.org/abs/2410.22852",
        "description": "arXiv:2410.22852v1 Announce Type: new \nAbstract: Terahertz (THz) integrated sensing and communication (ISAC) technology is envisioned to achieve high communication performance alongside advanced sensing abilities. For various applications of ISAC, accurate environment reconstruction including geometry reconstruction and material identification is critical. This paper presents a highly precise geometry reconstruction algorithm and material identification scheme for a monostatic sensing case in a typical indoor scenario. Experiments are conducted in the frequency range from 290 GHz to 310 GHz using a vector network analyzer (VNA)-based channel sounder by co-locating the transmitter and receiver. A joint delay and angle space-alternating generalized expectation-maximization (SAGE)-based algorithm is implemented to estimate multipath component (MPC) parameters and the indoor geometry is reconstructed based on the extracted parameters. Furthermore, a geometry-based method is employed to model and remove the spurious path of the corner, reaching an accuracy of 1.75 cm. Additionally, a material database using THz time-domain spectroscopy (THz-TDS) is established, capturing reflection losses of over 200 common material samples. Applying this database to our monostatic sensing, the measured reflection losses of wall and window frame are accurately identified as cement and steel, respectively. Our results demonstrate the centimeter-level geometry reconstruction and accurate material identification for practical THz ISAC scenarios, which unleash unprecedented sensing potential compared to microwave and millimeter-wave bands.",
        "published": "2024-10-31 04:00:00",
        "id": "c9854b54-30c2-4595-85a9-38315091169a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种用于典型室内单站感知场景下的高精度几何重建算法和材料识别方案，通过实验实现厘米级几何重建和准确的材料识别。"
        },
        "tokens": 893
    },
    {
        "title": "Data subsampling for Poisson regression with pth-root-link",
        "link": "https://arxiv.org/abs/2410.22872",
        "description": "arXiv:2410.22872v1 Announce Type: new \nAbstract: We develop and analyze data subsampling techniques for Poisson regression, the standard model for count data $y\\in\\mathbb{N}$. In particular, we consider the Poisson generalized linear model with ID- and square root-link functions. We consider the method of coresets, which are small weighted subsets that approximate the loss function of Poisson regression up to a factor of $1\\pm\\varepsilon$. We show $\\Omega(n)$ lower bounds against coresets for Poisson regression that continue to hold against arbitrary data reduction techniques up to logarithmic factors. By introducing a novel complexity parameter and a domain shifting approach, we show that sublinear coresets with $1\\pm\\varepsilon$ approximation guarantee exist when the complexity parameter is small. In particular, the dependence on the number of input points can be reduced to polylogarithmic. We show that the dependence on other input parameters can also be bounded sublinearly, though not always logarithmically. In particular, we show that the square root-link admits an $O(\\log(y_{\\max}))$ dependence, where $y_{\\max}$ denotes the largest count presented in the data, while the ID-link requires a $\\Theta(\\sqrt{y_{\\max}/\\log(y_{\\max})})$ dependence. As an auxiliary result for proving the tightness of the bound with respect to $y_{\\max}$ in the case of the ID-link, we show an improved bound on the principal branch of the Lambert $W_0$ function, which may be of independent interest. We further show the limitations of our analysis when $p$th degree root-link functions for $p\\geq 3$ are considered, which indicate that other analytical or computational methods would be required if such a generalization is even possible.",
        "published": "2024-10-31 04:00:00",
        "id": "335f96af-f137-4938-832d-976a1db25260",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文针对Poisson回归开发和分析数据子采样技术，研究了Poisson广义线性模型的ID - 和平方根 - 链接函数，探讨了核心集方法，给出了下限，通过新复杂度参数和域转移方法证明特定条件下亚线性核心集存在，还展示了分析在p次根链接函数（p≥3）时的局限性。"
        },
        "tokens": 999
    },
    {
        "title": "AtGCN: A Graph Convolutional Network For Ataxic Gait Detection",
        "link": "https://arxiv.org/abs/2410.22862",
        "description": "arXiv:2410.22862v1 Announce Type: new \nAbstract: Video-based gait analysis can be defined as the task of diagnosing pathologies, such as ataxia, using videos of patients walking in front of a camera. This paper presents a graph convolution network called AtGCN for detecting ataxic gait and identifying its severity using 2D videos. The problem is especially challenging as the deviation of an ataxic gait from a healthy gait is very subtle. The datasets for ataxic gait detection are also quite small, with the largest dataset having only 149 videos. The paper addresses the first problem using special spatiotemporal graph convolution that successfully captures important gait-related features. To handle the small dataset size, a deep spatiotemporal graph convolution network pre-trained on an action recognition dataset is systematically truncated and then fine-tuned on the ataxia dataset to obtain the AtGCN model. The paper also presents an augmentation strategy that segments a video sequence into multiple gait cycles. The proposed AtGCN model then operates on a graph of body part locations belonging to a single gait cycle. The evaluation results support the strength of the proposed AtGCN model, as it outperforms the state-of-the-art in detection and severity prediction with an accuracy of 93.46% and a MAE of 0.4169, respectively.",
        "published": "2024-10-31 04:00:00",
        "id": "7f93eeef-7784-4843-b834-e4258d055e52",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出用于检测共济失调步态及其严重程度的图卷积网络AtGCN，通过特殊时空图卷积捕捉特征、预训练后微调解决数据集小的问题，还提出分割视频序列的增强策略，模型检测和严重程度预测表现优于现有技术。"
        },
        "tokens": 887
    },
    {
        "title": "Adaptive Paradigm Synergy: Can a Cross-Paradigm Objective Enhance Long-Tailed Learning?",
        "link": "https://arxiv.org/abs/2410.22883",
        "description": "arXiv:2410.22883v1 Announce Type: new \nAbstract: Self-supervised learning (SSL) has achieved impressive results across several computer vision tasks, even rivaling supervised methods. However, its performance degrades on real-world datasets with long-tailed distributions due to difficulties in capturing inherent class imbalances. Although supervised long-tailed learning offers significant insights, the absence of labels in SSL prevents direct transfer of these strategies.To bridge this gap, we introduce Adaptive Paradigm Synergy (APS), a cross-paradigm objective that seeks to unify the strengths of both paradigms. Our approach reexamines contrastive learning from a spatial structure perspective, dynamically adjusting the uniformity of latent space structure through adaptive temperature tuning. Furthermore, we draw on a re-weighting strategy from supervised learning to compensate for the shortcomings of temperature adjustment in explicit quantity perception.Extensive experiments on commonly used long-tailed datasets demonstrate that APS improves performance effectively and efficiently. Our findings reveal the potential for deeper integration between supervised and self-supervised learning, paving the way for robust models that handle real-world class imbalance.",
        "published": "2024-10-31 04:00:00",
        "id": "1146fa46-d1cf-4688-8318-5a3e3e921f15",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了Adaptive Paradigm Synergy (APS)这种跨范式目标，通过自适应温度调整重新审视对比学习并借鉴监督学习的重加权策略，以解决长尾分布数据集中自监督学习性能下降问题并提升模型性能。"
        },
        "tokens": 821
    },
    {
        "title": "Coupling deal.II and FROSch: A Sustainable and Accessible (O)RAS Preconditioner",
        "link": "https://arxiv.org/abs/2410.22871",
        "description": "arXiv:2410.22871v1 Announce Type: new \nAbstract: In this work, restricted additive Schwarz (RAS) and optimized restricted additive Schwarz (ORAS) preconditioners from the Trilinos package FROSch (Fast and Robust Overlapping Schwarz) are employed to solve model problems implemented using deal.II (differential equations analysis library). Therefore, a Tpetra-based interface for coupling deal.II and FROSch is implemented. While RAS preconditioners have been available before, ORAS preconditioners have been newly added to FROSch. The FROSch-deal.II interface works for both Lagrange-based and N\\'ed\\'elec finite elements. Here, as model problems, nonstationary, nonlinear, variational-monolithic fluid-structure interaction and the indefinite time-harmonic Maxwell's equations are considered. Several numerical experiments in two and three spatial dimensions confirm the performance of the preconditioners as well as the FROSch-deal.II interface. In conclusion, the overall software interface is straightforward and easy to use while giving satisfactory solver performances for challenging PDE systems.",
        "published": "2024-10-31 04:00:00",
        "id": "c4cdd16a-46bd-4a6b-8486-55bebebc9a94",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "实现基于Tpetra的接口来耦合deal.II和FROSch，使用其中的预处理器求解模型问题，数值实验证实预处理器和接口的性能。"
        },
        "tokens": 823
    },
    {
        "title": "DAVINCI: A Single-Stage Architecture for Constrained CAD Sketch Inference",
        "link": "https://arxiv.org/abs/2410.22857",
        "description": "arXiv:2410.22857v1 Announce Type: new \nAbstract: This work presents DAVINCI, a unified architecture for single-stage Computer-Aided Design (CAD) sketch parameterization and constraint inference directly from raster sketch images. By jointly learning both outputs, DAVINCI minimizes error accumulation and enhances the performance of constrained CAD sketch inference. Notably, DAVINCI achieves state-of-the-art results on the large-scale SketchGraphs dataset, demonstrating effectiveness on both precise and hand-drawn raster CAD sketches. To reduce DAVINCI's reliance on large-scale annotated datasets, we explore the efficacy of CAD sketch augmentations. We introduce Constraint-Preserving Transformations (CPTs), i.e. random permutations of the parametric primitives of a CAD sketch that preserve its constraints. This data augmentation strategy allows DAVINCI to achieve reasonable performance when trained with only 0.1% of the SketchGraphs dataset. Furthermore, this work contributes a new version of SketchGraphs, augmented with CPTs. The newly introduced CPTSketchGraphs dataset includes 80 million CPT-augmented sketches, thus providing a rich resource for future research in the CAD sketch domain.",
        "published": "2024-10-31 04:00:00",
        "id": "c51ece8d-812e-4c96-ba8a-357899089a8e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "DAVINCI是一种单阶段架构，用于从光栅草图图像直接进行约束CAD草图推理，在SketchGraphs数据集上取得了最先进的成果，文中探索了CAD草图增强的功效，还贡献了新的CPTSketchGraphs数据集。"
        },
        "tokens": 850
    },
    {
        "title": "SFA-UNet: More Attention to Multi-Scale Contrast and Contextual Information in Infrared Small Object Segmentation",
        "link": "https://arxiv.org/abs/2410.22881",
        "description": "arXiv:2410.22881v1 Announce Type: new \nAbstract: Computer vision researchers have extensively worked on fundamental infrared visual recognition for the past few decades. Among various approaches, deep learning has emerged as the most promising candidate. However, Infrared Small Object Segmentation (ISOS) remains a major focus due to several challenges including: 1) the lack of effective utilization of local contrast and global contextual information; 2) the potential loss of small objects in deep models; and 3) the struggling to capture fine-grained details and ignore noise. To address these challenges, we propose a modified U-Net architecture, named SFA-UNet, by combining Scharr Convolution (SC) and Fast Fourier Convolution (FFC) in addition to vertical and horizontal Attention gates (AG) into UNet. SFA-UNet utilizes double convolution layers with the addition of SC and FFC in its encoder and decoder layers. SC helps to learn the foreground-to-background contrast information whereas FFC provide multi-scale contextual information while mitigating the small objects vanishing problem. Additionally, the introduction of vertical AGs in encoder layers enhances the model's focus on the targeted object by ignoring irrelevant regions. We evaluated the proposed approach on publicly available, SIRST and IRSTD datasets, and achieved superior performance by an average 0.75% with variance of 0.025 of all combined metrics in multiple runs as compared to the existing state-of-the-art methods",
        "published": "2024-10-31 04:00:00",
        "id": "28b3b0c6-950a-4ec2-9ecf-bb891f3fd134",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为SFA - UNet的改进U - Net架构，通过结合多种卷积和注意力门解决红外小目标分割的挑战，在公开数据集上取得较优性能。"
        },
        "tokens": 896
    },
    {
        "title": "Prune and Repaint: Content-Aware Image Retargeting for any Ratio",
        "link": "https://arxiv.org/abs/2410.22865",
        "description": "arXiv:2410.22865v1 Announce Type: new \nAbstract: Image retargeting is the task of adjusting the aspect ratio of images to suit different display devices or presentation environments. However, existing retargeting methods often struggle to balance the preservation of key semantics and image quality, resulting in either deformation or loss of important objects, or the introduction of local artifacts such as discontinuous pixels and inconsistent regenerated content. To address these issues, we propose a content-aware retargeting method called PruneRepaint. It incorporates semantic importance for each pixel to guide the identification of regions that need to be pruned or preserved in order to maintain key semantics. Additionally, we introduce an adaptive repainting module that selects image regions for repainting based on the distribution of pruned pixels and the proportion between foreground size and target aspect ratio, thus achieving local smoothness after pruning. By focusing on the content and structure of the foreground, our PruneRepaint approach adaptively avoids key content loss and deformation, while effectively mitigating artifacts with local repainting. We conduct experiments on the public RetargetMe benchmark and demonstrate through objective experimental results and subjective user studies that our method outperforms previous approaches in terms of preserving semantics and aesthetics, as well as better generalization across diverse aspect ratios. Codes will be available at https://github.com/fhshen2022/PruneRepaint.",
        "published": "2024-10-31 04:00:00",
        "id": "2acef98b-c402-4bdc-bdd3-c20f261ffeb1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为PruneRepaint的内容感知重定向方法来调整图像宽高比，通过考虑像素语义重要性和自适应重绘模块，在保持关键语义的同时减轻局部伪影，实验证明其在语义、美学保存和泛化方面优于以往方法且代码将公开。"
        },
        "tokens": 887
    },
    {
        "title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies",
        "link": "https://arxiv.org/abs/2410.22886",
        "description": "arXiv:2410.22886v1 Announce Type: new \nAbstract: Curriculum Learning has been a popular strategy to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge. However, it has not led to considerable improvements over non-curriculum models. We assess whether theoretical linguistic acquisition theories can be used to specify more fine-grained curriculum learning strategies, creating age-ordered corpora of Child-Directed Speech for four typologically distant language families to implement SSLMs and acquisition-inspired curricula cross-lingually. Comparing the success of three objective curricula (Growing, Inwards and MMM) that precisely replicate the predictions of acquisition theories on a standard SSLM architecture, we find fine-grained acquisition-inspired curricula can outperform non-curriculum baselines and performance benefits of curricula strategies in SSLMs can be derived by specifying fine-grained language-specific curricula that precisely replicate language acquisition theories.",
        "published": "2024-10-31 04:00:00",
        "id": "c31b019c-f3db-469e-a605-2f679464a2c5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究通过构建特定语料库评估基于语言习得理论的课程学习策略对小尺度语言模型（SSLM）的效果，发现精细的课程学习策略能提升SSLM性能。"
        },
        "tokens": 793
    },
    {
        "title": "Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector",
        "link": "https://arxiv.org/abs/2410.22888",
        "description": "arXiv:2410.22888v1 Announce Type: new \nAbstract: Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature. To facilitate research on this critical safety problem, we first construct a new laRge-scale Adervsarial images dataset with Diverse hArmful Responses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses. With the new RADAR dataset, we further develop a novel and effective iN-time Embedding-based AdveRSarial Image DEtection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call the attacking direction, to achieve the detection of adversarial images against benign ones in the input. Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency, and cross-model transferrability of our proposed method. Our code is available at https://github.com/mob-scu/RADAR-NEARSIDE",
        "published": "2024-10-31 04:00:00",
        "id": "622fe292-8bcb-4371-aa57-d2689097828d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "视觉语言模型易受对抗性攻击，为推动相关研究先构建了新数据集RADAR，进而开发出NEARSIDE方法，实验证明该方法对两种视觉语言模型有效、高效且可跨模型迁移，代码已开源。"
        },
        "tokens": 825
    },
    {
        "title": "VPO: Leveraging the Number of Votes in Preference Optimization",
        "link": "https://arxiv.org/abs/2410.22891",
        "description": "arXiv:2410.22891v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) trains a language model using human preference data, bypassing the explicit reward modeling phase of Reinforcement Learning from Human Feedback (RLHF). By iterating over sentence pairs in a preference dataset, DPO enhances generation quality by increasing the likelihood of producing preferred sentences over less favored ones. Preference datasets are typically created by selecting preferred sentences through a voting process involving multiple individuals, as opinions can vary due to the subjective nature of human preferences. While the number of votes offers insight into whether a sentence pair is clearly preferable or controversial, current methods do not fully leverage this information. In this paper, we introduce a technique that leverages user voting data to better align with diverse subjective preferences. We employ the Bayesian Minimum Mean Square Error (Bayesian MMSE) estimator to model the probability that one generation is preferable to another. Using this estimated probability as a target, we develop the Vote-based Preference Optimization (VPO) framework, which incorporates the number of votes on both sides to distinguish between controversial and obvious generation pairs. We show that previous algorithms, such as DPO and Identity Preference Optimization (IPO), can be extended using the proposed framework, termed VDPO and VIPO. Our experiments demonstrate that these proposed algorithms outperform various existing methods, including their base algorithms.",
        "published": "2024-10-31 04:00:00",
        "id": "11af956c-c98f-43c8-83be-72f0251769bf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文介绍一种利用用户投票数据的技术VPO，使用贝叶斯MMSE估计器建模一个生成结果优于另一个的概率，开发出包含VPO框架的算法VDPO和VIPO，实验表明这些算法优于现有方法。"
        },
        "tokens": 878
    },
    {
        "title": "Human-inspired Grasping Strategies of Fresh Fruits and Vegetables Applied to Robotic Manipulation",
        "link": "https://arxiv.org/abs/2410.22893",
        "description": "arXiv:2410.22893v1 Announce Type: new \nAbstract: Robotic manipulation of fresh fruits and vegetables, including the grasping of multiple loose items, has a strong industrial need but it still is a challenging task for robotic manipulation. This paper outlines the distinctive manipulation strategies used by humans to pick loose fruits and vegetables with the aim to better adopt them for robotic manipulation of diverse items. In this work we present a first version of a robotic setup designed to pick different single or multiple fresh items, featuring multi-fingered compliant robotic gripper. We analyse human grasping strategies from the perspective of industrial Key Performance Indicators (KPIs) used in the logistic sector. The robotic system was validated using the same KPIs, as well as taking into account human performance and strategies. This paper lays the foundation for future development of the robotic demonstrator for fresh fruit and vegetable intelligent manipulation, and outlines the need for generic approaches to handle the complexity of the task.",
        "published": "2024-10-31 04:00:00",
        "id": "755033bc-8e49-4b71-8b25-701f3a0c7a55",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文旨在借鉴人类抓取生鲜果蔬的策略用于机器人操作，介绍了相关机器人装置的初版设计，从物流行业关键绩效指标的角度分析人类抓取策略，并用相同指标验证机器人系统，为生鲜果蔬智能操作机器人演示器的未来发展奠定基础。"
        },
        "tokens": 805
    },
    {
        "title": "Constrained Trajectory Optimization for Hybrid Dynamical Systems",
        "link": "https://arxiv.org/abs/2410.22894",
        "description": "arXiv:2410.22894v1 Announce Type: new \nAbstract: Hybrid dynamical systems pose significant challenges for effective planning and control, especially when additional constraints such as obstacle avoidance, state boundaries, and actuation limits are present. In this letter, we extend the recently proposed Hybrid iLQR method [1] to handle state and input constraints within an indirect optimization framework, aiming to preserve computational efficiency and ensure dynamic feasibility. Specifically, we incorporate two constraint handling mechanisms into the Hybrid iLQR: Discrete Barrier State and Augmented Lagrangian methods. Comprehensive simulations across various operational situations are conducted to evaluate and compare the performance of these extended methods in terms of convergence and their ability to handle infeasible starting trajectories. Results indicate that while the Discrete Barrier State approach is more computationally efficient, the Augmented Lagrangian method outperforms it in complex and real-world scenarios with infeasible initial trajectories.",
        "published": "2024-10-31 04:00:00",
        "id": "67e892f9-c6f4-4ad5-aacd-5f5e5d483a20",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章将Hybrid iLQR方法扩展为在间接优化框架内处理状态和输入约束，采用离散屏障状态和增广拉格朗日方法，通过模拟评估比较了两种方法在收敛性和处理不可行起始轨迹能力方面的性能。"
        },
        "tokens": 784
    },
    {
        "title": "Combining psychoanalysis and computer science: an empirical study of the relationship between emotions and the Lacanian discourses",
        "link": "https://arxiv.org/abs/2410.22895",
        "description": "arXiv:2410.22895v1 Announce Type: new \nAbstract: This research explores the interdisciplinary interaction between psychoanalysis and computer science, suggesting a mutually beneficial exchange. Indeed, psychoanalytic concepts can enrich technological applications involving unconscious, elusive aspects of the human factor, such as social media and other interactive digital platforms. Conversely, computer science, especially Artificial Intelligence (AI), can contribute quantitative concepts and methods to psychoanalysis, identifying patterns and emotional cues in human expression. In particular, this research aims to apply computer science methods to establish fundamental relationships between emotions and Lacanian discourses. Such relations are discovered in our approach via empirical investigation and statistical analysis, and are eventually validated in a theoretical (psychoanalytic) way. It is worth noting that, although emotions have been sporadically studied in Lacanian theory, to the best of our knowledge a systematic, detailed investigation of their role is missing. Such fine-grained understanding of the role of emotions can also make the identification of Lacanian discourses more effective and easy in practise. In particular, our methods indicate the emotions with highest differentiation power in terms of corresponding discourses; conversely, we identify for each discourse the most characteristic emotions it admits. As a matter of fact, we develop a method which we call Lacanian Discourse Discovery (LDD), that simplifies (via systematizing) the identification of Lacanian discourses in texts. Although the main contribution of this paper is inherently theoretical (psychoanalytic), it can also facilitate major practical applications in the realm of interactive digital systems. Indeed, our approach can be automated through Artificial Intelligence methods that effectively identify emotions (and corresponding discourses) in texts.",
        "published": "2024-10-31 04:00:00",
        "id": "53fff2e7-1530-477a-be39-d1355656a117",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "论文探索精神分析与计算机科学的跨学科交互，重点是用计算机科学方法建立情感与拉康话语之间的关系，开发了拉康话语发现方法，成果可通过人工智能方法自动化应用于交互数字系统。"
        },
        "tokens": 940
    },
    {
        "title": "A Graph-Based Model for Vehicle-Centric Data Sharing Ecosystem",
        "link": "https://arxiv.org/abs/2410.22897",
        "description": "arXiv:2410.22897v1 Announce Type: new \nAbstract: The development of technologies has prompted a paradigm shift in the automotive industry, with an increasing focus on connected services and autonomous driving capabilities. This transformation allows vehicles to collect and share vast amounts of vehicle-specific and personal data. While these technological advancements offer enhanced user experiences, they also raise privacy concerns. To understand the ecosystem of data collection and sharing in modern vehicles, we adopted the ontology 101 methodology to incorporate information extracted from different sources, including analysis of privacy policies using GPT-4, a small-scale systematic literature review, and an existing ontology, to develop a high-level conceptual graph-based model, aiming to get insights into how modern vehicles handle data exchange among different parties. This serves as a foundational model with the flexibility and scalability to further expand for modelling and analysing data sharing practices across diverse contexts. Two realistic examples were developed to demonstrate the usefulness and effectiveness of discovering insights into privacy regarding vehicle-related data sharing. We also recommend several future research directions, such as exploring advanced ontology languages for reasoning tasks, supporting topological analysis for discovering data privacy risks/concerns, and developing useful tools for comparative analysis, to strengthen the understanding of the vehicle-centric data sharing ecosystem.",
        "published": "2024-10-31 04:00:00",
        "id": "676239aa-cae5-43e5-9eef-5b1da5f284c9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为理解现代汽车数据收集和共享生态系统，采用本体101方法构建基于概念图的模型，给出两个实例展示其在发现车辆相关数据共享隐私方面的有效性，并推荐了未来研究方向。"
        },
        "tokens": 840
    },
    {
        "title": "YOLOv11 for Vehicle Detection: Advancements, Performance, and Applications in Intelligent Transportation Systems",
        "link": "https://arxiv.org/abs/2410.22898",
        "description": "arXiv:2410.22898v1 Announce Type: new \nAbstract: Accurate vehicle detection is essential for the development of intelligent transportation systems, autonomous driving, and traffic monitoring. This paper presents a detailed analysis of YOLO11, the latest advancement in the YOLO series of deep learning models, focusing exclusively on vehicle detection tasks. Building upon the success of its predecessors, YOLO11 introduces architectural improvements designed to enhance detection speed, accuracy, and robustness in complex environments. Using a comprehensive dataset comprising multiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we evaluate YOLO11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLO11 surpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more occluded vehicles while maintaining a competitive inference time, making it well-suited for real-time applications. Comparative analysis shows significant improvements in the detection of complex vehicle geometries, further contributing to the development of efficient and scalable vehicle detection systems. This research highlights YOLO11's potential to enhance autonomous vehicle performance and traffic monitoring systems, offering insights for future developments in the field.",
        "published": "2024-10-31 04:00:00",
        "id": "65111efc-192d-474e-b3b2-f408b700e481",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍了YOLO11在车辆检测方面的性能、优势及应用，通过综合数据集评估其性能指标，发现它在检测小且遮挡车辆方面优于之前版本且推理时间有竞争力，有助于提升自动驾驶和交通监测系统。"
        },
        "tokens": 854
    },
    {
        "title": "Wormhole Loss for Partial Shape Matching",
        "link": "https://arxiv.org/abs/2410.22899",
        "description": "arXiv:2410.22899v1 Announce Type: new \nAbstract: When matching parts of a surface to its whole, a fundamental question arises: Which points should be included in the matching process? The issue is intensified when using isometry to measure similarity, as it requires the validation of whether distances measured between pairs of surface points should influence the matching process. The approach we propose treats surfaces as manifolds equipped with geodesic distances, and addresses the partial shape matching challenge by introducing a novel criterion to meticulously search for consistent distances between pairs of points. The new criterion explores the relation between intrinsic geodesic distances between the points, geodesic distances between the points and surface boundaries, and extrinsic distances between boundary points measured in the embedding space. It is shown to be less restrictive compared to previous measures and achieves state-of-the-art results when used as a loss function in training networks for partial shape matching.",
        "published": "2024-10-31 04:00:00",
        "id": "dd272cf1-c1b5-4157-8bf2-818922fdfe4d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新准则用于部分形状匹配，将表面视为配备测地距离的流形，探索点间内在测地距离、点与表面边界间测地距离、边界点在嵌入空间中的外在距离之间的关系，作为损失函数可达到先进水平。"
        },
        "tokens": 772
    },
    {
        "title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
        "link": "https://arxiv.org/abs/2410.22901",
        "description": "arXiv:2410.22901v1 Announce Type: new \nAbstract: We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (\\url{https://songkey.github.io/hellomeme}).",
        "published": "2024-10-31 04:00:00",
        "id": "8522f7ba-0538-4313-abe8-b2e661ad0c6e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种将适配器插入文本到图像基础模型的有效方法，通过优化与2D特征图相关的注意力机制提升适配器性能，已在表情包视频生成任务中验证并取得成果，将发布相关代码。"
        },
        "tokens": 767
    },
    {
        "title": "From Babble to Words: Pre-Training Language Models on Continuous Streams of Phonemes",
        "link": "https://arxiv.org/abs/2410.22906",
        "description": "arXiv:2410.22906v1 Announce Type: new \nAbstract: Language models are typically trained on large corpora of text in their default orthographic form. However, this is not the only option; representing data as streams of phonemes can offer unique advantages, from deeper insights into phonological language acquisition to improved performance on sound-based tasks. The challenge lies in evaluating the impact of phoneme-based training, as most benchmarks are also orthographic. To address this, we develop a pipeline to convert text datasets into a continuous stream of phonemes. We apply this pipeline to the 100-million-word pre-training dataset from the BabyLM challenge, as well as to standard language and grammatical benchmarks, enabling us to pre-train and evaluate a model using phonemic input representations. Our results show that while phoneme-based training slightly reduces performance on traditional language understanding tasks, it offers valuable analytical and practical benefits.",
        "published": "2024-10-31 04:00:00",
        "id": "137f0343-ca6a-4438-ae72-afad17fbf192",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究人员开发一种将文本数据集转换为连续音素流的管道，应用于预训练数据集和相关基准测试，发现基于音素的训练虽会略降低传统语言理解任务性能，但有分析和实用价值。"
        },
        "tokens": 783
    },
    {
        "title": "The Evolution Of The Digital Inheritance: Legal, Technical, And Practical Dimensions Of Cryptocurrency Transfer Through Succession In French-Inspired Legal Systems",
        "link": "https://arxiv.org/abs/2410.22907",
        "description": "arXiv:2410.22907v1 Announce Type: new \nAbstract: In recent years, cryptocurrencies have enjoyed increased popularity in all domains. Thus, in this context, it is important to understand how these digital assets can be transmitted, both legally and efficiently, in the event of the death of their owner. The present paper analyses the mechanisms of cryptocurrencies, analysing from a technical point of view aspects related to blockchain technology, virtual wallets or cryptographic keys, as well as various types of operations regarding this type of virtual currencies. The study also examines the legal aspects related to cryptocurrencies, with an emphasis on the diversity of their status in different global jurisdictions as well as the impact on inheritance planning. The case studies present tangible examples related to successions with cryptocurrencies as the main object, thus completing the exposition related to the main challenges faced by the heirs in the transfer process. In this way, this paper offers possible solutions and recommendations related to inheritance planning with cryptocurrencies as its main object, including the legal and fiscal aspects that must be taken into account when planning a digital succession.",
        "published": "2024-10-31 04:00:00",
        "id": "b7c77e9c-3c63-467f-b4a0-22073587aed2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章从技术、法律和实践层面分析了法国式法律体系下加密货币在继承中的转移，包括加密货币机制、法律方面的考量，还通过案例研究展示了相关挑战，并提出数字继承规划的解决方案和建议。"
        },
        "tokens": 826
    },
    {
        "title": "Federated UCBVI: Communication-Efficient Federated Regret Minimization with Heterogeneous Agents",
        "link": "https://arxiv.org/abs/2410.22908",
        "description": "arXiv:2410.22908v1 Announce Type: new \nAbstract: In this paper, we present the Federated Upper Confidence Bound Value Iteration algorithm ($\\texttt{Fed-UCBVI}$), a novel extension of the $\\texttt{UCBVI}$ algorithm (Azar et al., 2017) tailored for the federated learning framework. We prove that the regret of $\\texttt{Fed-UCBVI}$ scales as $\\tilde{\\mathcal{O}}(\\sqrt{H^3 |\\mathcal{S}| |\\mathcal{A}| T / M})$, with a small additional term due to heterogeneity, where $|\\mathcal{S}|$ is the number of states, $|\\mathcal{A}|$ is the number of actions, $H$ is the episode length, $M$ is the number of agents, and $T$ is the number of episodes. Notably, in the single-agent setting, this upper bound matches the minimax lower bound up to polylogarithmic factors, while in the multi-agent scenario, $\\texttt{Fed-UCBVI}$ has linear speed-up. To conduct our analysis, we introduce a new measure of heterogeneity, which may hold independent theoretical interest. Furthermore, we show that, unlike existing federated reinforcement learning approaches, $\\texttt{Fed-UCBVI}$'s communication complexity only marginally increases with the number of agents.",
        "published": "2024-10-31 04:00:00",
        "id": "c34b7bdc-19be-45c1-8c49-c2b8ea07704a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出Fed - UCBVI算法，分析其遗憾规模，引入新的异质性衡量方法，且该算法通信复杂度随代理数量增加幅度小。"
        },
        "tokens": 877
    },
    {
        "title": "UniRiT: Towards Few-Shot Non-Rigid Point Cloud Registration",
        "link": "https://arxiv.org/abs/2410.22909",
        "description": "arXiv:2410.22909v1 Announce Type: new \nAbstract: Non-rigid point cloud registration is a critical challenge in 3D scene understanding, particularly in surgical navigation. Although existing methods achieve excellent performance when trained on large-scale, high-quality datasets, these datasets are prohibitively expensive to collect and annotate, e.g., organ data in authentic medical scenarios. With insufficient training samples and data noise, existing methods degrade significantly since non-rigid patterns are more flexible and complicated than rigid ones, and the distributions across samples are more distinct, leading to higher difficulty in representation learning with few data. In this work, we aim to deal with this challenging few-shot non-rigid point cloud registration problem. Based on the observation that complex non-rigid transformation patterns can be decomposed into rigid and small non-rigid transformations, we propose a novel and effective framework, UniRiT. UniRiT adopts a two-step registration strategy that first aligns the centroids of the source and target point clouds and then refines the registration with non-rigid transformations, thereby significantly reducing the problem complexity. To validate the performance of UniRiT on real-world datasets, we introduce a new dataset, MedMatch3D, which consists of real human organs and exhibits high variability in sample distribution. We further establish a new challenging benchmark for few-shot non-rigid registration. Extensive empirical results demonstrate that UniRiT achieves state-of-the-art performance on MedMatch3D, improving the existing best approach by 94.22%.",
        "published": "2024-10-31 04:00:00",
        "id": "bd71615b-79d1-43cb-af3a-5ef6f5c476bb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "针对非刚性点云注册在训练样本不足和数据有噪声时性能下降的问题，提出UniRiT框架，引入MedMatch3D数据集并建立新基准，UniRiT在该数据集上达到最先进性能。"
        },
        "tokens": 902
    },
    {
        "title": "An Efficient Representation of Whole-body Model Predictive Control for Online Compliant Dual-arm Mobile Manipulation",
        "link": "https://arxiv.org/abs/2410.22910",
        "description": "arXiv:2410.22910v1 Announce Type: new \nAbstract: Dual-arm mobile manipulators can transport and manipulate large-size objects with simple end-effectors. To interact with dynamic environments with strict safety and compliance requirements, achieving whole-body motion planning online while meeting various hard constraints for such highly redundant mobile manipulators poses a significant challenge. We tackle this challenge by presenting an efficient representation of whole-body motion trajectories within our bilevel model-based predictive control (MPC) framework. We utilize B\\'ezier-curve parameterization to represent the optimized collision-free trajectories of two collaborating end-effectors in the first MPC, facilitating fast long-horizon object-oriented motion planning in SE(3) while considering approximated feasibility constraints. This approach is further applied to parameterize whole-body trajectories in the second MPC for whole-body motion generation with predictive admittance control in a relatively short horizon while satisfying whole-body hard constraints. This representation enables two MPCs with continuous properties, thereby avoiding inaccurate model-state transition and dense decision-variable settings in existing MPCs using the discretization method. It strengthens the online execution of the bilevel MPC framework in high-dimensional space and facilitates the generation of consistent commands for our hybrid position/velocity-controlled robot. The simulation comparisons and real-world experiments demonstrate the efficiency and robustness of this approach in various scenarios for static and dynamic obstacle avoidance, and compliant interaction control with the manipulated object and external disturbances.",
        "published": "2024-10-31 04:00:00",
        "id": "f7793b1d-8478-42dc-aeae-df918b63c553",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出双级基于模型预测控制（MPC）框架内全身运动轨迹的有效表示，通过贝塞尔曲线参数化优化无碰撞轨迹，用于双臂移动机械手在高维空间中进行在线全身运动规划、避障及交互控制，经模拟比较和实验证明了该方法的有效性和鲁棒性。"
        },
        "tokens": 909
    },
    {
        "title": "CopRA: A Progressive LoRA Training Strategy",
        "link": "https://arxiv.org/abs/2410.22911",
        "description": "arXiv:2410.22911v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) is a parameter-efficient technique for rapidly fine-tuning foundation models. In standard LoRA training dynamics, models tend to quickly converge to a local optimum near the initialization. However, this local optimum may not be ideal for out-of-distribution data or tasks such as merging and pruning. In this work, we propose a novel progressive training strategy for LoRA with random layer dropping. This strategy also optimizes the Shapley value of LoRA parameters in each layer, treating each layer as a player in a cooperative game. We refer to this method as Cooperative LoRA (CopRA). Our experimental results demonstrate that parameters trained with CopRA exhibit linear mode connectivity, which enables efficient model merging. This also paves the way for federated learning and multi-task learning via LoRA merging. Additionally, by optimizing the Shapley value, CopRA shows superior performance in pruning tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "cbaa9caa-ea72-4fcd-9915-167cc7a404fc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为CopRA的LoRA渐进式训练策略，实验结果显示CopRA训练的参数具有线性模式连通性，在模型合并方面表现高效，在剪枝任务中性能优越。"
        },
        "tokens": 783
    },
    {
        "title": "Explainable Behavior Cloning: Teaching Large Language Model Agents through Learning by Demonstration",
        "link": "https://arxiv.org/abs/2410.22916",
        "description": "arXiv:2410.22916v1 Announce Type: new \nAbstract: Autonomous mobile app interaction has become increasingly important with growing complexity of mobile applications. Developing intelligent agents that can effectively navigate and interact with mobile apps remains a significant challenge. In this paper, we propose an Explainable Behavior Cloning LLM Agent (EBC-LLMAgent), a novel approach that combines large language models (LLMs) with behavior cloning by learning demonstrations to create intelligent and explainable agents for autonomous mobile app interaction. EBC-LLMAgent consists of three core modules: Demonstration Encoding, Code Generation, and UI Mapping, which work synergistically to capture user demonstrations, generate executable codes, and establish accurate correspondence between code and UI elements. We introduce the Behavior Cloning Chain Fusion technique to enhance the generalization capabilities of the agent. Extensive experiments on five popular mobile applications from diverse domains demonstrate the superior performance of EBC-LLMAgent, achieving high success rates in task completion, efficient generalization to unseen scenarios, and the generation of meaningful explanations.",
        "published": "2024-10-31 04:00:00",
        "id": "65d293a6-4441-472f-8818-4ccf3786c3fc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出EBC - LLMAgent用于自主移动应用交互，含三个核心模块，采用行为克隆链融合技术，在五个流行移动应用实验中表现优异。"
        },
        "tokens": 799
    },
    {
        "title": "Simulation-Free Training of Neural ODEs on Paired Data",
        "link": "https://arxiv.org/abs/2410.22918",
        "description": "arXiv:2410.22918v1 Announce Type: new \nAbstract: In this work, we investigate a method for simulation-free training of Neural Ordinary Differential Equations (NODEs) for learning deterministic mappings between paired data. Despite the analogy of NODEs as continuous-depth residual networks, their application in typical supervised learning tasks has not been popular, mainly due to the large number of function evaluations required by ODE solvers and numerical instability in gradient estimation. To alleviate this problem, we employ the flow matching framework for simulation-free training of NODEs, which directly regresses the parameterized dynamics function to a predefined target velocity field. Contrary to generative tasks, however, we show that applying flow matching directly between paired data can often lead to an ill-defined flow that breaks the coupling of the data pairs (e.g., due to crossing trajectories). We propose a simple extension that applies flow matching in the embedding space of data pairs, where the embeddings are learned jointly with the dynamic function to ensure the validity of the flow which is also easier to learn. We demonstrate the effectiveness of our method on both regression and classification tasks, where our method outperforms existing NODEs with a significantly lower number of function evaluations. The code is available at https://github.com/seminkim/simulation-free-node.",
        "published": "2024-10-31 04:00:00",
        "id": "92dcf8d0-b4b5-4f36-8030-a047035d8663",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章探讨了一种对神经常微分方程（NODEs）进行无模拟训练以学习成对数据间确定性映射的方法，采用流匹配框架解决传统训练问题，提出在数据对嵌入空间应用流匹配的扩展方法，在回归和分类任务中验证了该方法的有效性并开源了代码。"
        },
        "tokens": 877
    },
    {
        "title": "Cyber-physical WebAssembly: Secure Hardware Interfaces and Pluggable Drivers",
        "link": "https://arxiv.org/abs/2410.22919",
        "description": "arXiv:2410.22919v1 Announce Type: new \nAbstract: The rapid expansion of Internet of Things (IoT), edge, and embedded devices in the past decade has introduced numerous challenges in terms of security and configuration management. Simultaneously, advances in cloud-native development practices have greatly enhanced the development experience and facilitated quicker updates, thereby enhancing application security. However, applying these advances to IoT, edge, and embedded devices remains a complex task, primarily due to the heterogeneous environments and the need to support devices with extended lifespans. WebAssembly and the WebAssembly System Interface (WASI) has emerged as a promising technology to bridge this gap. As WebAssembly becomes more popular on IoT, edge, and embedded devices, there is a growing demand for hardware interface support in WebAssembly programs. This work presents WASI proposals and proof-of-concept implementations to enable hardware interaction with I2C and USB, which are two commonly used protocols in IoT, directly from WebAssembly applications. This is achieved by running the device drivers within WebAssembly as well. A thorough evaluation of the proof of concepts shows that WASI-USB introduces a minimal overhead of at most 8% compared to native operating system USB APIs. However, the results show that runtime initialization overhead can be significant in low-latency applications.",
        "published": "2024-10-31 04:00:00",
        "id": "9c14d3fe-e251-431a-954c-1f5d965ecd71",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "随着物联网、边缘和嵌入式设备的发展，WebAssembly和WASI技术有望解决其安全和配置管理问题，文中展示了WASI提案和概念验证实现，可使WebAssembly应用直接与I2C和USB硬件接口交互，评估显示WASI - USB相比原生系统USB API开销极小但低延迟应用中初始化开销显著。"
        },
        "tokens": 887
    },
    {
        "title": "High-Fidelity Document Stain Removal via A Large-Scale Real-World Dataset and A Memory-Augmented Transformer",
        "link": "https://arxiv.org/abs/2410.22922",
        "description": "arXiv:2410.22922v1 Announce Type: new \nAbstract: Document images are often degraded by various stains, significantly impacting their readability and hindering downstream applications such as document digitization and analysis. The absence of a comprehensive stained document dataset has limited the effectiveness of existing document enhancement methods in removing stains while preserving fine-grained details. To address this challenge, we construct StainDoc, the first large-scale, high-resolution ($2145\\times2245$) dataset specifically designed for document stain removal. StainDoc comprises over 5,000 pairs of stained and clean document images across multiple scenes. This dataset encompasses a diverse range of stain types, severities, and document backgrounds, facilitating robust training and evaluation of document stain removal algorithms. Furthermore, we propose StainRestorer, a Transformer-based document stain removal approach. StainRestorer employs a memory-augmented Transformer architecture that captures hierarchical stain representations at part, instance, and semantic levels via the DocMemory module. The Stain Removal Transformer (SRTransformer) leverages these feature representations through a dual attention mechanism: an enhanced spatial attention with an expanded receptive field, and a channel attention captures channel-wise feature importance. This combination enables precise stain removal while preserving document content integrity. Extensive experiments demonstrate StainRestorer's superior performance over state-of-the-art methods on the StainDoc dataset and its variants StainDoc\\_Mark and StainDoc\\_Seal, establishing a new benchmark for document stain removal. Our work highlights the potential of memory-augmented Transformers for this task and contributes a valuable dataset to advance future research.",
        "published": "2024-10-31 04:00:00",
        "id": "a1a374d7-8ce7-40aa-923d-60cc99af0caa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决文档污渍去除问题，构建首个大规模高分辨率文档污渍数据集StainDoc，提出基于Transformer的StainRestorer方法，实验证明其在数据集上优于现有方法。"
        },
        "tokens": 930
    },
    {
        "title": "An Individual Identity-Driven Framework for Animal Re-Identification",
        "link": "https://arxiv.org/abs/2410.22927",
        "description": "arXiv:2410.22927v1 Announce Type: new \nAbstract: Reliable re-identification of individuals within large wildlife populations is crucial for biological studies, ecological research, and wildlife conservation. Classic computer vision techniques offer a promising direction for Animal Re-identification (Animal ReID), but their backbones' close-set nature limits their applicability and generalizability. Despite the demonstrated effectiveness of vision-language models like CLIP in re-identifying persons and vehicles, their application to Animal ReID remains limited due to unique challenges, such as the various visual representations of animals, including variations in poses and forms. To address these limitations, we leverage CLIP's cross-modal capabilities to introduce a two-stage framework, the \\textbf{Indiv}idual \\textbf{A}nimal \\textbf{ID}entity-Driven (IndivAID) framework, specifically designed for Animal ReID. In the first stage, IndivAID trains a text description generator by extracting individual semantic information from each image, generating both image-specific and individual-specific textual descriptions that fully capture the diverse visual concepts of each individual across animal images. In the second stage, IndivAID refines its learning of visual concepts by dynamically incorporating individual-specific textual descriptions with an integrated attention module to further highlight discriminative features of individuals for Animal ReID. Evaluation against state-of-the-art methods across eight benchmark datasets and a real-world Stoat dataset demonstrates IndivAID's effectiveness and applicability. Code is available at \\url{https://github.com/ywu840/IndivAID}.",
        "published": "2024-10-31 04:00:00",
        "id": "7074e404-d2b0-40ac-bb5e-440a4c2b3be9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于个体身份驱动的动物重新识别框架IndivAID，利用CLIP跨模态能力分两阶段处理动物图像的个体语义信息，经评估证明有效且适用。"
        },
        "tokens": 902
    },
    {
        "title": "GPTR: Gaussian Process Trajectory Representation for Continuous-Time Motion Estimation",
        "link": "https://arxiv.org/abs/2410.22931",
        "description": "arXiv:2410.22931v1 Announce Type: new \nAbstract: Continuous-time trajectory representation has gained significant popularity in recent years, as it offers an elegant formulation that allows the fusion of a larger number of sensors and sensing modalities, overcoming limitations of traditional discrete-time frameworks. To bolster the adoption of the continuous-time paradigm, we propose a so-called Gaussian Process Trajectory Representation (GPTR) framework for continuous-time motion estimation (CTME) tasks. Our approach stands out by employing a third-order random jerk model, featuring closed-form expressions for both rotational and translational state derivatives. This model provides smooth, continuous trajectory representations that are crucial for precise estimation of complex motion. To support the wider robotics and computer vision communities, we have made the source code for GPTR available as a light-weight header-only library. This format was chosen for its ease of integration, allowing developers to incorporate GPTR into existing systems without needing extensive code modifications. Moreover, we also provide a set of optimization examples with LiDAR, camera, IMU, UWB factors, and closed-form analytical Jacobians under the proposed GP framework. Our experiments demonstrate the efficacy and efficiency of GP-based trajectory representation in various motion estimation tasks, and the examples can serve as the prototype to help researchers quickly develop future applications such as batch optimization, calibration, sensor fusion, trajectory planning, etc., with continuous-time trajectory representation. Our project is accessible at https://github.com/brytsknguyen/gptr .",
        "published": "2024-10-31 04:00:00",
        "id": "3bea6f93-1857-4221-be17-5831a15d0aa1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出高斯过程轨迹表示（GPTR）框架用于连续时间运动估计任务，有三阶随机加加速度模型，提供源代码，还有优化示例，实验证明其有效性和效率。"
        },
        "tokens": 881
    },
    {
        "title": "Multi-Agent Large Language Models for Conversational Task-Solving",
        "link": "https://arxiv.org/abs/2410.22932",
        "description": "arXiv:2410.22932v1 Announce Type: new \nAbstract: In an era where single large language models have dominated the landscape of artificial intelligence for years, multi-agent systems arise as new protagonists in conversational task-solving. While previous studies have showcased their potential in reasoning tasks and creative endeavors, an analysis of their limitations concerning the conversational paradigms and the impact of individual agents is missing. It remains unascertained how multi-agent discussions perform across tasks of varying complexity and how the structure of these conversations influences the process. To fill that gap, this work systematically evaluates multi-agent systems across various discussion paradigms, assessing their strengths and weaknesses in both generative tasks and question-answering tasks. Alongside the experiments, I propose a taxonomy of 20 multi-agent research studies from 2022 to 2024, followed by the introduction of a framework for deploying multi-agent LLMs in conversational task-solving. I demonstrate that while multi-agent systems excel in complex reasoning tasks, outperforming a single model by leveraging expert personas, they fail on basic tasks. Concretely, I identify three challenges that arise: 1) While longer discussions enhance reasoning, agents fail to maintain conformity to strict task requirements, which leads to problem drift, making shorter conversations more effective for basic tasks. 2) Prolonged discussions risk alignment collapse, raising new safety concerns for these systems. 3) I showcase discussion monopolization through long generations, posing the problem of fairness in decision-making for tasks like summarization. This work uncovers both the potential and challenges that arise with multi-agent interaction and varying conversational paradigms, providing insights into how future research could improve the efficiency, performance, and safety of multi-agent LLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "ed04e71e-c05d-4234-aba8-d0b0a6878711",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "文章系统评估多智能体系统在不同讨论范式中的优劣，对2022 - 2024年的20项多智能体研究进行分类，提出一个用于对话任务解决的框架，揭示多智能体交互和不同对话范式中的潜力与挑战。"
        },
        "tokens": 960
    },
    {
        "title": "Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder",
        "link": "https://arxiv.org/abs/2410.22936",
        "description": "arXiv:2410.22936v1 Announce Type: new \nAbstract: While pre-trained image autoencoders are increasingly utilized in computer vision, the application of inverse graphics in 2D latent spaces has been under-explored. Yet, besides reducing the training and rendering complexity, applying inverse graphics in the latent space enables a valuable interoperability with other latent-based 2D methods. The major challenge is that inverse graphics cannot be directly applied to such image latent spaces because they lack an underlying 3D geometry. In this paper, we propose an Inverse Graphics Autoencoder (IG-AE) that specifically addresses this issue. To this end, we regularize an image autoencoder with 3D-geometry by aligning its latent space with jointly trained latent 3D scenes. We utilize the trained IG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline, which we implement in an open-source extension of the Nerfstudio framework, thereby unlocking latent scene learning for its supported methods. We experimentally confirm that Latent NeRFs trained with IG-AE present an improved quality compared to a standard autoencoder, all while exhibiting training and rendering accelerations with respect to NeRFs trained in the image space. Our project page can be found at https://ig-ae.github.io .",
        "published": "2024-10-31 04:00:00",
        "id": "d083b871-bca4-4a89-b843-67f5fd3d28ba",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出反向图形自动编码器（IG - AE），将NeRF引入潜在空间，通过实验证实其训练的潜在NeRF比标准自动编码器质量更好，且训练和渲染加速，相关代码已开源。"
        },
        "tokens": 863
    },
    {
        "title": "Thoughtful Adoption of NLP for Civic Participation: Understanding Differences Among Policymakers",
        "link": "https://arxiv.org/abs/2410.22937",
        "description": "arXiv:2410.22937v1 Announce Type: new \nAbstract: Natural language processing (NLP) tools have the potential to boost civic participation and enhance democratic processes because they can significantly increase governments' capacity to gather and analyze citizen opinions. However, their adoption in government remains limited, and harnessing their benefits while preventing unintended consequences remains a challenge. While prior work has focused on improving NLP performance, this work examines how different internal government stakeholders influence NLP tools' thoughtful adoption. We interviewed seven politicians (politically appointed officials as heads of government institutions) and thirteen public servants (career government employees who design and administrate policy interventions), inquiring how they choose whether and how to use NLP tools to support civic participation processes. The interviews suggest that policymakers across both groups focused on their needs for career advancement and the need to showcase the legitimacy and fairness of their work when considering NLP tool adoption and use. Because these needs vary between politicians and public servants, their preferred NLP features and tool designs also differ. Interestingly, despite their differing needs and opinions, neither group clearly identifies who should advocate for NLP adoption to enhance civic participation or address the unintended consequences of a poorly considered adoption. This lack of clarity in responsibility might have caused the governments' low adoption of NLP tools. We discuss how these findings reveal new insights for future HCI research. They inform the design of NLP tools for increasing civic participation efficiency and capacity, the design of other tools and methods that ensure thoughtful adoption of AI tools in government, and the design of NLP tools for collaborative use among users with different incentives and needs.",
        "published": "2024-10-31 04:00:00",
        "id": "22dde484-bbeb-4caf-9d12-e78227129c81",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "研究自然语言处理（NLP）工具在公民参与中的应用，通过对政客和公务员的访谈发现，他们在考虑采用NLP工具时关注点不同，且未明确谁应倡导采用NLP工具以增强公民参与或应对不良采用的意外后果，这可能导致政府对NLP工具采用率低。"
        },
        "tokens": 941
    },
    {
        "title": "DiffLight: A Partial Rewards Conditioned Diffusion Model for Traffic Signal Control with Missing Data",
        "link": "https://arxiv.org/abs/2410.22938",
        "description": "arXiv:2410.22938v1 Announce Type: new \nAbstract: The application of reinforcement learning in traffic signal control (TSC) has been extensively researched and yielded notable achievements. However, most existing works for TSC assume that traffic data from all surrounding intersections is fully and continuously available through sensors. In real-world applications, this assumption often fails due to sensor malfunctions or data loss, making TSC with missing data a critical challenge. To meet the needs of practical applications, we introduce DiffLight, a novel conditional diffusion model for TSC under data-missing scenarios in the offline setting. Specifically, we integrate two essential sub-tasks, i.e., traffic data imputation and decision-making, by leveraging a Partial Rewards Conditioned Diffusion (PRCD) model to prevent missing rewards from interfering with the learning process. Meanwhile, to effectively capture the spatial-temporal dependencies among intersections, we design a Spatial-Temporal transFormer (STFormer) architecture. In addition, we propose a Diffusion Communication Mechanism (DCM) to promote better communication and control performance under data-missing scenarios. Extensive experiments on five datasets with various data-missing scenarios demonstrate that DiffLight is an effective controller to address TSC with missing data. The code of DiffLight is released at https://github.com/lokol5579/DiffLight-release.",
        "published": "2024-10-31 04:00:00",
        "id": "3c259aa8-dcd1-4700-a246-a60906c5cbf3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "DiffLight是一种用于离线设置下数据缺失场景下交通信号控制的条件扩散模型，集成交通数据插补和决策两个子任务，设计了STFormer架构并提出DCM机制，实验证明其对解决缺失数据的交通信号控制有效且代码已发布。"
        },
        "tokens": 881
    },
    {
        "title": "AdaptiveISP: Learning an Adaptive Image Signal Processor for Object Detection",
        "link": "https://arxiv.org/abs/2410.22939",
        "description": "arXiv:2410.22939v1 Announce Type: new \nAbstract: Image Signal Processors (ISPs) convert raw sensor signals into digital images, which significantly influence the image quality and the performance of downstream computer vision tasks. Designing ISP pipeline and tuning ISP parameters are two key steps for building an imaging and vision system. To find optimal ISP configurations, recent works use deep neural networks as a proxy to search for ISP parameters or ISP pipelines. However, these methods are primarily designed to maximize the image quality, which are sub-optimal in the performance of high-level computer vision tasks such as detection, recognition, and tracking. Moreover, after training, the learned ISP pipelines are mostly fixed at the inference time, whose performance degrades in dynamic scenes. To jointly optimize ISP structures and parameters, we propose AdaptiveISP, a task-driven and scene-adaptive ISP. One key observation is that for the majority of input images, only a few processing modules are needed to improve the performance of downstream recognition tasks, and only a few inputs require more processing. Based on this, AdaptiveISP utilizes deep reinforcement learning to automatically generate an optimal ISP pipeline and the associated ISP parameters to maximize the detection performance. Experimental results show that AdaptiveISP not only surpasses the prior state-of-the-art methods for object detection but also dynamically manages the trade-off between detection performance and computational cost, especially suitable for scenes with large dynamic range variations. Project website: https://openimaginglab.github.io/AdaptiveISP/.",
        "published": "2024-10-31 04:00:00",
        "id": "081798ad-6323-42fd-bc5d-d1d488142578",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "AdaptiveISP通过深度强化学习自动生成最优ISP管道和相关参数以最大化检测性能，在目标检测方面表现优于现有方法且能动态管理检测性能和计算成本的权衡。"
        },
        "tokens": 885
    },
    {
        "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
        "link": "https://arxiv.org/abs/2410.22944",
        "description": "arXiv:2410.22944v1 Announce Type: new \nAbstract: Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to condition their responses by focusing on specific features whilst ignoring others, leading to different behaviours based on what features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time: for instance, robustness can be improved by focusing on task-causal features and ignoring spurious features, and social bias can be mitigated by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, and thereby facilitating more robust, fair, and controllable LLM applications in real-world environments.",
        "published": "2024-10-31 04:00:00",
        "id": "3810a05f-4362-40dd-9869-f588d22877a5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Focus Instruction Tuning (FIT)，可训练大型语言模型 (LLM) 通过聚焦特定特征并忽略其他特征来调整响应，实验表明其能改善鲁棒性、减轻社会偏见等，可在新环境下引导行为并实现推广应用。"
        },
        "tokens": 821
    },
    {
        "title": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation",
        "link": "https://arxiv.org/abs/2410.22952",
        "description": "arXiv:2410.22952v1 Announce Type: new \nAbstract: A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViTs) involves adapting the model to downstream tasks by learning a low-rank adaptation matrix. This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter. However, these low-rank strategies typically employ a fixed bottleneck dimensionality, which limits their flexibility in handling layer-wise variations. To address this limitation, we propose a novel PEFT approach inspired by Singular Value Decomposition (SVD) for representing the adaptation matrix. SVD decomposes a matrix into the product of a left unitary matrix, a diagonal matrix of scaling values, and a right unitary matrix. We utilize Householder transformations to construct orthogonal matrices that efficiently mimic the unitary matrices, requiring only a vector. The diagonal values are learned in a layer-wise manner, allowing them to flexibly capture the unique properties of each layer. This approach enables the generation of adaptation matrices with varying ranks across different layers, providing greater flexibility in adapting pre-trained models. Experiments on standard downstream vision tasks demonstrate that our method achieves promising fine-tuning performance.",
        "published": "2024-10-31 04:00:00",
        "id": "585e17ae-3957-43a6-ad8b-f1cc75726afb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于奇异值分解（SVD）的预训练视觉Transformer（ViT）参数高效微调（PEFT）新方法，用Householder变换构建正交矩阵，对角值分层学习，实验表明该方法微调性能良好。"
        },
        "tokens": 867
    },
    {
        "title": "ELBOing Stein: Variational Bayes with Stein Mixture Inference",
        "link": "https://arxiv.org/abs/2410.22948",
        "description": "arXiv:2410.22948v1 Announce Type: new \nAbstract: Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs approximate Bayesian inference by representing the posterior with a set of particles. However, SVGD suffers from variance collapse, i.e. poor predictions due to underestimating uncertainty [Ba et al., 2021], even for moderately-dimensional models such as small Bayesian neural networks (BNNs). To address this issue, we generalize SVGD by letting each particle parameterize a component distribution in a mixture model. Our method, Stein Mixture Inference (SMI), optimizes a lower bound to the evidence (ELBO) and introduces user-specified guides parameterized by particles. SMI extends the Nonlinear SVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI effectively avoids variance collapse, judging by a previously described test developed for this purpose, and performs well on standard data sets. In addition, SMI requires considerably fewer particles than SVGD to accurately estimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO optimization and user-specified guides establishes a promising approach towards variational Bayesian inference in the case of tall and wide data.",
        "published": "2024-10-31 04:00:00",
        "id": "4b999f3e-eae8-4c2c-947b-90684cd7dd61",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "为解决SVGD存在的方差崩溃问题，提出Stein Mixture Inference (SMI)方法，优化证据下限并引入用户指定的引导，在小BNN的不确定性估计上效果良好且所需粒子更少。"
        },
        "tokens": 858
    },
    {
        "title": "MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering",
        "link": "https://arxiv.org/abs/2410.22949",
        "description": "arXiv:2410.22949v1 Announce Type: new \nAbstract: Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein delta network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.",
        "published": "2024-10-31 04:00:00",
        "id": "4bf96743-df74-4590-b04b-4d2279a1be8f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出MutaPLM这一统一框架用于解释和引导蛋白质突变，介绍其相关组件、构建的数据集MutaDescribe，经实验证明它在解释突变效应等方面表现出色且代码、模型和数据已开源。"
        },
        "tokens": 840
    },
    {
        "title": "SpiroActive: Active Learning for Efficient Data Acquisition for Spirometry",
        "link": "https://arxiv.org/abs/2410.22950",
        "description": "arXiv:2410.22950v1 Announce Type: new \nAbstract: Respiratory illnesses are a significant global health burden. Respiratory illnesses, primarily Chronic obstructive pulmonary disease (COPD), is the seventh leading cause of poor health worldwide and the third leading cause of death worldwide, causing 3.23 million deaths in 2019, necessitating early identification and diagnosis for effective mitigation. Among the diagnostic tools employed, spirometry plays a crucial role in detecting respiratory abnormalities. However, conventional clinical spirometry methods often entail considerable costs and practical limitations like the need for specialized equipment, trained personnel, and a dedicated clinical setting, making them less accessible. To address these challenges, wearable spirometry technologies have emerged as promising alternatives, offering accurate, cost-effective, and convenient solutions. The development of machine learning models for wearable spirometry heavily relies on the availability of high-quality ground truth spirometry data, which is a laborious and expensive endeavor. In this research, we propose using active learning, a sub-field of machine learning, to mitigate the challenges associated with data collection and labeling. By strategically selecting samples from the ground truth spirometer, we can mitigate the need for resource-intensive data collection. We present evidence that models trained on small subsets obtained through active learning achieve comparable/better results than models trained on the complete dataset.",
        "published": "2024-10-31 04:00:00",
        "id": "665322ca-a468-46fa-9b24-a90ea7b36efa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "呼吸疾病是全球健康负担，其中慢性阻塞性肺病较严重，肺活量测定法在呼吸疾病检测中重要但传统临床方法有局限，可穿戴肺活量测定技术是有前景的替代，而机器学习模型构建依赖高质量数据，本研究提出用主动学习减轻数据收集和标记挑战且有证据表明主动学习小样本训练的模型效果可比或优于全数据集训练模型。"
        },
        "tokens": 894
    },
    {
        "title": "KALAM: toolKit for Automating high-Level synthesis of Analog computing systeMs",
        "link": "https://arxiv.org/abs/2410.22946",
        "description": "arXiv:2410.22946v1 Announce Type: new \nAbstract: Diverse computing paradigms have emerged to meet the growing needs for intelligent energy-efficient systems. The Margin Propagation (MP) framework, being one such initiative in the analog computing domain, stands out due to its scalability across biasing conditions, temperatures, and diminishing process technology nodes. However, the lack of digital-like automation tools for designing analog systems (including that of MP analog) hinders their adoption for designing large systems. The inherent scalability and modularity of MP systems present a unique opportunity in this regard. This paper introduces KALAM (toolKit for Automating high-Level synthesis of Analog computing systeMs), which leverages factor graphs as the foundational paradigm for synthesizing MP-based analog computing systems. Factor graphs are the basis of various signal processing tasks and, when coupled with MP, can be used to design scalable and energy-efficient analog signal processors. Using Python scripting language, the KALAM automation flow translates an input factor graph to its equivalent SPICE-compatible circuit netlist that can be used to validate the intended functionality. KALAM also allows the integration of design optimization strategies such as precision tuning, variable elimination, and mathematical simplification. We demonstrate KALAM's versatility for tasks such as Bayesian inference, Low-Density Parity Check (LDPC) decoding, and Artificial Neural Networks (ANN). Simulation results of the netlists align closely with software implementations, affirming the efficacy of our proposed automation tool.",
        "published": "2024-10-31 04:00:00",
        "id": "c213cb10-05ef-4853-bc0b-d0313bbaca3e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍KALAM工具包用于自动化模拟计算系统高级合成，以因子图为基础范式合成基于MP的模拟计算系统，用Python将输入因子图转换为SPICE兼容电路网表，可集成设计优化策略，在贝叶斯推理、LDPC解码和神经网络等任务展示通用性，模拟结果与软件实现相近。"
        },
        "tokens": 924
    },
    {
        "title": "ISAC Prototype System for Multi-Domain Cooperative Communication Networks",
        "link": "https://arxiv.org/abs/2410.22956",
        "description": "arXiv:2410.22956v1 Announce Type: new \nAbstract: Future wireless networks are poised to transform into integrated sensing and communication (ISAC) networks, unlocking groundbreaking services such as digital twinning. To harness the full potential of ISAC networks, it is essential to experimentally validate their sensing capabilities and the role of sensing in boosting communication. However, current prototype systems fall short in supporting multiple sensing functions or validating sensing-assisted communication. In response, we have developed an advanced ISAC prototype system that incorporates monostatic, bistatic, and network sensing modes. This system supports multimodal data collection and synchronization, ensuring comprehensive experimental validation. On the communication front, it excels in sensing-aided beam tracking and real-time high-definition video transmission. For sensing applications, it provides precise angle and range measurements, real-time angle-range imaging, and radio-based simultaneous localization and mapping (SLAM). Our prototype aligns with the 5G New Radio standard, offering scalability for up to 16 user equipments (UEs) in uplink transmission and 10 UEs in downlink transmission. Real-world tests showcase the system's superior accuracy, with root mean square errors of 2.3 degrees for angle estimation and 0.3 meters (m) for range estimation. Additionally, the estimation errors for multimodal-aided real-time radio SLAM localization and mapping are 0.25 m and 0.8 m, respectively.",
        "published": "2024-10-31 04:00:00",
        "id": "ccbb433e-60ac-48dc-8714-ed1db4b93528",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "开发了一个先进的ISAC原型系统，具备多种传感模式，在通信和传感应用方面有诸多功能，符合5G新空口标准，经实际测试展示出较高的精度。"
        },
        "tokens": 880
    },
    {
        "title": "EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models",
        "link": "https://arxiv.org/abs/2410.22959",
        "description": "arXiv:2410.22959v1 Announce Type: new \nAbstract: Image restoration has experienced significant advancements due to the development of deep learning. Nevertheless, it encounters challenges related to ill-posed problems, resulting in deviations between single model predictions and ground-truths. Ensemble learning, as a powerful machine learning technique, aims to address these deviations by combining the predictions of multiple base models. Most existing works adopt ensemble learning during the design of restoration models, while only limited research focuses on the inference-stage ensemble of pre-trained restoration models. Regression-based methods fail to enable efficient inference, leading researchers in academia and industry to prefer averaging as their choice for post-training ensemble. To address this, we reformulate the ensemble problem of image restoration into Gaussian mixture models (GMMs) and employ an expectation maximization (EM)-based algorithm to estimate ensemble weights for aggregating prediction candidates. We estimate the range-wise ensemble weights on a reference set and store them in a lookup table (LUT) for efficient ensemble inference on the test set. Our algorithm is model-agnostic and training-free, allowing seamless integration and enhancement of various pre-trained image restoration models. It consistently outperforms regression based methods and averaging ensemble approaches on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining. The codes and all estimated weights have been released in Github.",
        "published": "2024-10-31 04:00:00",
        "id": "fe5bb4ee-5535-4aa2-b435-96de7533768e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为EnsIR的图像恢复集成算法，将图像恢复的集成问题重新表述为高斯混合模型，采用基于期望最大化算法估计集成权重，在多个图像恢复任务的基准测试中表现优于其他方法，算法具有模型无关和无需训练的特点，代码和权重已在Github发布。"
        },
        "tokens": 888
    },
    {
        "title": "A Study of Secure Algorithms for Vertical Federated Learning: Take Secure Logistic Regression as an Example",
        "link": "https://arxiv.org/abs/2410.22960",
        "description": "arXiv:2410.22960v1 Announce Type: new \nAbstract: After entering the era of big data, more and more companies build services with machine learning techniques. However, it is costly for companies to collect data and extract helpful handcraft features on their own. Although it is a way to combine with other companies' data for boosting the model's performance, this approach may be prohibited by laws. In other words, finding the balance between sharing data with others and keeping data from privacy leakage is a crucial topic worthy of close attention. This paper focuses on distributed data and conducts secure model training tasks on a vertical federated learning scheme. Here, secure implies that the whole process is executed in the encrypted domain. Therefore, the privacy concern is released.",
        "published": "2024-10-31 04:00:00",
        "id": "2d82b107-a0ca-4a79-90fe-e00c7f688342",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "在大数据时代，公司收集数据和提取特征成本高，与其他公司数据结合虽可提升模型性能但可能受法律限制，本文关注分布式数据，以安全逻辑回归为例进行垂直联邦学习中的安全模型训练任务，在加密域执行以解决隐私问题。"
        },
        "tokens": 762
    },
    {
        "title": "Even the \"Devil\" has Rights!",
        "link": "https://arxiv.org/abs/2410.22963",
        "description": "arXiv:2410.22963v1 Announce Type: new \nAbstract: There have been works discussing the adoption of a human rights framework for responsible AI, emphasizing various rights such as the right to contribute to scientific advancements. Yet, to the best of our knowledge, this is the first attempt to take this framework with special focus on computer vision and documenting human rights violations in its community. This work summarizes such incidents accompanied with evidence from the lens of a female African Muslim Hijabi researcher. While previous works resorted to qualitative surveys that gather opinions from various researchers in the field, this work argues that a single documented violation is sufficient to warrant attention regardless of the stature of this researcher. Incidents documented in this work include silence on Genocides that are occurring while promoting the governments contributing to it, a broken reviewing system and corruption in the faculty support systems. This work discusses that demonizing individuals for discrimination based on gender, ethnicity, creed or reprisal has been a successful tool for exclusion with documented evidence from a single case. We argue that human rights are guaranteed for every single individual even the ones that might be labelled as devils in the community for whichever reasons to dismantle such a tool from its roots.",
        "published": "2024-10-31 04:00:00",
        "id": "67124def-df7f-4d66-ac6f-dae5f458b516",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文 arXiv:2410.22963v1是首次从女性非洲穆斯林头巾研究者的角度记录计算机视觉社区人权侵犯事件，包括对种族灭绝沉默、评审系统和教师支持系统腐败等，认为人权应保障每个人，包括被视为‘恶魔’的人。"
        },
        "tokens": 847
    },
    {
        "title": "Dynamic Threshold-based Two-layer Online Unsupervised Anomaly Detector",
        "link": "https://arxiv.org/abs/2410.22967",
        "description": "arXiv:2410.22967v1 Announce Type: new \nAbstract: The proliferation of the Internet of Things (IoT) has heightened the vulnerability to cyber threats, making it imperative to develop Anomaly Detection Systems (ADSs) capable of adapting to emerging or novel attacks. Prior research has predominantly concentrated on offline unsupervised learning techniques to protect ADSs, which are impractical for real-world applications. Furthermore, these studies often rely heavily on the assumption of known legitimate behaviors and fall short of meeting the interpretability requirements in security contexts, thereby hindering their practical adoption. In response, this paper introduces Adaptive NAD, a comprehensive framework aimed at enhancing and interpreting online unsupervised anomaly detection within security domains. We propose an interpretable two-layer anomaly detection approach that generates dependable, high-confidence pseudo-labels. Subsequently, we incorporate an online learning mechanism that updates Adaptive NAD using an innovative threshold adjustment method to accommodate new threats. Experimental findings reveal that Adaptive NAD surpasses state-of-the-art solutions by achieving improvements of over 5.4% and 23.0% in SPAUC on the CIC-Darknet2020 and CIC-DoHBrw-2020 datasets, respectively. The code for Adaptive NAD is publicly available at https://github.com/MyLearnCodeSpace/Adaptive-NAD.",
        "published": "2024-10-31 04:00:00",
        "id": "6ffabb8f-234f-4eb4-a1a7-f89078962067",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出Adaptive NAD框架，采用可解释的两层异常检测方法生成伪标签并结合在线学习机制更新，在特定数据集上SPAUC表现优于现有方案，代码已公开。"
        },
        "tokens": 853
    },
    {
        "title": "Private Synthetic Text Generation with Diffusion Models",
        "link": "https://arxiv.org/abs/2410.22971",
        "description": "arXiv:2410.22971v1 Announce Type: new \nAbstract: How capable are diffusion models of generating synthetics texts? Recent research shows their strengths, with performance reaching that of auto-regressive LLMs. But are they also good in generating synthetic data if the training was under differential privacy? Here the evidence is missing, yet the promises from private image generation look strong. In this paper we address this open question by extensive experiments. At the same time, we critically assess (and reimplement) previous works on synthetic private text generation with LLMs and reveal some unmet assumptions that might have led to violating the differential privacy guarantees. Our results partly contradict previous non-private findings and show that fully open-source LLMs outperform diffusion models in the privacy regime. Our complete source codes, datasets, and experimental setup is publicly available to foster future research.",
        "published": "2024-10-31 04:00:00",
        "id": "29bf511f-444c-402f-af74-5c18d173809e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过大量实验探讨扩散模型在差分隐私下生成合成文本的能力，重新评估前人在使用大语言模型生成私有合成文本方面的工作，结果显示开源大语言模型在隐私机制下优于扩散模型，相关资源已公开以促进研究。"
        },
        "tokens": 769
    },
    {
        "title": "Bonafide at LegalLens 2024 Shared Task: Using Lightweight DeBERTa Based Encoder For Legal Violation Detection and Resolution",
        "link": "https://arxiv.org/abs/2410.22977",
        "description": "arXiv:2410.22977v1 Announce Type: new \nAbstract: In this work, we present two systems -- Named Entity Resolution (NER) and Natural Language Inference (NLI) -- for detecting legal violations within unstructured textual data and for associating these violations with potentially affected individuals, respectively. Both these systems are lightweight DeBERTa based encoders that outperform the LLM baselines. The proposed NER system achieved an F1 score of 60.01\\% on Subtask A of the LegalLens challenge, which focuses on identifying violations. The proposed NLI system achieved an F1 score of 84.73\\% on Subtask B of the LegalLens challenge, which focuses on resolving these violations by matching them with pre-existing legal complaints of class action cases. Our NER system ranked sixth and NLI system ranked fifth on the LegalLens leaderboard. We release the trained models and inference scripts.",
        "published": "2024-10-31 04:00:00",
        "id": "1847bb7a-bc68-4a91-918e-17a192af992a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文介绍了基于轻量级DeBERTa编码器的命名实体识别（NER）和自然语言推理（NLI）两个系统，用于检测法律违规行为并关联受影响个体，在LegalLens挑战中NER系统F1得分为60.01%，NLI系统为84.73%，分别排名第六和第五，且将发布训练模型和推理脚本。"
        },
        "tokens": 849
    },
    {
        "title": "Higher-order Cross-structural Embedding Model for Time Series Analysis",
        "link": "https://arxiv.org/abs/2410.22984",
        "description": "arXiv:2410.22984v1 Announce Type: new \nAbstract: Time series analysis has gained significant attention due to its critical applications in diverse fields such as healthcare, finance, and sensor networks. The complexity and non-stationarity of time series make it challenging to capture the interaction patterns across different timestamps. Current approaches struggle to model higher-order interactions within time series, and focus on learning temporal or spatial dependencies separately, which limits performance in downstream tasks. To address these gaps, we propose Higher-order Cross-structural Embedding Model for Time Series (High-TS), a novel framework that jointly models both temporal and spatial perspectives by combining multiscale Transformer with Topological Deep Learning (TDL). Meanwhile, High-TS utilizes contrastive learning to integrate these two structures for generating robust and discriminative representations. Extensive experiments show that High-TS outperforms state-of-the-art methods in various time series tasks and demonstrate the importance of higher-order cross-structural information in improving model performance.",
        "published": "2024-10-31 04:00:00",
        "id": "54e1ad50-cefc-4f20-87f6-8f14e997f0e1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种高阶跨结构嵌入模型High - TS用于时间序列分析，结合多尺度变换器与拓扑深度学习联合建模时空视角，利用对比学习生成表征，实验表明其在多项任务中优于现有方法。"
        },
        "tokens": 791
    },
    {
        "title": "Troubling Taxonomies in GenAI Evaluation",
        "link": "https://arxiv.org/abs/2410.22985",
        "description": "arXiv:2410.22985v1 Announce Type: new \nAbstract: To evaluate the societal impacts of GenAI requires a model of how social harms emerge from interactions between GenAI, people, and societal structures. Yet a model is rarely explicitly defined in societal impact evaluations, or in the taxonomies of societal impacts that support them. In this provocation, we argue that societal impacts should be conceptualised as application- and context-specific, incommensurable, and shaped by questions of social power. Doing so leads us to conclude that societal impact evaluations using existing taxonomies are inherently limited, in terms of their potential to reveal how GenAI systems may interact with people when introduced into specific social contexts. We therefore propose a governance-first approach to managing societal harms attended by GenAI technologies.",
        "published": "2024-10-31 04:00:00",
        "id": "3728fdfd-88de-46c9-abb2-b7995f69637d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文指出评估生成式人工智能（GenAI）的社会影响需要明确模型，但现有社会影响评估及分类法很少做到，认为社会影响应被概念化为特定于应用和情境、不可通约且受社会权力问题影响，现有分类法有固有局限，提议治理优先的方法来管理GenAI技术带来的社会危害。"
        },
        "tokens": 773
    },
    {
        "title": "PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster Search and Rescue",
        "link": "https://arxiv.org/abs/2410.22982",
        "description": "arXiv:2410.22982v1 Announce Type: new \nAbstract: This paper introduces a comprehensive framework for Post-Disaster Search and Rescue (PDSR), aiming to optimize search and rescue operations leveraging Unmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision and availability of sensing capabilities, particularly in various catastrophic scenarios. Central to this concept is the rapid deployment of UAV swarms equipped with diverse sensing, communication, and intelligence capabilities, functioning as an integrated system that incorporates multiple technologies and approaches for efficient detection of individuals buried beneath rubble or debris following a disaster. Within this framework, we propose architectural solution and address associated challenges to ensure optimal performance in real-world disaster scenarios. The proposed framework aims to achieve complete coverage of damaged areas significantly faster than traditional methods using a multi-tier swarm architecture. Furthermore, integrating multi-modal sensing data with machine learning for data fusion could enhance detection accuracy, ensuring precise identification of survivors.",
        "published": "2024-10-31 04:00:00",
        "id": "ef019e57-ed91-4dbf-b784-dab89249c9f3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一个用于灾后搜索和救援的综合框架，旨在利用无人机优化搜索救援行动，采用多层群架构实现快速覆盖受灾区域，还可通过多模态传感数据与机器学习的数据融合提高检测幸存者的准确性。"
        },
        "tokens": 789
    },
    {
        "title": "Dual-Optimized Adaptive Graph Reconstruction for Multi-View Graph Clustering",
        "link": "https://arxiv.org/abs/2410.22983",
        "description": "arXiv:2410.22983v1 Announce Type: new \nAbstract: Multi-view clustering is an important machine learning task for multi-media data, encompassing various domains such as images, videos, and texts. Moreover, with the growing abundance of graph data, the significance of multi-view graph clustering (MVGC) has become evident. Most existing methods focus on graph neural networks (GNNs) to extract information from both graph structure and feature data to learn distinguishable node representations. However, traditional GNNs are designed with the assumption of homophilous graphs, making them unsuitable for widely prevalent heterophilous graphs. Several techniques have been introduced to enhance GNNs for heterophilous graphs. While these methods partially mitigate the heterophilous graph issue, they often neglect the advantages of traditional GNNs, such as their simplicity, interpretability, and efficiency. In this paper, we propose a novel multi-view graph clustering method based on dual-optimized adaptive graph reconstruction, named DOAGC. It mainly aims to reconstruct the graph structure adapted to traditional GNNs to deal with heterophilous graph issues while maintaining the advantages of traditional GNNs. Specifically, we first develop an adaptive graph reconstruction mechanism that accounts for node correlation and original structural information. To further optimize the reconstruction graph, we design a dual optimization strategy and demonstrate the feasibility of our optimization strategy through mutual information theory. Numerous experiments demonstrate that DOAGC effectively mitigates the heterophilous graph problem.",
        "published": "2024-10-31 04:00:00",
        "id": "884a3aa5-42b6-4162-a499-cbb74c158593",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于双优化自适应图重构的多视图图聚类方法DOAGC，旨在解决异质图问题并保持传统图神经网络优势。"
        },
        "tokens": 880
    },
    {
        "title": "LumiSculpt: A Consistency Lighting Control Network for Video Generation",
        "link": "https://arxiv.org/abs/2410.22979",
        "description": "arXiv:2410.22979v1 Announce Type: new \nAbstract: Lighting plays a pivotal role in ensuring the naturalness of video generation, significantly influencing the aesthetic quality of the generated content. However, due to the deep coupling between lighting and the temporal features of videos, it remains challenging to disentangle and model independent and coherent lighting attributes, limiting the ability to control lighting in video generation. In this paper, inspired by the established controllable T2I models, we propose LumiSculpt, which, for the first time, enables precise and consistent lighting control in T2V generation models.LumiSculpt equips the video generation with strong interactive capabilities, allowing the input of custom lighting reference image sequences. Furthermore, the core learnable plug-and-play module of LumiSculpt facilitates remarkable control over lighting intensity, position, and trajectory in latent video diffusion models based on the advanced DiT backbone.Additionally, to effectively train LumiSculpt and address the issue of insufficient lighting data, we construct LumiHuman, a new lightweight and flexible dataset for portrait lighting of images and videos. Experimental results demonstrate that LumiSculpt achieves precise and high-quality lighting control in video generation.",
        "published": "2024-10-31 04:00:00",
        "id": "0076edef-961d-4368-8659-5d94c25a295d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "LumiSculpt可实现T2V生成模型中精确一致的光照控制，其核心模块可控制潜视频扩散模型中的光照属性，构建的LumiHuman数据集可用于有效训练，实验表明该模型可在视频生成中实现高质量光照控制。"
        },
        "tokens": 853
    },
    {
        "title": "DisenTS: Disentangled Channel Evolving Pattern Modeling for Multivariate Time Series Forecasting",
        "link": "https://arxiv.org/abs/2410.22981",
        "description": "arXiv:2410.22981v1 Announce Type: new \nAbstract: Multivariate time series forecasting plays a crucial role in various real-world applications. Significant efforts have been made to integrate advanced network architectures and training strategies that enhance the capture of temporal dependencies, thereby improving forecasting accuracy. On the other hand, mainstream approaches typically utilize a single unified model with simplistic channel-mixing embedding or cross-channel attention operations to account for the critical intricate inter-channel dependencies. Moreover, some methods even trade capacity for robust prediction based on the channel-independent assumption. Nonetheless, as time series data may display distinct evolving patterns due to the unique characteristics of each channel (including multiple strong seasonalities and trend changes), the unified modeling methods could yield suboptimal results. To this end, we propose DisenTS, a tailored framework for modeling disentangled channel evolving patterns in general multivariate time series forecasting. The central idea of DisenTS is to model the potential diverse patterns within the multivariate time series data in a decoupled manner. Technically, the framework employs multiple distinct forecasting models, each tasked with uncovering a unique evolving pattern. To guide the learning process without supervision of pattern partition, we introduce a novel Forecaster Aware Gate (FAG) module that generates the routing signals adaptively according to both the forecasters' states and input series' characteristics. The forecasters' states are derived from the Linear Weight Approximation (LWA) strategy, which quantizes the complex deep neural networks into compact matrices. Additionally, the Similarity Constraint (SC) is further proposed to guide each model to specialize in an underlying pattern by minimizing the mutual information between the representations.",
        "published": "2024-10-31 04:00:00",
        "id": "01547212-01b1-44a0-87be-c13470576901",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "DisenTS是一种用于多变量时间序列预测的框架，旨在以解耦方式对潜在的多样化模式进行建模，采用多个预测模型，通过FAG模块引导学习过程，并提出相似性约束以指导模型专门处理潜在模式。"
        },
        "tokens": 936
    },
    {
        "title": "V2X-Assisted Distributed Computing and Control Framework for Connected and Automated Vehicles under Ramp Merging Scenario",
        "link": "https://arxiv.org/abs/2410.22987",
        "description": "arXiv:2410.22987v1 Announce Type: new \nAbstract: This paper investigates distributed computing and cooperative control of connected and automated vehicles (CAVs) in ramp merging scenario under transportation cyber-physical system. Firstly, a centralized cooperative trajectory planning problem is formulated subject to the safely constraints and traffic performance in ramp merging scenario, where the trajectories of all vehicles are jointly optimized. To get rid of the reliance on a central controller and reduce computation time, a distributed solution to this problem implemented among CAVs through Vehicles-to-Everything (V2X) communication is proposed. Unlike existing method, our method can distribute the computational task among CAVs and carry out parallel solving through V2X communication. Then, a multi-vehicles model predictive control (MPC) problem aimed at maximizing system stability and minimizing control input is formulated based on the solution of the first problem subject to strict safety constants and input limits. Due to these complex constraints, this problem becomes high-dimensional, centralized, and non-convex. To solve it in a short time, a decomposition and convex reformulation method, namely distributed cooperative iterative model predictive control (DCIMPC), is proposed. This method leverages the communication capability of CAVs to decompose the problem, making full use of the computational resources on vehicles to achieve fast solutions and distributed control. The two above problems with their corresponding solving methods form the systemic framework of the V2X assisted distributed computing and control. Simulations have been conducted to evaluate the framework's convergence, safety, and solving speed. Additionally, extra experiments are conducted to validate the performance of DCIMPC. The results show that our method can greatly improve computation speed without sacrificing system performance.",
        "published": "2024-10-31 04:00:00",
        "id": "9cdcdcff-e67e-4054-9fb0-9dc2a4015fdf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出一种V2X辅助的分布式计算和控制框架，用于解决坡道合并场景下网联自动驾驶车辆（CAVs）的分布式计算和协同控制问题，包括提出分布式解决方案和分解凸重构方法，并通过仿真和实验评估框架性能。"
        },
        "tokens": 949
    },
    {
        "title": "Adaptive finite elements for obstacle problems",
        "link": "https://arxiv.org/abs/2410.22991",
        "description": "arXiv:2410.22991v1 Announce Type: new \nAbstract: We summarise three applications of the obstacle problem to membrane contact, elastoplastic torsion and cavitation modelling, and show how the resulting models can be solved using mixed finite elements. It is challenging to construct fixed computational meshes for any inequality-constrained problem because the coincidence set has an unknown shape. Consequently, we demonstrate how $h$-adaptivity can be used to resolve the unknown coincidence set. We demonstrate some practical challenges that must be overcome in the application of the adaptive method.",
        "published": "2024-10-31 04:00:00",
        "id": "a158c704-1e67-4f6a-80ec-a4ecc82f5061",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "总结了障碍问题在膜接触、弹塑性扭转和空化建模中的三个应用，展示如何用混合有限元解决相关模型，阐述h - 适应性可用于解决未知重合集及应用自适应方法时需克服的挑战。"
        },
        "tokens": 704
    },
    {
        "title": "Dynamic Matching with Post-allocation Service and its Application to Refugee Resettlement",
        "link": "https://arxiv.org/abs/2410.22992",
        "description": "arXiv:2410.22992v1 Announce Type: new \nAbstract: Motivated by our collaboration with a major refugee resettlement agency in the U.S., we study a dynamic matching problem where each new arrival (a refugee case) must be matched immediately and irrevocably to one of the static resources (a location with a fixed annual quota). In addition to consuming the static resource, each case requires post-allocation service from a server, such as a translator. Given the time-consuming nature of service, a server may not be available at a given time, thus we refer to it as a dynamic resource. Upon matching, the case will wait to avail service in a first-come-first-serve manner. Bursty matching to a location may result in undesirable congestion at its corresponding server. Consequently, the central planner (the agency) faces a dynamic matching problem with an objective that combines the matching reward (captured by pair-specific employment outcomes) with the cost for congestion for dynamic resources and over-allocation for the static ones. Motivated by the observed fluctuations in the composition of refugee pools across the years, we design algorithms that do not rely on distributional knowledge constructed based on past years' data. To that end, we develop learning-based algorithms that are asymptotically optimal in certain regimes, easy to interpret, and computationally fast. Our design is based on learning the dual variables of the underlying optimization problem; however, the main challenge lies in the time-varying nature of the dual variables associated with dynamic resources. To overcome this challenge, our theoretical development brings together techniques from Lyapunov analysis, adversarial online learning, and stochastic optimization. On the application side, when tested on real data from our partner agency, our method outperforms existing ones making it a viable candidate for replacing the current practice upon experimentation.",
        "published": "2024-10-31 04:00:00",
        "id": "ec7c48db-f63d-4c83-adbe-46a1229bfa07",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "受与美国一家大型难民安置机构合作的启发，研究动态匹配问题，设计不依赖过往数据分布知识的算法，经测试该方法优于现有方法。"
        },
        "tokens": 946
    },
    {
        "title": "VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning",
        "link": "https://arxiv.org/abs/2410.22995",
        "description": "arXiv:2410.22995v1 Announce Type: new \nAbstract: Although previous research on large language models (LLMs) and large multi-modal models (LMMs) has systematically explored mathematical problem-solving (MPS) within visual contexts, the analysis of how these models process visual information during problem-solving remains insufficient. To address this gap, we present VisAidMath, a benchmark for evaluating the MPS process related to visual information. We follow a rigorous data curation pipeline involving both automated processes and manual annotations to ensure data quality and reliability. Consequently, this benchmark includes 1,200 challenging problems from various mathematical branches, vision-aid formulations, and difficulty levels, collected from diverse sources such as textbooks, examination papers, and Olympiad problems. Based on the proposed benchmark, we conduct comprehensive evaluations on ten mainstream LLMs and LMMs, highlighting deficiencies in the visual-aided reasoning process. For example, GPT-4V only achieves 45.33% accuracy in the visual-aided reasoning task, even with a drop of 2 points when provided with golden visual aids. In-depth analysis reveals that the main cause of deficiencies lies in hallucination regarding the implicit visual reasoning process, shedding light on future research directions in the visual-aided MPS process.",
        "published": "2024-10-31 04:00:00",
        "id": "d9ac73d2-c001-44f1-a3eb-33eb0f75c7ae",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出VisAidMath基准用于评估与视觉信息相关的数学问题解决过程，通过严格数据管理流程构建包含1200个难题的基准，对10个主流大语言和多模态模型进行评估，指出视觉辅助推理过程中的不足及原因。"
        },
        "tokens": 866
    },
    {
        "title": "Semantic Enrichment of the Quantum Cascade Laser Properties in Text- A Knowledge Graph Generation Approach",
        "link": "https://arxiv.org/abs/2410.22996",
        "description": "arXiv:2410.22996v1 Announce Type: new \nAbstract: A well structured collection of the various Quantum Cascade Laser (QCL) design and working properties data provides a platform to analyze and understand the relationships between these properties. By analyzing these relationships, we can gain insights into how different design features impact laser performance properties such as the working temperature. Most of these QCL properties are captured in scientific text. There is therefore need for efficient methodologies that can be utilized to extract QCL properties from text and generate a semantically enriched and interlinked platform where the properties can be analyzed to uncover hidden relations. There is also the need to maintain provenance and reference information on which these properties are based. Semantic Web technologies such as Ontologies and Knowledge Graphs have proven capability in providing interlinked data platforms for knowledge representation in various domains. In this paper, we propose an approach for generating a QCL properties Knowledge Graph (KG) from text for semantic enrichment of the properties. The approach is based on the QCL ontology and a Retrieval Augmented Generation (RAG) enabled information extraction pipeline based on GPT 4-Turbo language model. The properties of interest include: working temperature, laser design type, lasing frequency, laser optical power and the heterostructure. The experimental results demonstrate the feasibility and effectiveness of this approach for efficiently extracting QCL properties from unstructured text and generating a QCL properties Knowledge Graph, which has potential applications in semantic enrichment and analysis of QCL data.",
        "published": "2024-10-31 04:00:00",
        "id": "e2a0f03d-110d-4db6-a54e-2c084c369c72",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种从文本生成量子级联激光器（QCL）属性知识图谱（KG）的方法，以实现属性的语义丰富化，通过基于QCL本体和GPT 4 - Turbo语言模型的信息提取管道，对包括工作温度等在内的QCL属性进行提取并构建知识图谱，实验证明了该方法的可行性和有效性。"
        },
        "tokens": 935
    },
    {
        "title": "A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics",
        "link": "https://arxiv.org/abs/2410.22997",
        "description": "arXiv:2410.22997v1 Announce Type: new \nAbstract: Recent advances in LLM have been instrumental in autonomous robot control and human-robot interaction by leveraging their vast general knowledge and capabilities to understand and reason across a wide range of tasks and scenarios. Previous works have investigated various prompt engineering techniques for improving the performance of \\glspl{LLM} to accomplish tasks, while others have proposed methods that utilize LLMs to plan and execute tasks based on the available functionalities of a given robot platform. In this work, we consider both lines of research by comparing prompt engineering techniques and combinations thereof within the application of high-level task planning and execution in service robotics. We define a diverse set of tasks and a simple set of functionalities in simulation, and measure task completion accuracy and execution time for several state-of-the-art models.",
        "published": "2024-10-31 04:00:00",
        "id": "73d1e81d-ba73-421a-9db7-cab3ae21e506",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在服务机器人领域，文章通过在模拟环境中定义任务和功能，对比提示工程技术及其组合在高级任务规划和执行中的应用，衡量多个先进模型的任务完成准确率和执行时间。"
        },
        "tokens": 756
    },
    {
        "title": "Towards Constraint-aware Learning for Resource Allocation in NFV-enabled Networks",
        "link": "https://arxiv.org/abs/2410.22999",
        "description": "arXiv:2410.22999v1 Announce Type: new \nAbstract: Virtual Network Embedding (VNE) is a challenging combinatorial optimization problem that refers to resource allocation associated with hard and multifaceted constraints in network function virtualization (NFV). Existing works for VNE struggle to handle such complex constraints, leading to compromised system performance and stability. In this paper, we propose a \\textbf{CON}straint-\\textbf{A}ware \\textbf{L}earning framework for VNE, named \\textbf{CONAL}, to achieve efficient constraint management. Concretely, we formulate the VNE problem as a constrained Markov decision process with violation tolerance. This modeling approach aims to improve both resource utilization and solution feasibility by precisely evaluating solution quality and the degree of constraint violation. We also propose a reachability-guided optimization with an adaptive reachability budget method that dynamically assigns budget values. This method achieves persistent zero violation to guarantee the feasibility of VNE solutions and more stable policy optimization by handling instances without any feasible solution. Furthermore, we propose a constraint-aware graph representation method to efficiently learn cross-graph relations and constrained path connectivity in VNE. Finally, extensive experimental results demonstrate the superiority of our proposed method over state-of-the-art baselines. Our code is available at https://github.com/GeminiLight/conal-vne.",
        "published": "2024-10-31 04:00:00",
        "id": "ceda7d19-59f1-4b3d-94b0-f696fe85078f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为CONAL的约束感知学习框架用于VNE，通过多种方法实现高效约束管理，实验结果显示其优于现有基准。"
        },
        "tokens": 847
    },
    {
        "title": "\\textsc{Long$^2$RAG}: Evaluating Long-Context \\& Long-Form Retrieval-Augmented Generation with Key Point Recall",
        "link": "https://arxiv.org/abs/2410.23000",
        "description": "arXiv:2410.23000v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) is a promising approach to address the limitations of fixed knowledge in large language models (LLMs). However, current benchmarks for evaluating RAG systems suffer from two key deficiencies: (1) they fail to adequately measure LLMs' capability in handling \\emph{long-context retrieval} due to a lack of datasets that reflect the characteristics of retrieved documents, and (2) they lack a comprehensive evaluation method for assessing LLMs' ability to generate \\emph{long-form responses} that effectively exploits retrieved information. To address these shortcomings, we introduce the \\textsc{Long$^2$RAG} benchmark and the Key Point Recall (\\textit{KPR}) metric. \\textsc{Long$^2$RAG} comprises 280 questions spanning 10 domains and across 8 question categories, each associated with 5 retrieved documents with an average length of 2,444 words. \\textit{KPR} evaluates the extent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses, providing a more nuanced assessment of their ability to exploit retrieved information. Our dataset and scripts are available at https://github.com/QZH-777/longrag.",
        "published": "2024-10-31 04:00:00",
        "id": "4a9faec3-f9db-4037-8f80-23789764d99b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Long²RAG基准和关键要点召回（KPR）指标以解决当前检索增强生成（RAG）系统评估基准的两个缺陷，相关数据集和脚本可在指定网址获取。"
        },
        "tokens": 876
    },
    {
        "title": "Scoring Rules and Calibration for Imprecise Probabilities",
        "link": "https://arxiv.org/abs/2410.23001",
        "description": "arXiv:2410.23001v1 Announce Type: new \nAbstract: What does it mean to say that, for example, the probability for rain tomorrow is between 20% and 30%? The theory for the evaluation of precise probabilistic forecasts is well-developed and is grounded in the key concepts of proper scoring rules and calibration. For the case of imprecise probabilistic forecasts (sets of probabilities), such theory is still lacking. In this work, we therefore generalize proper scoring rules and calibration to the imprecise case. We develop these concepts as relative to data models and decision problems. As a consequence, the imprecision is embedded in a clear context. We establish a close link to the paradigm of (group) distributional robustness and in doing so provide new insights for it. We argue that proper scoring rules and calibration serve two distinct goals, which are aligned in the precise case, but intriguingly are not necessarily aligned in the imprecise case. The concept of decision-theoretic entropy plays a key role for both goals. Finally, we demonstrate the theoretical insights in machine learning practice, in particular we illustrate subtle pitfalls relating to the choice of loss function in distributional robustness.",
        "published": "2024-10-31 04:00:00",
        "id": "bd9fa7a1-5b86-4584-9baf-055c9de24b41",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文将精确概率预测评估中的适当评分规则和校准概念推广到不精确概率情况，将不精确性置于特定情境中，阐述了其与分布鲁棒性范式的紧密联系，并指出在不精确情况下两者目标的不同，最后展示了在机器学习实践中的理论见解。"
        },
        "tokens": 852
    },
    {
        "title": "DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes",
        "link": "https://arxiv.org/abs/2410.23004",
        "description": "arXiv:2410.23004v1 Announce Type: new \nAbstract: Grasping in cluttered scenes remains highly challenging for dexterous hands due to the scarcity of data. To address this problem, we present a large-scale synthetic benchmark, encompassing 1319 objects, 8270 scenes, and 427 million grasps. Beyond benchmarking, we also propose a novel two-stage grasping method that learns efficiently from data by using a diffusion model that conditions on local geometry. Our proposed generative method outperforms all baselines in simulation experiments. Furthermore, with the aid of test-time-depth restoration, our method demonstrates zero-shot sim-to-real transfer, attaining 90.7% real-world dexterous grasping success rate in cluttered scenes.",
        "published": "2024-10-31 04:00:00",
        "id": "caad42d7-a4f5-4681-ae54-c41983518d58",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "DexGraspNet 2.0提出大规模合成基准和两阶段抓取方法，其生成式方法在模拟实验中优于所有基线且通过测试时深度恢复实现零样本模拟到现实的迁移，在杂乱场景中的实际灵巧抓取成功率达90.7%。"
        },
        "tokens": 782
    },
    {
        "title": "Improving Musical Accompaniment Co-creation via Diffusion Transformers",
        "link": "https://arxiv.org/abs/2410.23005",
        "description": "arXiv:2410.23005v1 Announce Type: new \nAbstract: Building upon Diff-A-Riff, a latent diffusion model for musical instrument accompaniment generation, we present a series of improvements targeting quality, diversity, inference speed, and text-driven control. First, we upgrade the underlying autoencoder to a stereo-capable model with superior fidelity and replace the latent U-Net with a Diffusion Transformer. Additionally, we refine text prompting by training a cross-modality predictive network to translate text-derived CLAP embeddings to audio-derived CLAP embeddings. Finally, we improve inference speed by training the latent model using a consistency framework, achieving competitive quality with fewer denoising steps. Our model is evaluated against the original Diff-A-Riff variant using objective metrics in ablation experiments, demonstrating promising advancements in all targeted areas. Sound examples are available at: https://sonycslparis.github.io/improved_dar/.",
        "published": "2024-10-31 04:00:00",
        "id": "323809e4-a332-49c6-b365-88ffd3a8f2c4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "基于Diff - A - Riff提出一系列改进，包括升级自动编码器、替换网络结构、改进文本提示和提高推理速度，评估显示在多个目标领域有进展并给出示例网址。"
        },
        "tokens": 778
    },
    {
        "title": "SoundCollage: Automated Discovery of New Classes in Audio Datasets",
        "link": "https://arxiv.org/abs/2410.23008",
        "description": "arXiv:2410.23008v1 Announce Type: new \nAbstract: Developing new machine learning applications often requires the collection of new datasets. However, existing datasets may already contain relevant information to train models for new purposes. We propose SoundCollage: a framework to discover new classes within audio datasets by incorporating (1) an audio pre-processing pipeline to decompose different sounds in audio samples and (2) an automated model-based annotation mechanism to identify the discovered classes. Furthermore, we introduce clarity measure to assess the coherence of the discovered classes for better training new downstream applications. Our evaluations show that the accuracy of downstream audio classifiers within discovered class samples and held-out datasets improves over the baseline by up to 34.7% and 4.5%, respectively, highlighting the potential of SoundCollage in making datasets reusable by labeling with newly discovered classes. To encourage further research in this area, we open-source our code at https://github.com/nokia-bell-labs/audio-class-discovery.",
        "published": "2024-10-31 04:00:00",
        "id": "7b58b3ce-9428-408f-96d6-7e4e7833b0e6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "SoundCollage是一个在音频数据集中发现新类别的框架，包括音频预处理管道和自动注释机制，还引入清晰度衡量标准，实验显示其有助于提高下游音频分类器的准确性，代码已开源。"
        },
        "tokens": 798
    },
    {
        "title": "Proceedings Eighth Symposium on Working Formal Methods",
        "link": "https://arxiv.org/abs/2410.23020",
        "description": "arXiv:2410.23020v1 Announce Type: new \nAbstract: The Working Formal Methods Symposium (FROM) is a series of workshops that aim to bring together researchers and practitioners who work on formal methods by contributing new theoretical results, methods, techniques, and frameworks, and/or by creating or using software tools that apply theoretical contributions.",
        "published": "2024-10-31 04:00:00",
        "id": "2d83874f-8fe1-4e16-b4b1-cd979511948f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 2
            },
            "keyFacts": "第八届工作形式化方法研讨会会议论文集发布，该研讨会旨在聚集研究人员和从业者交流形式化方法相关的理论成果、方法、技术和框架，以及相关软件工具。"
        },
        "tokens": 650
    },
    {
        "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback",
        "link": "https://arxiv.org/abs/2410.23022",
        "description": "arXiv:2410.23022v1 Announce Type: new \nAbstract: Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples; or are limited to reward functions expressible by compact code, which may require source code and have difficulty capturing nuanced semantics; or require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets nor source code. We make our code available at \\url{URL} (coming soon).",
        "published": "2024-10-31 04:00:00",
        "id": "bb4c78eb-4801-4413-b5e9-16997a2f8695",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出ONI架构，通过利用大型语言模型反馈，在无需外部数据集和源代码的情况下，学习强化学习策略和内在奖励函数，解决了当前从自然语言描述合成密集奖励方法中的局限，并在NetHack学习环境中的稀疏奖励任务上达到了最先进的性能。"
        },
        "tokens": 922
    },
    {
        "title": "A Universal Sets-level Optimization Framework for Next Set Recommendation",
        "link": "https://arxiv.org/abs/2410.23023",
        "description": "arXiv:2410.23023v1 Announce Type: new \nAbstract: Next Set Recommendation (NSRec), encompassing related tasks such as next basket recommendation and temporal sets prediction, stands as a trending research topic. Although numerous attempts have been made on this topic, there are certain drawbacks: (i) Existing studies are still confined to utilizing objective functions commonly found in Next Item Recommendation (NIRec), such as binary cross entropy and BPR, which are calculated based on individual item comparisons; (ii) They place emphasis on building sophisticated learning models to capture intricate dependency relationships across sequential sets, but frequently overlook pivotal dependency in their objective functions; (iii) Diversity factor within sequential sets is frequently overlooked. In this research, we endeavor to unveil a universal and S ets-level optimization framework for N ext Set Recommendation (SNSRec), offering a holistic fusion of diversity distribution and intricate dependency relationships within temporal sets. To realize this, the following contributions are made: (i) We directly model the temporal set in a sequence as a cohesive entity, leveraging the Structured Determinantal Point Process (SDPP), wherein the probabilistic DPP distribution prioritizes collections of structures (sequential sets) instead of individual items; (ii) We introduce a co-occurrence representation to discern and acknowledge the importance of different sets; (iii) We propose a sets-level optimization criterion, which integrates the diversity distribution and dependency relations across the entire sequence of sets, guiding the model to recommend relevant and diversified set. Extensive experiments on real-world datasets show that our approach consistently outperforms previous methods on both relevance and diversity.",
        "published": "2024-10-31 04:00:00",
        "id": "f55c7298-f8d4-4710-b430-24bb07d5e107",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出了一个用于下一组推荐（NSRec）的通用集合级优化框架SNSRec，它融合了多样性分布和时间集内的复杂依赖关系，实验表明该方法在相关性和多样性上优于之前的方法。"
        },
        "tokens": 922
    },
    {
        "title": "Offline Reinforcement Learning and Sequence Modeling for Downlink Link Adaptation",
        "link": "https://arxiv.org/abs/2410.23031",
        "description": "arXiv:2410.23031v1 Announce Type: new \nAbstract: Contemporary radio access networks employ link adaption (LA) algorithms to optimize the modulation and coding schemes to adapt to the prevailing propagation conditions and are near-optimal in terms of the achieved spectral efficiency. LA is a challenging task in the presence of mobility, fast fading, and imperfect channel quality information and limited knowledge of the receiver characteristics at the transmitter, which render model-based LA algorithms complex and suboptimal. Model-based LA is especially difficult as connected user equipment devices become increasingly heterogeneous in terms of receiver capabilities, antenna configurations and hardware characteristics. Recognizing these difficulties, previous works have proposed reinforcement learning (RL) for LA, which faces deployment difficulties due to their potential negative impacts on live performance. To address this challenge, this paper considers offline RL to learn LA policies from data acquired in live networks with minimal or no intrusive effects on the network operation. We propose three LA designs based on batch-constrained deep Q-learning, conservative Q-learning, and decision transformers, showing that offline RL algorithms can achieve performance of state-of-the-art online RL methods when data is collected with a proper behavioral policy.",
        "published": "2024-10-31 04:00:00",
        "id": "2d8134b0-5d56-426d-a6a6-266d170feafb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出基于离线强化学习（Offline RL）进行下行链路自适应（Downlink Link Adaptation），介绍三种设计，指出离线RL算法在数据按适当行为策略收集时可达到在线RL方法的性能。"
        },
        "tokens": 829
    },
    {
        "title": "Camber-changing flapping hydrofoils for efficient and environmental-safe water propulsion system",
        "link": "https://arxiv.org/abs/2410.23032",
        "description": "arXiv:2410.23032v1 Announce Type: new \nAbstract: This research introduces a novel hydrofoil-based propulsion framework for unmanned aquatic robots, inspired by the undulating locomotion observed in select aquatic species. The proposed system incorporates a camber-modulating mechanism to enhance hydrofoil propulsive force generation and eventually efficiency. Through dynamic simulations, we validate the effectiveness of the camber-adjusting hydrofoil compared to a symmetric counterpart. The results demonstrate a significant improvement in horizontal thrust, emphasizing the potential of the cambering approach to enhance propulsive performance. Additionally, a prototype flipper design is presented, featuring individual control of heave and pitch motions, as well as a camber-adjustment mechanism. The integrated system not only provides efficient water-based propulsion but also offers the capacity for generating vertical forces during take-off maneuvers for seaplanes. The design is tailored to harness wave energy, contributing to the exploration of alternative energy resources. This work advances the understanding of bionic oscillatory principles for aquatic robots and provides a foundation for future developments in environmentally safe and agile underwater exploration.",
        "published": "2024-10-31 04:00:00",
        "id": "cc756ed8-ac58-482e-897d-cebb7bf79307",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出一种新型水翼推进框架，通过弯度调节机制提升推进力和效率，经模拟验证有效，还展示了原型鳍状肢设计，该系统有助于水下探索并利用波能。"
        },
        "tokens": 814
    },
    {
        "title": "Exploring the Potential of Multi-modal Sensing Framework for Forest Ecology",
        "link": "https://arxiv.org/abs/2410.23033",
        "description": "arXiv:2410.23033v1 Announce Type: new \nAbstract: Forests offer essential resources and services to humanity, yet preserving and restoring them presents challenges, particularly due to the limited availability of actionable data, especially in hard-to-reach areas like forest canopies. Accessibility continues to pose a challenge for biologists collecting data in forest environments, often requiring them to invest significant time and energy in climbing trees to place sensors. This operation not only consumes resources but also exposes them to danger. Efforts in robotics have been directed towards accessing the tree canopy using robots. A swarm of drones has showcased autonomous navigation through the canopy, maneuvering with agility and evading tree collisions, all aimed at mapping the area and collecting data. However, relying solely on free-flying drones has proven insufficient for data collection. Flying drones within the canopy generates loud noise, disturbing animals and potentially corrupting the data. Additionally, commercial drones often have limited autonomy for dexterous tasks where aerial physical interaction could be required, further complicating data acquisition efforts. Aerial deployed sensor placement methods such as bio-gliders and sensor shooting have proven effective for data collection within the lower canopy. However, these methods face challenges related to retrieving the data and sensors, often necessitating human intervention.",
        "published": "2024-10-31 04:00:00",
        "id": "3614b8f3-82b1-499a-936a-44f23da19cbe",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 3
            },
            "keyFacts": "森林为人类提供重要资源和服务，但保护和恢复森林面临数据获取挑战，在森林冠层等难以到达区域数据尤其缺乏，机器人领域虽有探索但存在不足。"
        },
        "tokens": 844
    },
    {
        "title": "Accurate Solutions to Optimal Control Problems via a Flexible Mesh and Integrated Residual Transcription",
        "link": "https://arxiv.org/abs/2410.23037",
        "description": "arXiv:2410.23037v1 Announce Type: new \nAbstract: We propose joining a flexible mesh design with an integrated residual transcription in order to improve the accuracy of numerical solutions to optimal control problems. This approach is particularly useful when state or input trajectories are non-smooth, but it may also be beneficial when dynamics constraints are stiff. Additionally, we implement an initial phase that will ensure a feasible solution is found and can be implemented immediately in real-time controllers. Subsequent iterations with warm-starting will improve the solution until optimality is achieved. Optimizing over the mesh node locations allows for discontinuities to be captured exactly, while integrated residuals account for the approximation error in-between the nodal points. First, we numerically show the improved convergence order for the flexible mesh. We then present the feasibility-driven approach to solve control problems and show how flexible meshing and integrated residual methods can be used in practice. The presented numerical examples demonstrate for the first time the numerical implementation of a flexible mesh for an integrated residual transcription. The results show that our proposed method can be more than two times more accurate than conventional fixed mesh collocation for the same computational time and more than three times more accurate for the same problem size.",
        "published": "2024-10-31 04:00:00",
        "id": "40366350-cc08-452c-b546-bf6b4159d0fe",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出将灵活网格设计与集成残差转录相结合以提高最优控制问题数值解的准确性，阐述其优势、初始阶段实施、后续迭代改进等，并展示了灵活网格数值收敛阶的提升及可行性驱动方法，通过数值实例表明该方法比传统固定网格配置更准确。"
        },
        "tokens": 855
    },
    {
        "title": "Neural Attention Field: Emerging Point Relevance in 3D Scenes for One-Shot Dexterous Grasping",
        "link": "https://arxiv.org/abs/2410.23039",
        "description": "arXiv:2410.23039v1 Announce Type: new \nAbstract: One-shot transfer of dexterous grasps to novel scenes with object and context variations has been a challenging problem. While distilled feature fields from large vision models have enabled semantic correspondences across 3D scenes, their features are point-based and restricted to object surfaces, limiting their capability of modeling complex semantic feature distributions for hand-object interactions. In this work, we propose the \\textit{neural attention field} for representing semantic-aware dense feature fields in the 3D space by modeling inter-point relevance instead of individual point features. Core to it is a transformer decoder that computes the cross-attention between any 3D query point with all the scene points, and provides the query point feature with an attention-based aggregation. We further propose a self-supervised framework for training the transformer decoder from only a few 3D pointclouds without hand demonstrations. Post-training, the attention field can be applied to novel scenes for semantics-aware dexterous grasping from one-shot demonstration. Experiments show that our method provides better optimization landscapes by encouraging the end-effector to focus on task-relevant scene regions, resulting in significant improvements in success rates on real robots compared with the feature-field-based methods.",
        "published": "2024-10-31 04:00:00",
        "id": "59dc776d-8c92-4d4d-acd9-57ac3704a5bc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出神经注意力场用于表示3D空间中语义感知的密集特征场，通过点间相关性建模，还提出自监督框架训练其解码器，实验表明该方法可提高机器人成功率。"
        },
        "tokens": 850
    },
    {
        "title": "Emotional RAG: Enhancing Role-Playing Agents through Emotional Retrieval",
        "link": "https://arxiv.org/abs/2410.23041",
        "description": "arXiv:2410.23041v1 Announce Type: new \nAbstract: As LLMs exhibit a high degree of human-like capability, increasing attention has been paid to role-playing research areas in which responses generated by LLMs are expected to mimic human replies. This has promoted the exploration of role-playing agents in various applications, such as chatbots that can engage in natural conversations with users and virtual assistants that can provide personalized support and guidance. The crucial factor in the role-playing task is the effective utilization of character memory, which stores characters' profiles, experiences, and historical dialogues. Retrieval Augmented Generation (RAG) technology is used to access the related memory to enhance the response generation of role-playing agents. Most existing studies retrieve related information based on the semantic similarity of memory to maintain characters' personalized traits, and few attempts have been made to incorporate the emotional factor in the retrieval argument generation (RAG) of LLMs. Inspired by the Mood-Dependent Memory theory, which indicates that people recall an event better if they somehow reinstate during recall the original emotion they experienced during learning, we propose a novel emotion-aware memory retrieval framework, termed Emotional RAG, which recalls the related memory with consideration of emotional state in role-playing agents. Specifically, we design two kinds of retrieval strategies, i.e., combination strategy and sequential strategy, to incorporate both memory semantic and emotional states during the retrieval process. Extensive experiments on three representative role-playing datasets demonstrate that our Emotional RAG framework outperforms the method without considering the emotional factor in maintaining the personalities of role-playing agents. This provides evidence to further reinforce the Mood-Dependent Memory theory in psychology.",
        "published": "2024-10-31 04:00:00",
        "id": "09ef048e-dc85-4407-9be7-b9539e8bbcf3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受Mood - Dependent Memory理论启发，提出Emotional RAG框架，设计两种检索策略，实验证明其在角色扮演任务中维护角色个性方面优于未考虑情感因素的方法。"
        },
        "tokens": 926
    },
    {
        "title": "Toward Understanding In-context vs. In-weight Learning",
        "link": "https://arxiv.org/abs/2410.23042",
        "description": "arXiv:2410.23042v1 Announce Type: new \nAbstract: It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning. We do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor. Through a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge. These theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results. We then extend the study to a full large language model, showing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.",
        "published": "2024-10-31 04:00:00",
        "id": "040512ab-d276-4d0d-a0c3-ab7ceea3975e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文从理论上分析了简化分布特性对Transformer中上下文学习（in - context learning）出现和消失的影响，并通过实验验证，还将研究扩展到大型语言模型。"
        },
        "tokens": 783
    },
    {
        "title": "Legitimate ground-truth-free metrics for deep uncertainty classification scoring",
        "link": "https://arxiv.org/abs/2410.23046",
        "description": "arXiv:2410.23046v1 Announce Type: new \nAbstract: Despite the increasing demand for safer machine learning practices, the use of Uncertainty Quantification (UQ) methods in production remains limited. This limitation is exacerbated by the challenge of validating UQ methods in absence of UQ ground truth. In classification tasks, when only a usual set of test data is at hand, several authors suggested different metrics that can be computed from such test points while assessing the quality of quantified uncertainties. This paper investigates such metrics and proves that they are theoretically well-behaved and actually tied to some uncertainty ground truth which is easily interpretable in terms of model prediction trustworthiness ranking. Equipped with those new results, and given the applicability of those metrics in the usual supervised paradigm, we argue that our contributions will help promoting a broader use of UQ in deep learning.",
        "published": "2024-10-31 04:00:00",
        "id": "bebc8755-f542-43e9-b150-545761fd1d49",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究在无不确定性量化（UQ）基本事实情况下验证UQ方法的挑战，证明相关度量在理论上表现良好且与不确定性基本事实相关，有助于推动UQ在深度学习中的广泛应用。"
        },
        "tokens": 767
    },
    {
        "title": "TumblerBots: Tumbling Robotic sensors for Minimally-invasive Benthic Monitoring",
        "link": "https://arxiv.org/abs/2410.23049",
        "description": "arXiv:2410.23049v1 Announce Type: new \nAbstract: Robotic systems show significant promise for water environmental sensing applications such as water quality monitoring, pollution mapping and biodiversity data collection.\n  Conventional deployment methods often disrupt fragile ecosystems, preventing depiction of the undisturbed environmental condition. In response to this challenge, we propose a novel framework utilizing a lightweight tumbler system equipped with a sensing unit, deployed via a drone. This design minimizes disruption to the water habitat by maintaining a slow descent. The sensing unit is detached once on the water surface, enabling precise and non-invasive data collection from the benthic zone.\n  The tumbler is designed to be lightweight and compact, enabling deployment via a drone. The sensing pod, which detaches from the tumbler and descends to the bottom of the water body, is equipped with temperature and pressure sensors, as well as a buoyancy system. The later, activated upon task completion, utilizes a silicon membrane inflated via a chemical reaction. The reaction generates a pressure of 70 kPa, causing the silicon membrane to expand by 30\\%, which exceeds the 5.7\\% volume increase required for positive buoyancy. The tumblers, made from ecofriendly materials to minimize environmental impact when lost during the mission, were tested for their gliding ratio and descent rate. They exhibit a low descent rate, in the range of 0.8 to 2.5 meters per seconds, which minimizes disturbance to the ecosystem upon water landing. Additionally, the system demonstrated robustness in moderate to strong wind conditions during outdoor tests, validating the overall framework.",
        "published": "2024-10-31 04:00:00",
        "id": "6bad68b8-3f62-421b-9362-bafd9fb50624",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种利用轻型翻滚系统（配传感单元，由无人机投放）进行水底监测的新框架，介绍了翻滚器和传感吊舱的设计、功能、测试结果等。"
        },
        "tokens": 923
    },
    {
        "title": "Controlling Language and Diffusion Models by Transporting Activations",
        "link": "https://arxiv.org/abs/2410.23054",
        "description": "arXiv:2410.23054v1 Announce Type: new \nAbstract: The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.",
        "published": "2024-10-31 04:00:00",
        "id": "63b9d4fe-6558-4ac2-9a68-54257d44ef3a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Activation Transport (AcT)框架，这一框架基于最优传输理论引导激活，在大语言模型和文本到图像扩散模型中展现有效性和多功能性。"
        },
        "tokens": 801
    },
    {
        "title": "LLMs Integration in Software Engineering Team Projects: Roles, Impact, and a Pedagogical Design Space for AI Tools in Computing Education",
        "link": "https://arxiv.org/abs/2410.23069",
        "description": "arXiv:2410.23069v1 Announce Type: new \nAbstract: This work takes a pedagogical lens to explore the implications of generative AI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a semester-long 2nd-year undergraduate Software Engineering Team Project. Qualitative findings from survey (39 students) and interviews (eight students) provide insights into the students' views on the impact of GenAI use on their coding experience, learning, and self-efficacy. Our results address a particular gap in understanding the role and implications of GenAI on teamwork, team-efficacy, and team dynamics. The analysis of the learning aspects is distinguished by the application of learning and pedagogy informed lenses to discuss the data. We propose a preliminary design space for GenAI-based programming learning tools highlighting the importance of considering the roles that GenAI can play during the learning process, the varying support-ability patterns that can be applied to each role, and the importance of supporting transparency in GenAI for team members and students in addition to educators.",
        "published": "2024-10-31 04:00:00",
        "id": "f7f9f33f-d1ba-4acf-8b10-2b8bd0c009b8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章从教学视角探究生成式AI模型和工具（如ChatGPT和GitHub Copilot）在本科软件工程团队项目中的影响，通过对学生的调查和访谈得出定性结果，并提出基于GenAI的编程学习工具的初步设计空间。"
        },
        "tokens": 827
    },
    {
        "title": "FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator",
        "link": "https://arxiv.org/abs/2410.23059",
        "description": "arXiv:2410.23059v1 Announce Type: new \nAbstract: Soft robotic manipulators are generally slow despite their great adaptability, resilience, and compliance. This limitation also extends to current soft robotic micromanipulators. Here, we introduce FilMBot, a 3-DOF film-based, electromagnetically actuated, soft kinematic robotic micromanipulator achieving speeds up to 2117 $\\deg$/s and 2456 $\\deg$/s in $\\alpha$ and $\\beta$ angular motions, with corresponding linear velocities of 1.61 m/s and 1.92 m/s using a 4-cm needle end-effector, and 1.57 m/s along the Z axis. The robot can reach ~1.50 m/s in path-following tasks, operates at frequencies up to 30 Hz, and remains functional up to 50 Hz. It demonstrates high precision (~6.3 $\\mu$m, or ~0.05% of its workspace) in small path-following tasks. The novel combination of the low-stiffness soft kinematic film structure and strong electromagnetic actuation in FilMBot opens new avenues for soft robotics. Furthermore, its simple construction and inexpensive, readily accessible components could broaden the application of micromanipulators beyond current academic and professional users.",
        "published": "2024-10-31 04:00:00",
        "id": "bac9c1c9-5599-4d57-b397-78bfada34afd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "FilMBot是一种基于薄膜、电磁驱动的软运动学机器人微操纵器，速度快、精度高，其结构和驱动方式为软机器人技术开辟新途径，结构简单且组件廉价易获取，有望拓宽微操纵器的应用范围。"
        },
        "tokens": 887
    },
    {
        "title": "Learned RESESOP for solving inverse problems with inexact forward operator",
        "link": "https://arxiv.org/abs/2410.23061",
        "description": "arXiv:2410.23061v1 Announce Type: new \nAbstract: When solving inverse problems, one has to deal with numerous potential sources of model inexactnesses, like object motion, calibration errors, or simplified data models. Regularized Sequential Subspace Optimization (ReSeSOp) allows to compensate for such inaccuracies within the reconstruction step by employing consecutive projections onto suitably defined subspaces. However, this approach relies on a priori estimates for the model inexactness levels which are typically unknown. In dynamic imaging applications, where inaccuracies arise from the unpredictable dynamics of the object, these estimates are particularly challenging to determine in advance. To overcome this limitation, we propose a learned version of ReSeSOp which allows to approximate inexactness levels on the fly. The proposed framework generalizes established unrolled iterative reconstruction schemes to inexact forward operators and is particularly tailored to the structure of dynamic problems. We also present a comprehensive mathematical analysis regarding the effect of dependencies within the forward problem, clarifying when and why dividing the overall problem into subproblems is essential. The proposed method is evaluated on various examples from dynamic imaging, including datasets from a rheological CT experiment, brain MRI, and real-time cardiac MRI. The respective results emphasize improvements in reconstruction quality while ensuring adequate data consistency.",
        "published": "2024-10-31 04:00:00",
        "id": "be195ec6-4434-41cd-81c8-ccb7f937d0ad",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种学习版的ReSeSOp以解决正向算子不精确的逆问题，在动态成像示例上进行评估，结果显示该方法能提高重建质量并确保数据一致性。"
        },
        "tokens": 846
    },
    {
        "title": "Don't Just Pay Attention, PLANT It: Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification",
        "link": "https://arxiv.org/abs/2410.23066",
        "description": "arXiv:2410.23066v1 Announce Type: new \nAbstract: State-of-the-art Extreme Multi-Label Text Classification (XMTC) models rely heavily on multi-label attention layers to focus on key tokens in input text, but obtaining optimal attention weights is challenging and resource-intensive. To address this, we introduce PLANT -- Pretrained and Leveraged AtteNTion -- a novel transfer learning strategy for fine-tuning XMTC decoders. PLANT surpasses existing state-of-the-art methods across all metrics on mimicfull, mimicfifty, mimicfour, eurlex, and wikiten datasets. It particularly excels in few-shot scenarios, outperforming previous models specifically designed for few-shot scenarios by over 50 percentage points in F1 scores on mimicrare and by over 36 percentage points on mimicfew, demonstrating its superior capability in handling rare codes. PLANT also shows remarkable data efficiency in few-shot scenarios, achieving precision comparable to traditional models with significantly less data. These results are achieved through key technical innovations: leveraging a pretrained Learning-to-Rank model as the planted attention layer, integrating mutual-information gain to enhance attention, introducing an inattention mechanism, and implementing a stateful-decoder to maintain context. Comprehensive ablation studies validate the importance of these contributions in realizing the performance gains.",
        "published": "2024-10-31 04:00:00",
        "id": "6a3faecc-db2c-4e90-9192-ddfce9b55917",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍了PLANT这种新的迁移学习策略用于微调XMTC解码器，在多个数据集指标上超越现有方法，在少样本场景表现优秀，数据效率高，这是通过多种技术创新实现的。"
        },
        "tokens": 873
    },
    {
        "title": "The Days On Days Off Scheduling Problem",
        "link": "https://arxiv.org/abs/2410.23056",
        "description": "arXiv:2410.23056v1 Announce Type: new \nAbstract: Personnel scheduling problems have received considerable academic attention due to their relevance in various real-world applications. These problems involve preparing feasible schedules for an organization's employees and often account for factors such as qualifications of workers and holiday requests, resulting in complex constraints. While certain versions of the personnel rostering problem are widely acknowledged as NP-hard, there is limited theoretical analysis specific to many of its variants. Many studies simply assert the NP-hardness of the general problem without investigating whether the specific cases they address inherit this computational complexity.\n  In this paper, we examine a variant of the personnel scheduling problems, which involves scheduling a homogeneous workforce subject to constraints concerning both the total number and the number of consecutive work days and days off. This problem was claimed to be NP-complete by [Brunner+2013]. In this paper, we prove its NP-completeness and investigate how the combination of constraints contributes to this complexity. Furthermore, we analyze various special cases that arise from the omission of certain parameters, classifying them as either NP-complete or polynomial-time solvable. For the latter, we provide easy-to-implement and efficient algorithms to not only determine feasibility, but also compute a corresponding schedule.",
        "published": "2024-10-31 04:00:00",
        "id": "eecfb01d-8f9d-4b89-89d7-2c88ab0fea28",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "本文研究人员调度问题的一个变体，证明其NP完全性并分析约束组合对复杂性的贡献，还分析特殊情况并对多项式时间可解情况提供算法。"
        },
        "tokens": 826
    },
    {
        "title": "CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation",
        "link": "https://arxiv.org/abs/2410.23090",
        "description": "arXiv:2410.23090v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "c30b6491-53ac-476f-a66a-5bc69150ef67",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍CORAL，一个用于评估多轮对话检索增强生成（RAG）系统的大规模基准，支持会话RAG的三个核心任务并提出统一框架进行评估。"
        },
        "tokens": 779
    },
    {
        "title": "RSNet: A Light Framework for The Detection of Multi-scale Remote Sensing Targets",
        "link": "https://arxiv.org/abs/2410.23073",
        "description": "arXiv:2410.23073v1 Announce Type: new \nAbstract: Recent developments in synthetic aperture radar (SAR) ship detection have seen deep learning techniques achieve remarkable progress in accuracy and speed. However, the detection of small targets against complex backgrounds remains a significant challenge. To tackle these difficulties, this letter presents RSNet, a lightweight framework aimed at enhancing ship detection capabilities in SAR imagery. RSNet features the Waveletpool-ContextGuided (WCG) backbone for enhanced accuracy with fewer parameters, and the Waveletpool-StarFusion (WSF) head for efficient parameter reduction. Additionally, a Lightweight-Shared (LS) module minimizes the detection head's parameter load. Experiments on the SAR Ship Detection Dataset (SSDD) and High-Resolution SAR Image Dataset (HRSID) demonstrate that RSNet achieves a strong balance between lightweight design and detection performance, surpassing many state-of-the-art detectors, reaching 72.5\\% and 67.6\\% in \\textbf{\\(\\mathbf{mAP_{.50:95}}\\) }respectively with 1.49M parameters. Our code will be released soon.",
        "published": "2024-10-31 04:00:00",
        "id": "605fafd1-7f54-4bd2-ba2d-24a812a0a307",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出RSNet这一轻量级框架用于提高SAR图像中的船舶检测能力，其具有特定的骨干和头部结构，实验表明该框架在轻量化和检测性能间取得平衡并超越许多先进探测器。"
        },
        "tokens": 836
    },
    {
        "title": "S3PT: Scene Semantics and Structure Guided Clustering to Boost Self-Supervised Pre-Training for Autonomous Driving",
        "link": "https://arxiv.org/abs/2410.23085",
        "description": "arXiv:2410.23085v1 Announce Type: new \nAbstract: Recent self-supervised clustering-based pre-training techniques like DINO and Cribo have shown impressive results for downstream detection and segmentation tasks. However, real-world applications such as autonomous driving face challenges with imbalanced object class and size distributions and complex scene geometries. In this paper, we propose S3PT a novel scene semantics and structure guided clustering to provide more scene-consistent objectives for self-supervised training. Specifically, our contributions are threefold: First, we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals. Second, we introduce object diversity consistent spatial clustering, to handle imbalanced and diverse object sizes, ranging from large background areas to small objects such as pedestrians and traffic signs. Third, we propose a depth-guided spatial clustering to regularize learning based on geometric information of the scene, thus further refining region separation on the feature level. Our learned representations significantly improve performance in downstream semantic segmentation and 3D object detection tasks on the nuScenes, nuImages, and Cityscapes datasets and show promising domain translation properties.",
        "published": "2024-10-31 04:00:00",
        "id": "8f4201be-d48b-41ef-8c68-0734d8e7b588",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出S3PT场景语义和结构引导聚类方法以提供更场景一致的自监督训练目标，可提升下游语义分割和3D对象检测任务性能并展示出良好的域转换特性。"
        },
        "tokens": 831
    },
    {
        "title": "Statistical-Computational Trade-offs for Density Estimation",
        "link": "https://arxiv.org/abs/2410.23087",
        "description": "arXiv:2410.23087v1 Announce Type: new \nAbstract: We study the density estimation problem defined as follows: given $k$ distributions $p_1, \\ldots, p_k$ over a discrete domain $[n]$, as well as a collection of samples chosen from a ``query'' distribution $q$ over $[n]$, output $p_i$ that is ``close'' to $q$. Recently~\\cite{aamand2023data} gave the first and only known result that achieves sublinear bounds in {\\em both} the sampling complexity and the query time while preserving polynomial data structure space. However, their improvement over linear samples and time is only by subpolynomial factors.\n  Our main result is a lower bound showing that, for a broad class of data structures, their bounds cannot be significantly improved. In particular, if an algorithm uses $O(n/\\log^c k)$ samples for some constant $c>0$ and polynomial space, then the query time of the data structure must be at least $k^{1-O(1)/\\log \\log k}$, i.e., close to linear in the number of distributions $k$. This is a novel \\emph{statistical-computational} trade-off for density estimation, demonstrating that any data structure must use close to a linear number of samples or take close to linear query time. The lower bound holds even in the realizable case where $q=p_i$ for some $i$, and when the distributions are flat (specifically, all distributions are uniform over half of the domain $[n]$). We also give a simple data structure for our lower bound instance with asymptotically matching upper bounds. Experiments show that the data structure is quite efficient in practice.",
        "published": "2024-10-31 04:00:00",
        "id": "d5955c7c-07ab-40a8-90bd-c64e6b7844d9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究密度估计中的统计 - 计算权衡，给出下界表明特定数据结构的界限难以显著改进，还给出实例中有渐近匹配上界的数据结构且实验表明其在实践中高效。"
        },
        "tokens": 951
    },
    {
        "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference",
        "link": "https://arxiv.org/abs/2410.23079",
        "description": "arXiv:2410.23079v1 Announce Type: new \nAbstract: Large language models (LLMs) are essential in natural language processing but often struggle with inference speed and computational efficiency, limiting real-time deployment. The key-value (KV) cache mechanism reduces computational overhead in transformer models, but challenges in maintaining contextual understanding remain. In this paper, we propose BUZZ, a novel KV caching algorithm that leverages structured contextual information to minimize cache memory usage while enhancing inference speed. BUZZ employs a beehive-structured sparse cache, incorporating a sliding window to capture recent information and dynamically segmenting historical tokens into chunks to prioritize important tokens in local neighborhoods. We evaluate BUZZ on four real-world datasets: CNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ (1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while maintaining over 99% accuracy in long-text summarization, and (2) surpasses state-of-the-art performance in multi-document question answering by $\\textbf{7.69%}$ under the same memory limit, where full cache methods encounter out-of-memory issues. Additionally, BUZZ achieves significant inference speedup with a $\\log{n}$ time complexity. The code is available at https://github.com/JunqiZhao888/buzz-llm.",
        "published": "2024-10-31 04:00:00",
        "id": "dccd64bd-cbcb-48ec-91fd-a73b6f9eaf8f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为BUZZ的新型KV缓存算法，利用结构上下文信息减少缓存内存使用、提高推理速度，在四个真实世界数据集上进行评估，取得了减少缓存内存使用、超越问答性能、实现显著推理加速等成果，代码可在指定网址获取。"
        },
        "tokens": 902
    },
    {
        "title": "Developing a Self-Explanatory Transformer",
        "link": "https://arxiv.org/abs/2410.23083",
        "description": "arXiv:2410.23083v1 Announce Type: new \nAbstract: While IoT devices provide significant benefits, their rapid growth results in larger data volumes, increased complexity, and higher security risks. To manage these issues, techniques like encryption, compression, and mapping are used to process data efficiently and securely. General-purpose and AI platforms handle these tasks well, but mapping in natural language processing is often slowed by training times. This work explores a self-explanatory, training-free mapping transformer based on non-deterministic finite automata, designed for Field-Programmable Gate Arrays (FPGAs). Besides highlighting the advantages of this proposed approach in providing real-time, cost-effective processing and dataset-loading, we also address the challenges and considerations for enhancing the design in future iterations.",
        "published": "2024-10-31 04:00:00",
        "id": "44bf6d96-6b66-455f-9302-64ca12f2a19d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为应对物联网设备发展带来的数据问题，探索一种基于非确定性有限自动机的免训练映射变压器，该变压器用于FPGAs且具有自解释性，文章还提及这种方法在实时、成本效益处理和数据集加载方面的优势与未来设计需考虑的挑战。"
        },
        "tokens": 763
    },
    {
        "title": "Multi-Programming Language Sandbox for LLMs",
        "link": "https://arxiv.org/abs/2410.23074",
        "description": "arXiv:2410.23074v1 Announce Type: new \nAbstract: We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox designed to provide unified and comprehensive feedback from compiler and analysis tools for Large Language Models (LLMs). It can automatically identify the programming language of the code, compiling and executing it within an isolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox also integrates both traditional and LLM-based code analysis tools, providing a comprehensive analysis of generated code. MPLSandbox can be effortlessly integrated into the training and deployment of LLMs to improve the quality and correctness of their generated code. It also helps researchers streamline their workflows for various LLM-based code-related tasks, reducing the development cost. To validate the effectiveness of MPLSandbox, we integrate it into training and deployment approaches, and also employ it to optimize workflows for a wide range of real-world code-related tasks. Our goal is to enhance researcher productivity on LLM-based code-related tasks by simplifying and automating workflows through delegation to MPLSandbox.",
        "published": "2024-10-31 04:00:00",
        "id": "77dc3b2a-69cc-4786-a19a-2577f18951cf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍MPLSandbox，一种多编程语言沙盒，可自动识别代码编程语言、编译执行代码并提供分析，能集成到LLM训练和部署中以提升代码质量并降低开发成本，通过整合到相关流程验证其有效性。"
        },
        "tokens": 817
    },
    {
        "title": "First Place Solution to the ECCV 2024 ROAD++ Challenge @ ROAD++ Spatiotemporal Agent Detection 2024",
        "link": "https://arxiv.org/abs/2410.23077",
        "description": "arXiv:2410.23077v1 Announce Type: new \nAbstract: This report presents our team's solutions for the Track 1 of the 2024 ECCV ROAD++ Challenge. The task of Track 1 is spatiotemporal agent detection, which aims to construct an \"agent tube\" for road agents in consecutive video frames. Our solutions focus on the challenges in this task, including extreme-size objects, low-light scenarios, class imbalance, and fine-grained classification. Firstly, the extreme-size object detection heads are introduced to improve the detection performance of large and small objects. Secondly, we design a dual-stream detection model with a low-light enhancement stream to improve the performance of spatiotemporal agent detection in low-light scenes, and the feature fusion module to integrate features from different branches. Subsequently, we develop a multi-branch detection framework to mitigate the issues of class imbalance and fine-grained classification, and we design a pre-training and fine-tuning approach to optimize the above multi-branch framework. Besides, we employ some common data augmentation techniques, and improve the loss function and upsampling operation. We rank first in the test set of Track 1 for the ROAD++ Challenge 2024, and achieve 30.82% average video-mAP.",
        "published": "2024-10-31 04:00:00",
        "id": "33e1c62e-167b-4e04-b7db-d66cc271df5f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "This report presents the team's solutions for Track 1 of the 2024 ECCV ROAD++ Challenge in spatiotemporal agent detection, including measures for various challenges, and ranks first in the test set with 30.82% average video - mAP."
        },
        "tokens": 891
    },
    {
        "title": "PIP-MM: Pre-Integrating Prompt Information into Visual Encoding via Existing MLLM Structures",
        "link": "https://arxiv.org/abs/2410.23089",
        "description": "arXiv:2410.23089v1 Announce Type: new \nAbstract: The Multimodal Large Language Models (MLLMs) have activated the capabilitiesof Large Language Models (LLMs) in solving visual-language tasks by integratingvisual information. The prevailing approach in existing MLLMs involvesemploying an image encoder to extract visual features, converting thesefeatures into visual tokens via an adapter, and then integrating them with theprompt into the LLM. However, because the process of image encoding isprompt-agnostic, the extracted visual features only provide a coarsedescription of the image, impossible to focus on the requirements of theprompt. On one hand, it is easy for image features to lack information aboutthe prompt-specified objects, resulting in unsatisfactory responses. On theother hand, the visual features contain a large amount of irrelevantinformation, which not only increases the burden on memory but also worsens thegeneration effectiveness. To address the aforementioned issues, we propose\\textbf{PIP-MM}, a framework that \\textbf{P}re-\\textbf{I}ntegrates\\textbf{P}rompt information into the visual encoding process using existingmodules of MLLMs. Specifically, We utilize the frozen LLM in the MLLM tovectorize the input prompt, which summarizes the requirements of the prompt.Then, we input the prompt vector into our trained Multi-Layer Perceptron (MLP)to align with the visual input requirements, and subsequently replace the classembedding in the image encoder. Since our model only requires adding atrainable MLP, it can be applied to any MLLM. To validate the effectiveness ofPIP-MM, we conducted experiments on multiple benchmarks. Automated evaluationmetrics and manual assessments demonstrate the strong performance of PIP-MM.Particularly noteworthy is that our model maintains excellent generationresults even when half of the visual tokens are reduced.",
        "published": "2024-10-31 04:00:00",
        "id": "6ec81f88-8a7a-4749-878f-def461f2a1e9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出PIP - MM框架，将提示信息预集成到视觉编码过程，在多个基准上进行实验验证其有效性。"
        },
        "tokens": 957
    },
    {
        "title": "CNN Explainability with Multivector Tucker Saliency Maps for Self-Supervised Models",
        "link": "https://arxiv.org/abs/2410.23072",
        "description": "arXiv:2410.23072v1 Announce Type: new \nAbstract: Interpreting the decisions of Convolutional Neural Networks (CNNs) is essential for understanding their behavior, yet explainability remains a significant challenge, particularly for self-supervised models. Most existing methods for generating saliency maps rely on ground truth labels, restricting their use to supervised tasks. EigenCAM is the only notable label-independent alternative, leveraging Singular Value Decomposition to generate saliency maps applicable across CNN models, but it does not fully exploit the tensorial structure of feature maps. In this work, we introduce the Tucker Saliency Map (TSM) method, which applies Tucker tensor decomposition to better capture the inherent structure of feature maps, producing more accurate singular vectors and values. These are used to generate high-fidelity saliency maps, effectively highlighting objects of interest in the input. We further extend EigenCAM and TSM into multivector variants -Multivec-EigenCAM and Multivector Tucker Saliency Maps (MTSM)- which utilize all singular vectors and values, further improving saliency map quality. Quantitative evaluations on supervised classification models demonstrate that TSM, Multivec-EigenCAM, and MTSM achieve competitive performance with label-dependent methods. Moreover, TSM enhances explainability by approximately 50% over EigenCAM for both supervised and self-supervised models. Multivec-EigenCAM and MTSM further advance state-of-the-art explainability performance on self-supervised models, with MTSM achieving the best results.",
        "published": "2024-10-31 04:00:00",
        "id": "5a19bde9-1b15-47e7-9a17-9efb55b2ef4e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Tucker Saliency Map (TSM)方法及Multivec - EigenCAM和Multivector Tucker Saliency Maps (MTSM)变体，以解决卷积神经网络尤其是自监督模型的可解释性问题，经评估这些方法有较好表现。"
        },
        "tokens": 922
    },
    {
        "title": "CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for Adversarial Defense",
        "link": "https://arxiv.org/abs/2410.23091",
        "description": "arXiv:2410.23091v1 Announce Type: new \nAbstract: Despite ongoing efforts to defend neural classifiers from adversarial attacks, they remain vulnerable, especially to unseen attacks. In contrast, humans are difficult to be cheated by subtle manipulations, since we make judgments only based on essential factors. Inspired by this observation, we attempt to model label generation with essential label-causative factors and incorporate label-non-causative factors to assist data generation. For an adversarial example, we aim to discriminate the perturbations as non-causative factors and make predictions only based on the label-causative factors. Concretely, we propose a casual diffusion model (CausalDiff) that adapts diffusion models for conditional data generation and disentangles the two types of casual factors by learning towards a novel casual information bottleneck objective. Empirically, CausalDiff has significantly outperformed state-of-the-art defense methods on various unseen attacks, achieving an average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition Benchmark).",
        "published": "2024-10-31 04:00:00",
        "id": "ed8a5f6f-9d1e-44aa-8250-6c284e5dde41",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "CausalDiff是一种因果扩散模型，通过将标签生成与基本的标签因果因素建模并结合非因果因素辅助数据生成来防御神经网络分类器的对抗攻击，在多种未见过的攻击上性能优于现有方法。"
        },
        "tokens": 869
    },
    {
        "title": "First Place Solution to the ECCV 2024 ROAD++ Challenge @ ROAD++ Atomic Activity Recognition 2024",
        "link": "https://arxiv.org/abs/2410.23092",
        "description": "arXiv:2410.23092v1 Announce Type: new \nAbstract: This report presents our team's technical solution for participating in Track 3 of the 2024 ECCV ROAD++ Challenge. The task of Track 3 is atomic activity recognition, which aims to identify 64 types of atomic activities in road scenes based on video content. Our approach primarily addresses the challenges of small objects, discriminating between single object and a group of objects, as well as model overfitting in this task. Firstly, we construct a multi-branch activity recognition framework that not only separates different object categories but also the tasks of single object and object group recognition, thereby enhancing recognition accuracy. Subsequently, we develop various model ensembling strategies, including integrations of multiple frame sampling sequences, different frame sampling sequence lengths, multiple training epochs, and different backbone networks. Furthermore, we propose an atomic activity recognition data augmentation method, which greatly expands the sample space by flipping video frames and road topology, effectively mitigating model overfitting. Our methods rank first in the test set of Track 3 for the ROAD++ Challenge 2024, and achieve 69% mAP.",
        "published": "2024-10-31 04:00:00",
        "id": "fb91c970-ea46-4979-b823-07335a2df9a3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文介绍了参加2024年ECCV ROAD++挑战赛第3赛道（道路场景原子活动识别任务）的团队技术方案，包括构建多分支活动识别框架、多种模型集成策略和数据增强方法，该方案在测试集中排名第一，平均精度均值达69%。"
        },
        "tokens": 870
    },
    {
        "title": "Comparative Analysis of Demonstration Selection Algorithms for LLM In-Context Learning",
        "link": "https://arxiv.org/abs/2410.23099",
        "description": "arXiv:2410.23099v1 Announce Type: new \nAbstract: In-context learning can help Large Language Models (LLMs) to adapt new tasks without additional training. However, this performance heavily depends on the quality of the demonstrations, driving research into effective demonstration selection algorithms to optimize this process. These algorithms assist users in selecting the best $k$ input-label pairs (demonstration examples) based on a given test input, enabling LLMs to in-context learn the relationship between the provided examples and the test inputs. Despite all the proposed demonstration selection algorithms, their efficiency and effectiveness remain unclear. This lack of clarity make it difficult to apply these algorithms in real-world scenarios and poses challenges for future research aimed at developing improved methods. This paper revisits six proposed algorithms, evaluating them on five datasets from both efficiency and effectiveness perspectives. Our experiments reveal significant variations in algorithm performance across different tasks, with some methods struggling to outperform random selection in certain scenarios. We also find that increasing the number of demonstrations does not always lead to better performance, and that there are often trade-offs between accuracy and computational efficiency. Our code is available at https://github.com/Tizzzzy/Demonstration_Selection_Overview.",
        "published": "2024-10-31 04:00:00",
        "id": "6cee5c37-2db2-4d95-91f3-e10e09b325da",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文对用于大型语言模型（LLM）上下文学习的六种演示选择算法从效率和有效性两方面进行评估，发现算法性能在不同任务中差异显著，增加演示数量不总带来更好性能且准确性和计算效率间常需权衡。"
        },
        "tokens": 847
    },
    {
        "title": "Guided Game Level Repair via Explainable AI",
        "link": "https://arxiv.org/abs/2410.23101",
        "description": "arXiv:2410.23101v1 Announce Type: new \nAbstract: Procedurally generated levels created by machine learning models can be unsolvable without further editing. Various methods have been developed to automatically repair these levels by enforcing hard constraints during the post-processing step. However, as levels increase in size, these constraint-based repairs become increasingly slow. This paper proposes using explainability methods to identify specific regions of a level that contribute to its unsolvability. By assigning higher weights to these regions, constraint-based solvers can prioritize these problematic areas, enabling more efficient repairs. Our results, tested across three games, demonstrate that this approach can help to repair procedurally generated levels faster.",
        "published": "2024-10-31 04:00:00",
        "id": "6b477b7b-13c8-48ea-ace8-cae0e0cb5c97",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出使用可解释性方法识别游戏关卡中导致不可解的特定区域，通过对这些区域加权，让基于约束的求解器优先处理问题区域，从而更高效地修复由机器学习模型生成的不可解游戏关卡。"
        },
        "tokens": 728
    },
    {
        "title": "Automated Image-Based Identification and Consistent Classification of Fire Patterns with Quantitative Shape Analysis and Spatial Location Identification",
        "link": "https://arxiv.org/abs/2410.23105",
        "description": "arXiv:2410.23105v1 Announce Type: new \nAbstract: Fire patterns, consisting of fire effects that offer insights into fire behavior and origin, are traditionally classified based on investigators' visual observations, leading to subjective interpretations. This study proposes a framework for quantitative fire pattern classification to support fire investigators, aiming for consistency and accuracy. The framework integrates four components. First, it leverages human-computer interaction to extract fire patterns from surfaces, combining investigator expertise with computational analysis. Second, it employs an aspect ratio-based random forest model to classify fire pattern shapes. Third, fire scene point cloud segmentation enables precise identification of fire-affected areas and the mapping of 2D fire patterns to 3D scenes. Lastly, spatial relationships between fire patterns and indoor elements support an interpretation of the fire scene. These components provide a method for fire pattern analysis that synthesizes qualitative and quantitative data. The framework's classification results achieve 93% precision on synthetic data and 83% on real fire patterns.",
        "published": "2024-10-31 04:00:00",
        "id": "4f5721cb-c578-414f-a455-557913895163",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一个定量火灾模式分类框架，含四个组件，在合成数据和真实火灾模式上有一定精度。"
        },
        "tokens": 780
    },
    {
        "title": "Decoupling Semantic Similarity from Spatial Alignment for Neural Networks",
        "link": "https://arxiv.org/abs/2410.23107",
        "description": "arXiv:2410.23107v1 Announce Type: new \nAbstract: What representation do deep neural networks learn? How similar are images to each other for neural networks? Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity. To address this, one approach is to measure the similarity of activation responses to various inputs. Representational Similarity Matrices (RSMs) distill this similarity into scalar values for each input pair. These matrices encapsulate the entire similarity structure of a system, indicating which input leads to similar responses. While the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers. Thus this should be reflected in the definition of similarity between image responses for computer vision systems. Revisiting the established similarity calculations for RSMs we expose their sensitivity to spatial alignment. In this paper, we propose to solve this through semantic RSMs, which are invariant to spatial permutation. We measure semantic similarity between input responses by formulating it as a set-matching problem. Further, we quantify the superiority of semantic RSMs over spatio-semantic RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities.",
        "published": "2024-10-31 04:00:00",
        "id": "1afdcb0f-7840-4315-9271-d65f13189742",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对深度神经网络内部表征的高维性和复杂性导致其工作原理未被完全理解的情况，文章提出语义表征相似性矩阵（RSMs），其不受空间排列影响，可通过将输入响应的语义相似性构建为集合匹配问题来测量，并通过图像检索等方式量化其相对于空间 - 语义RSMs的优越性。"
        },
        "tokens": 891
    },
    {
        "title": "Controllable Game Level Generation: Assessing the Effect of Negative Examples in GAN Models",
        "link": "https://arxiv.org/abs/2410.23108",
        "description": "arXiv:2410.23108v1 Announce Type: new \nAbstract: Generative Adversarial Networks (GANs) are unsupervised models designed to learn and replicate a target distribution. The vanilla versions of these models can be extended to more controllable models. Conditional Generative Adversarial Networks (CGANs) extend vanilla GANs by conditioning both the generator and discriminator on some additional information (labels). Controllable models based on complementary learning, such as Rumi-GAN, have been introduced. Rumi-GANs leverage negative examples to enhance the generator's ability to learn positive examples. We evaluate the performance of two controllable GAN variants, CGAN and Rumi-GAN, in generating game levels targeting specific constraints of interest: playability and controllability. This evaluation is conducted under two scenarios: with and without the inclusion of negative examples. The goal is to determine whether incorporating negative examples helps the GAN models avoid generating undesirable outputs. Our findings highlight the strengths and weaknesses of each method in enforcing the generation of specific conditions when generating outputs based on given positive and negative examples.",
        "published": "2024-10-31 04:00:00",
        "id": "b3d82b7e-4b6e-46e6-b2ee-26b7c2b389eb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章评估了CGAN和Rumi - GAN两种可控GAN变体在生成游戏关卡时，有无负样本情况下对可玩性和可控性的表现，以确定负样本是否有助于避免不良输出。"
        },
        "tokens": 821
    },
    {
        "title": "Why Gradient Subspace? Identifying and Mitigating LoRA's Bottlenecks in Federated Fine-Tuning of Large Language Models",
        "link": "https://arxiv.org/abs/2410.23111",
        "description": "arXiv:2410.23111v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in task generalization for both text and vision data. While fine-tuning these models can significantly enhance their performance on specific downstream tasks, it often requires high-quality data that cannot be shared due to privacy concerns. Federated Learning (FL) offers a promising solution for collaborative training without direct data sharing. However, many parameter-efficient fine-tuning strategies for LLMs in FL, particularly those based on Low-Rank Adaptation (LoRA), face limitations. In this paper, we critically analyze the convergence and performance guarantees of popular FL frameworks utilizing LoRA, highlighting its suboptimal nature due to constrained subspace learning of low-rank matrices. This limitation hinders effective fine-tuning of LLMs in federated settings. Through rigorous analytical and empirical evaluations, we demonstrate that direct weight averaging outperforms LoRA-based strategies, leading to superior performance for fine-tuned models. Our comprehensive comparison exposes inefficiencies in LoRA approaches and underscores the advantages of full-rank weight aggregation. We extend our analysis to low-rank gradient-based optimizers, such as GaLore, used during local training steps. Our findings show that GaLore is a more effective alternative, outperforming federated LoRA methods like FlexLoRA and FFA-LoRA across both text and image modalities. While privacy remains paramount in FL discourse, our focus is on assessing performance outcomes of federated fine-tuned models and evaluating various FL frameworks from both theoretical and empirical perspectives. Our findings advocate reassessing the reliance on LoRA within FL contexts, paving the way for more efficient training methodologies.",
        "published": "2024-10-31 04:00:00",
        "id": "0599cc98-0596-4c19-9e28-34e6775b1ebd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文分析联邦学习框架中低秩适配（LoRA）在大型语言模型微调中的局限性，发现直接权重平均效果优于基于LoRA的策略，GaLore比联邦LoRA方法更有效，倡导重新评估联邦学习中对LoRA的依赖。"
        },
        "tokens": 966
    },
    {
        "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
        "link": "https://arxiv.org/abs/2410.23114",
        "description": "arXiv:2410.23114v1 Announce Type: new \nAbstract: Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, in this paper we design a unified framework to measure object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to conduct hallucination evaluation on (object, relation, object) triplets extracted from LVLMs' responses, and thus, could be easily generalized to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. We conduct comprehensive evaluations on Tri-HE and observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple yet effective training-free approach to mitigate hallucinations for LVLMs, with which, we exceed all open-sourced counterparts on Tri-HE, achieving comparable performance with the powerful GPT-4V. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.",
        "published": "2024-10-31 04:00:00",
        "id": "763cc524-7da2-4aab-b140-718afb5e70b2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出统一框架同时测量大型视觉 - 语言模型中的对象和关系幻觉，介绍Tri - HE基准，评估发现关系幻觉问题更严重，还设计了减轻幻觉的方法并公开数据集和代码。"
        },
        "tokens": 913
    },
    {
        "title": "Teaching a Language Model to Distinguish Between Similar Details using a Small Adversarial Training Set",
        "link": "https://arxiv.org/abs/2410.23118",
        "description": "arXiv:2410.23118v1 Announce Type: new \nAbstract: Language models can achieve high accuracy on natural language tasks such as NLI, but performance suffers on manually created adversarial examples. We investigate the performance of a language model trained on the Stanford Natural Language Inference (SNLI) corpus on a manually created adversarial test set. We then improve the model's performance by fine tuning the model on a small, manually created adversarial training set, designed to help the language model to learn to differentiate between similar words and phrases in the data. We show an increase in accuracy on the adversarial test set (+ 13%) while still maintaining good performance on the original NLI task. We also show an increase in accuracy from 91.2% to 92.9% on the most similar contradictions in the SNLI test set (as judged by cosine similarity).",
        "published": "2024-10-31 04:00:00",
        "id": "3ca3eebd-d5f9-4b14-aa12-cd1d0350bd90",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究语言模型在自然语言推理任务（如SNLI）中的表现，通过小的对抗训练集微调模型，提高其在对抗测试集上的准确率，且在原任务中仍保持良好性能，同时提升了对相似矛盾判断的准确率。"
        },
        "tokens": 784
    },
    {
        "title": "On Memorization of Large Language Models in Logical Reasoning",
        "link": "https://arxiv.org/abs/2410.23123",
        "description": "arXiv:2410.23123v1 Announce Type: new \nAbstract: Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&amp;K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&amp;K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Our code and data are available at https://memkklogic.github.io.",
        "published": "2024-10-31 04:00:00",
        "id": "8fdb3367-453a-435a-815a-50861beecb6c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文通过基于骑士与无赖谜题动态生成的逻辑推理基准，对大语言模型在推理任务中的记忆情况进行定量测量，发现模型微调后能解决训练谜题但谜题稍有变动就失败，微调虽导致大量记忆但也提高泛化性能，还分析了模型如何在推理和记忆间转换。"
        },
        "tokens": 918
    },
    {
        "title": "Educating for Hardware Specialization in the Chiplet Era: A Path for the HPC Community",
        "link": "https://arxiv.org/abs/2410.23127",
        "description": "arXiv:2410.23127v1 Announce Type: new \nAbstract: The advent of chiplet technology introduces cutting-edge opportunities for constructing highly heterogeneous platforms with specialized accelerators. However, the HPC community currently lacks expertise in hardware development, a gap that must be bridged to leverage these advancements. Additionally, technologies like chiplet is cutting-edge with limited educational resource available. This paper addresses potential hardware specialization direction in HPC and how to cultivate these skills among students and staff, emphasizing the importance of understanding and developing custom hardware (e.g., rapid prototyping and resource estimation). We have been mentoring graduate-level students and new staff in hardware designs in a hands-on manner, encouraging them to utilize modern open-source hardware tools for their designs, which facilitates the sharing of research ideas. Additionally, we provide a summary of theses tools as part of our approach to prototyping and mentoring.",
        "published": "2024-10-31 04:00:00",
        "id": "d8ee164c-5607-4924-8455-b96893ea04c6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "在芯片小粒时代，高性能计算社区缺乏硬件开发专长，本文探讨了硬件专业化方向并阐述如何培养相关技能，介绍了指导学生和员工进行硬件设计的情况及相关工具。"
        },
        "tokens": 774
    },
    {
        "title": "Real-Time Personalization for LLM-based Recommendation with Customized In-Context Learning",
        "link": "https://arxiv.org/abs/2410.23136",
        "description": "arXiv:2410.23136v1 Announce Type: new \nAbstract: Frequently updating Large Language Model (LLM)-based recommender systems to adapt to new user interests -- as done for traditional ones -- is impractical due to high training costs, even with acceleration methods. This work explores adapting to dynamic user interests without any model updates by leveraging In-Context Learning (ICL), which allows LLMs to learn new tasks from few-shot examples provided in the input. Using new-interest examples as the ICL few-shot examples, LLMs may learn real-time interest directly, avoiding the need for model updates. However, existing LLM-based recommenders often lose the in-context learning ability during recommendation tuning, while the original LLM's in-context learning lacks recommendation-specific focus. To address this, we propose RecICL, which customizes recommendation-specific in-context learning for real-time recommendations. RecICL organizes training examples in an in-context learning format, ensuring that in-context learning ability is preserved and aligned with the recommendation task during tuning.\n  Extensive experiments demonstrate RecICL's effectiveness in delivering real-time recommendations without requiring model updates. Our code is available at https://github.com/ym689/rec_icl.",
        "published": "2024-10-31 04:00:00",
        "id": "22352698-5c75-48f3-9ee2-4b63dd98ed7b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出RecICL通过定制推荐特定的上下文学习实现实时推荐，无需模型更新以适应动态用户兴趣。"
        },
        "tokens": 829
    },
    {
        "title": "Fair Division with Market Values",
        "link": "https://arxiv.org/abs/2410.23137",
        "description": "arXiv:2410.23137v1 Announce Type: new \nAbstract: We introduce a model of fair division with market values, where indivisible goods must be partitioned among agents with (additive) subjective valuations, and each good additionally has a market value. The market valuation can be viewed as a separate additive valuation that holds identically across all the agents. We seek allocations that are simultaneously fair with respect to the subjective valuations and with respect to the market valuation.\n  We show that an allocation that satisfies stochastically-dominant envy-freeness up to one good (SD-EF1) with respect to both the subjective valuations and the market valuation does not always exist, but the weaker guarantee of EF1 with respect to the subjective valuations along with SD-EF1 with respect to the market valuation can be guaranteed. We also study a number of other guarantees such as Pareto optimality, EFX, and MMS. In addition, we explore non-additive valuations and extend our model to cake-cutting. Along the way, we identify several tantalizing open questions.",
        "published": "2024-10-31 04:00:00",
        "id": "dd2e060c-aa9b-4d22-ab45-e69d65739303",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "介绍了带有市场价值的公平分配模型，探讨了多种分配保证，研究非加性估值并扩展到切蛋糕问题，提出了一些开放性问题。"
        },
        "tokens": 797
    },
    {
        "title": "FAIR-TAT: Improving Model Fairness Using Targeted Adversarial Training",
        "link": "https://arxiv.org/abs/2410.23142",
        "description": "arXiv:2410.23142v1 Announce Type: new \nAbstract: Deep neural networks are susceptible to adversarial attacks and common corruptions, which undermine their robustness. In order to enhance model resilience against such challenges, Adversarial Training (AT) has emerged as a prominent solution. Nevertheless, adversarial robustness is often attained at the expense of model fairness during AT, i.e., disparity in class-wise robustness of the model. While distinctive classes become more robust towards such adversaries, hard to detect classes suffer. Recently, research has focused on improving model fairness specifically for perturbed images, overlooking the accuracy of the most likely non-perturbed data. Additionally, despite their robustness against the adversaries encountered during model training, state-of-the-art adversarial trained models have difficulty maintaining robustness and fairness when confronted with diverse adversarial threats or common corruptions. In this work, we address the above concerns by introducing a novel approach called Fair Targeted Adversarial Training (FAIR-TAT). We show that using targeted adversarial attacks for adversarial training (instead of untargeted attacks) can allow for more favorable trade-offs with respect to adversarial fairness. Empirical results validate the efficacy of our approach.",
        "published": "2024-10-31 04:00:00",
        "id": "e586f1ba-ea3b-4c61-8362-6cfc098c2384",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "FAIR - TAT通过采用靶向对抗训练提升模型公平性，以解决对抗训练中模型公平性受影响以及现有对抗训练模型在应对不同对抗威胁或常见损坏时难以保持稳健性和公平性的问题。"
        },
        "tokens": 836
    },
    {
        "title": "The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in Lie Detection",
        "link": "https://arxiv.org/abs/2410.23143",
        "description": "arXiv:2410.23143v1 Announce Type: new \nAbstract: We investigate how low-quality AI advisors, lacking quality disclosures, can help spread text-based lies while seeming to help people detect lies. Participants in our experiment discern truth from lies by evaluating transcripts from a game show that mimicked deceptive social media exchanges on topics with objective truths. We find that when relying on low-quality advisors without disclosures, participants' truth-detection rates fall below their own abilities, which recovered once the AI's true effectiveness was revealed. Conversely, high-quality advisor enhances truth detection, regardless of disclosure. We discover that participants' expectations about AI capabilities contribute to their undue reliance on opaque, low-quality advisors.",
        "published": "2024-10-31 04:00:00",
        "id": "ab41437d-e70d-489d-8907-5e006cebc221",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现低质量且无质量披露的AI顾问会降低参与者辨别文本谎言的能力，而高质量的AI顾问有助于辨别，参与者对AI能力的预期会导致他们过度依赖不透明的低质量AI顾问。"
        },
        "tokens": 741
    },
    {
        "title": "Public Domain 12M: A Highly Aesthetic Image-Text Dataset with Novel Governance Mechanisms",
        "link": "https://arxiv.org/abs/2410.23144",
        "description": "arXiv:2410.23144v1 Announce Type: new \nAbstract: We present Public Domain 12M (PD12M), a dataset of 12.4 million high-quality public domain and CC0-licensed images with synthetic captions, designed for training text-to-image models. PD12M is the largest public domain image-text dataset to date, with sufficient size to train foundation models while minimizing copyright concerns. Through the Source.Plus platform, we also introduce novel, community-driven dataset governance mechanisms that reduce harm and support reproducibility over time.",
        "published": "2024-10-31 04:00:00",
        "id": "5d842c02-3507-40d0-83ee-4df33be8603c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Public Domain 12M（PD12M），一个含1240万高质量公共领域和CC0许可图像及合成字幕的数据集，有新型治理机制，可用于训练文本到图像模型。"
        },
        "tokens": 719
    },
    {
        "title": "Why Fine-grained Labels in Pretraining Benefit Generalization?",
        "link": "https://arxiv.org/abs/2410.23129",
        "description": "arXiv:2410.23129v1 Announce Type: new \nAbstract: Recent studies show that pretraining a deep neural network with fine-grained labeled data, followed by fine-tuning on coarse-labeled data for downstream tasks, often yields better generalization than pretraining with coarse-labeled data. While there is ample empirical evidence supporting this, the theoretical justification remains an open problem. This paper addresses this gap by introducing a \"hierarchical multi-view\" structure to confine the input data distribution. Under this framework, we prove that: 1) coarse-grained pretraining only allows a neural network to learn the common features well, while 2) fine-grained pretraining helps the network learn the rare features in addition to the common ones, leading to improved accuracy on hard downstream test samples.",
        "published": "2024-10-31 04:00:00",
        "id": "ca7b8ef8-af6d-417f-b132-32cc655c8dbe",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究在预训练中细粒度标签为何有利于泛化，通过引入层次多视图结构限制输入数据分布，证明粗粒度预训练只能让神经网络学好共同特征，细粒度预训练能让网络学习到稀有特征，从而提高下游测试样本的准确率。"
        },
        "tokens": 758
    },
    {
        "title": "Federated Learning under Periodic Client Participation and Heterogeneous Data: A New Communication-Efficient Algorithm and Analysis",
        "link": "https://arxiv.org/abs/2410.23131",
        "description": "arXiv:2410.23131v1 Announce Type: new \nAbstract: In federated learning, it is common to assume that clients are always available to participate in training, which may not be feasible with user devices in practice. Recent works analyze federated learning under more realistic participation patterns, such as cyclic client availability or arbitrary participation. However, all such works either require strong assumptions (e.g., all clients participate almost surely within a bounded window), do not achieve linear speedup and reduced communication rounds, or are not applicable in the general non-convex setting. In this work, we focus on nonconvex optimization and consider participation patterns in which the chance of participation over a fixed window of rounds is equal among all clients, which includes cyclic client availability as a special case. Under this setting, we propose a new algorithm, named Amplified SCAFFOLD, and prove that it achieves linear speedup, reduced communication, and resilience to data heterogeneity simultaneously. In particular, for cyclic participation, our algorithm is proved to enjoy $\\mathcal{O}(\\epsilon^{-2})$ communication rounds to find an $\\epsilon$-stationary point in the non-convex stochastic setting. In contrast, the prior work under the same setting requires $\\mathcal{O}(\\kappa^2 \\epsilon^{-4})$ communication rounds, where $\\kappa$ denotes the data heterogeneity. Therefore, our algorithm significantly reduces communication rounds due to better dependency in terms of $\\epsilon$ and $\\kappa$. Our analysis relies on a fine-grained treatment of the nested dependence between client participation and errors in the control variates, which results in tighter guarantees than previous work. We also provide experimental results with (1) synthetic data and (2) real-world data with a large number of clients $(N = 250)$, demonstrating the effectiveness of our algorithm under periodic client participation.",
        "published": "2024-10-31 04:00:00",
        "id": "b0ff572d-2d5c-4529-9ea0-f0151b8384a9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在联邦学习中，考虑所有客户端在固定轮数窗口内参与机会均等的参与模式，提出新算法Amplified SCAFFOLD，证明其在非凸随机设置下实现线性加速、减少通信和对数据异质性的恢复能力，理论和实验证明算法有效性。"
        },
        "tokens": 993
    },
    {
        "title": "Revisiting MAE pre-training for 3D medical image segmentation",
        "link": "https://arxiv.org/abs/2410.23132",
        "description": "arXiv:2410.23132v1 Announce Type: new \nAbstract: Self-Supervised Learning (SSL) presents an exciting opportunity to unlock the potential of vast, untapped clinical datasets, for various downstream applications that suffer from the scarcity of labeled data. While SSL has revolutionized fields like natural language processing and computer vision, their adoption in 3D medical image computing has been limited by three key pitfalls: Small pre-training dataset sizes, architectures inadequate for 3D medical image analysis, and insufficient evaluation practices. We address these issues by i) leveraging a large-scale dataset of 44k 3D brain MRI volumes and ii) using a Residual Encoder U-Net architecture within the state-of-the-art nnU-Net framework. iii) A robust development framework, incorporating 5 development and 8 testing brain MRI segmentation datasets, allowed performance-driven design decisions to optimize the simple concept of Masked Auto Encoders (MAEs) for 3D CNNs. The resulting model not only surpasses previous SSL methods but also outperforms the strong nnU-Net baseline by an average of approximately 3 Dice points. Furthermore, our model demonstrates exceptional stability, achieving the highest average rank of 2 out of 7 methods, compared to the second-best method's mean rank of 3.",
        "published": "2024-10-31 04:00:00",
        "id": "3f48e1b9-0731-4430-82c8-88ca74d8f4ed",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "利用44k 3D脑MRI数据，在nnU - Net框架下采用Residual Encoder U - Net架构优化MAE概念的模型在3D医学图像分割上表现优于之前的SSL方法和nnU - Net基线。"
        },
        "tokens": 869
    },
    {
        "title": "Crowdsourcing Lexical Diversity",
        "link": "https://arxiv.org/abs/2410.23133",
        "description": "arXiv:2410.23133v1 Announce Type: new \nAbstract: Lexical-semantic resources (LSRs), such as online lexicons or wordnets, are fundamental for natural language processing applications. In many languages, however, such resources suffer from quality issues: incorrect entries, incompleteness, but also, the rarely addressed issue of bias towards the English language and Anglo-Saxon culture. Such bias manifests itself in the absence of concepts specific to the language or culture at hand, the presence of foreign (Anglo-Saxon) concepts, as well as in the lack of an explicit indication of untranslatability, also known as cross-lingual \\emph{lexical gaps}, when a term has no equivalent in another language. This paper proposes a novel crowdsourcing methodology for reducing bias in LSRs. Crowd workers compare lexemes from two languages, focusing on domains rich in lexical diversity, such as kinship or food. Our LingoGap crowdsourcing tool facilitates comparisons through microtasks identifying equivalent terms, language-specific terms, and lexical gaps across languages. We validated our method by applying it to two case studies focused on food-related terminology: (1) English and Arabic, and (2) Standard Indonesian and Banjarese. These experiments identified 2,140 lexical gaps in the first case study and 951 in the second. The success of these experiments confirmed the usability of our method and tool for future large-scale lexicon enrichment tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "7a42de6a-efc1-404c-a448-00ce71bd56d6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的众包方法减少词汇语义资源中的偏差，通过 LingoGap 众包工具对比两种语言的词素，经两个食物相关术语的案例研究验证该方法的有效性。"
        },
        "tokens": 884
    },
    {
        "title": "Leader-Follower 3D Formation for Underwater Robots",
        "link": "https://arxiv.org/abs/2410.23128",
        "description": "arXiv:2410.23128v1 Announce Type: new \nAbstract: The schooling behavior of fish is hypothesized to confer many survival benefits, including foraging success, safety from predators, and energy savings through hydrodynamic interactions when swimming in formation. Underwater robot collectives may be able to achieve similar benefits in future applications, e.g. using formation control to achieve efficient spatial sampling for environmental monitoring. Although many theoretical algorithms exist for multi-robot formation control, they have not been tested in the underwater domain due to the fundamental challenges in underwater communication. Here we introduce a leader-follower strategy for underwater formation control that allows us to realize complex 3D formations, using purely vision-based perception and a reactive control algorithm that is low computation. We use a physical platform, BlueSwarm, to demonstrate for the first time an experimental realization of inline, side-by-side, and staggered swimming 3D formations. More complex formations are studied in a physics-based simulator, providing new insights into the convergence and stability of formations given underwater inertial/drag conditions. Our findings lay the groundwork for future applications of underwater robot swarms in aquatic environments with minimal communication.",
        "published": "2024-10-31 04:00:00",
        "id": "94886e5f-07d8-4a50-9be7-4c3aec45cceb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍水下机器人的领导者 - 跟随者3D编队策略，通过视觉感知和低计算反应控制算法实现复杂3D编队，在物理平台和模拟器上进行研究，为水下机器人集群应用奠定基础。"
        },
        "tokens": 823
    },
    {
        "title": "QWO: Speeding Up Permutation-Based Causal Discovery in LiGAMs",
        "link": "https://arxiv.org/abs/2410.23155",
        "description": "arXiv:2410.23155v1 Announce Type: new \nAbstract: Causal discovery is essential for understanding relationships among variables of interest in many scientific domains. In this paper, we focus on permutation-based methods for learning causal graphs in Linear Gaussian Acyclic Models (LiGAMs), where the permutation encodes a causal ordering of the variables. Existing methods in this setting are not scalable due to their high computational complexity. These methods are comprised of two main components: (i) constructing a specific DAG, $\\mathcal{G}^\\pi$, for a given permutation $\\pi$, which represents the best structure that can be learned from the available data while adhering to $\\pi$, and (ii) searching over the space of permutations (i.e., causal orders) to minimize the number of edges in $\\mathcal{G}^\\pi$. We introduce QWO, a novel approach that significantly enhances the efficiency of computing $\\mathcal{G}^\\pi$ for a given permutation $\\pi$. QWO has a speed-up of $O(n^2)$ ($n$ is the number of variables) compared to the state-of-the-art BIC-based method, making it highly scalable. We show that our method is theoretically sound and can be integrated into existing search strategies such as GRASP and hill-climbing-based methods to improve their performance.",
        "published": "2024-10-31 04:00:00",
        "id": "02afcd6d-ac99-48ce-bd94-d85165d36036",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出QWO方法提高计算线性高斯无环模型中特定DAG的效率，比现有基于BIC的方法快O(n²)，理论可靠且可融入现有搜索策略提升性能。"
        },
        "tokens": 869
    },
    {
        "title": "Energy-Efficient Intra-Domain Network Slicing for Multi-Layer Orchestration in Intelligent-Driven Distributed 6G Networks: Learning Generic Assignment Skills with Unsupervised Reinforcement Learning",
        "link": "https://arxiv.org/abs/2410.23161",
        "description": "arXiv:2410.23161v1 Announce Type: new \nAbstract: Since the 6th Generation (6G) of wireless networks is expected to provide a new level of network services and meet the emerging expectations of the future, it will be a complex and intricate networking system. 6Gs sophistication and robustness will be accompanied by complexities, which will require novel strategies to tackle them. This research work focuses on decentralized and multi-level system models for 6G networks and proposes an energy efficient automation strategy for edge domain management and Network Slicing (NS) with the main objective of reducing the networks complexity by leveraging scalability, efficiency, and generalization. Accordingly, we propose a pre-train phase to discover useful assignment skills in network edge domains by utilizing unsupervised Reinforcement Learning (unsupervised RL). The suggested technique does not depend on the domain specifications and thus is applicable to all the edge domains. Our proposed approach not only enables scalability and decentralization, but it also delivers efficiency by assisting domain controllers to provide various service types. We implemented the pre-training phase, and monitored that the discovered assignment skills span the entire interval of possible resource assignment portions for every service type.",
        "published": "2024-10-31 04:00:00",
        "id": "ff8674bd-364c-4396-9e7a-83ea075b0cbf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对6G网络的复杂性，提出利用无监督强化学习发现网络边缘域中的有用分配技能的预训练阶段，以实现边缘域管理和网络切片的节能自动化策略。"
        },
        "tokens": 846
    },
    {
        "title": "SciPIP: An LLM-based Scientific Paper Idea Proposer",
        "link": "https://arxiv.org/abs/2410.23166",
        "description": "arXiv:2410.23166v1 Announce Type: new \nAbstract: The exponential growth of knowledge and the increasing complexity of interdisciplinary research pose significant challenges for researchers, including information overload and difficulties in exploring novel ideas. The advancements in large language models (LLMs), such as GPT-4, have shown great potential in enhancing idea proposals, but how to effectively utilize large models for reasonable idea proposal has not been thoroughly explored. This paper proposes a scientific paper idea proposer (SciPIP). Based on a user-provided research background, SciPIP retrieves helpful papers from a literature database while leveraging the capabilities of LLMs to generate more novel and feasible ideas. To this end, 1) we construct a literature retrieval database, extracting lots of papers' multi-dimension information for fast access. Then, a literature retrieval method based on semantics, entity, and citation co-occurrences is proposed to search relevant literature from multiple aspects based on the user-provided background. 2) After literature retrieval, we introduce dual-path idea proposal strategies, where one path infers solutions from the retrieved literature and the other path generates original ideas through model brainstorming. We then combine the two to achieve a good balance between feasibility and originality. Through extensive experiments on the natural language processing (NLP) field, we demonstrate that SciPIP can retrieve citations similar to those of existing top conference papers and generate many ideas consistent with them. Additionally, we evaluate the originality of other ideas generated by SciPIP using large language models, further validating the effectiveness of our proposed method. The code and the database are released at https://github.com/cheerss/SciPIP.",
        "published": "2024-10-31 04:00:00",
        "id": "e6ad89a4-cb1f-414a-8512-6a27bd50a6cf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文《SciPIP: An LLM - based Scientific Paper Idea Proposer》提出基于用户提供的研究背景，通过构建文献检索数据库、采用文献检索方法、引入双路径想法提出策略来生成更具新颖性和可行性的科学论文想法，并在NLP领域进行实验验证，代码和数据库已开源。"
        },
        "tokens": 954
    },
    {
        "title": "TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters",
        "link": "https://arxiv.org/abs/2410.23168",
        "description": "arXiv:2410.23168v1 Announce Type: new \nAbstract: Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at \\url{https://github.com/Haiyang-W/TokenFormer}.",
        "published": "2024-10-31 04:00:00",
        "id": "58af3e1d-6198-4da2-b711-856c0f16e23c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍TokenFormer架构，它将模型参数视为令牌，用令牌 - 参数注意层取代Transformer中的线性投影，可逐步高效扩展，将模型从124M扩展到1.4B参数时性能可比从头训练的Transformer且降低训练成本。"
        },
        "tokens": 870
    },
    {
        "title": "The Persistence of Neural Collapse Despite Low-Rank Bias: An Analytic Perspective Through Unconstrained Features",
        "link": "https://arxiv.org/abs/2410.23169",
        "description": "arXiv:2410.23169v1 Announce Type: new \nAbstract: Modern deep neural networks have been observed to exhibit a simple structure in their final layer features and weights, commonly referred to as neural collapse. This phenomenon has also been noted in layers beyond the final one, an extension known as deep neural collapse. Recent findings indicate that such a structure is generally not optimal in the deep unconstrained feature model, an approximation of an expressive network. This is attributed to a low-rank bias induced by regularization, which favors solutions with lower-rank than those typically associated with deep neural collapse. In this work, we extend these observations to the cross-entropy loss and analyze how the low-rank bias influences various solutions. Additionally, we explore how this bias induces specific structures in the singular values of the weights at global optima. Furthermore, we examine the loss surface of these models and provide evidence that the frequent observation of deep neural collapse in practice, despite its suboptimality, may result from its higher degeneracy on the loss surface.",
        "published": "2024-10-31 04:00:00",
        "id": "b29cf6e2-61d5-495a-9fa3-a554b031d779",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "现代深度神经网络的最终层特征和权重存在一种被称为神经坍缩的结构，在其他层也有类似现象，本文研究了低秩偏差对交叉熵损失下的各种解的影响、对全局最优权重奇异值结构的诱导，以及模型损失面情况，解释了尽管神经坍缩非最优但仍常被观察到的原因。"
        },
        "tokens": 842
    },
    {
        "title": "ReasoningRec: Bridging Personalized Recommendations and Human-Interpretable Explanations through LLM Reasoning",
        "link": "https://arxiv.org/abs/2410.23180",
        "description": "arXiv:2410.23180v1 Announce Type: new \nAbstract: This paper presents ReasoningRec, a reasoning-based recommendation framework that leverages Large Language Models (LLMs) to bridge the gap between recommendations and human-interpretable explanations. In contrast to conventional recommendation systems that rely on implicit user-item interactions, ReasoningRec employs LLMs to model users and items, focusing on preferences, aversions, and explanatory reasoning. The framework utilizes a larger LLM to generate synthetic explanations for user preferences, subsequently used to fine-tune a smaller LLM for enhanced recommendation accuracy and human-interpretable explanation. Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM's capacity to generate plausible explanations. Empirical evaluations demonstrate that ReasoningRec surpasses state-of-the-art methods by up to 12.5\\% in recommendation prediction while concurrently providing human-intelligible explanations. The code is available here: https://github.com/millenniumbismay/reasoningrec.",
        "published": "2024-10-31 04:00:00",
        "id": "8873c491-3103-4cc4-b16f-dd27bd3b14d8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出ReasoningRec推理推荐框架，利用大型语言模型连接推荐与可解释性，实验表明其在推荐预测上超现有方法且能提供可理解的解释"
        },
        "tokens": 810
    },
    {
        "title": "VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning",
        "link": "https://arxiv.org/abs/2410.23156",
        "description": "arXiv:2410.23156v1 Announce Type: new \nAbstract: Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.",
        "published": "2024-10-31 04:00:00",
        "id": "dead32e1-e229-4692-8cf2-7815faa4d63d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Neuro - Symbolic Predicates这种一阶抽象语言用于机器人规划，阐述相关在线算法，与其他方法比较后显示该方法在样本复杂度、泛化能力和可解释性上有优势。"
        },
        "tokens": 746
    },
    {
        "title": "ProTransformer: Robustify Transformers via Plug-and-Play Paradigm",
        "link": "https://arxiv.org/abs/2410.23182",
        "description": "arXiv:2410.23182v1 Announce Type: new \nAbstract: Transformer-based architectures have dominated various areas of machine learning in recent years. In this paper, we introduce a novel robust attention mechanism designed to enhance the resilience of transformer-based architectures. Crucially, this technique can be integrated into existing transformers as a plug-and-play layer, improving their robustness without the need for additional training or fine-tuning. Through comprehensive experiments and ablation studies, we demonstrate that our ProTransformer significantly enhances the robustness of transformer models across a variety of prediction tasks, attack mechanisms, backbone architectures, and data domains. Notably, without further fine-tuning, the ProTransformer consistently improves the performance of vanilla transformers by 19.5%, 28.3%, 16.1%, and 11.4% for BERT, ALBERT, DistilBERT, and RoBERTa, respectively, under the classical TextFooler attack. Furthermore, ProTransformer shows promising resilience in large language models (LLMs) against prompting-based attacks, improving the performance of T5 and LLaMA by 24.8% and 17.8%, respectively, and enhancing Vicuna by an average of 10.4% against the Jailbreaking attack. Beyond the language domain, ProTransformer also demonstrates outstanding robustness in both vision and graph domains.",
        "published": "2024-10-31 04:00:00",
        "id": "5d4d736b-3f04-4857-a57f-0513076dffdb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文介绍一种新的鲁棒注意力机制ProTransformer，可作为即插即用层提升Transformer架构的鲁棒性，在多种预测任务、攻击机制、骨干架构和数据域中的实验表明其能显著提升模型性能。"
        },
        "tokens": 878
    },
    {
        "title": "Fourier Amplitude and Correlation Loss: Beyond Using L2 Loss for Skillful Precipitation Nowcasting",
        "link": "https://arxiv.org/abs/2410.23159",
        "description": "arXiv:2410.23159v1 Announce Type: new \nAbstract: Deep learning approaches have been widely adopted for precipitation nowcasting in recent years. Previous studies mainly focus on proposing new model architectures to improve pixel-wise metrics. However, they frequently result in blurry predictions which provide limited utility to forecasting operations. In this work, we propose a new Fourier Amplitude and Correlation Loss (FACL) which consists of two novel loss terms: Fourier Amplitude Loss (FAL) and Fourier Correlation Loss (FCL). FAL regularizes the Fourier amplitude of the model prediction and FCL complements the missing phase information. The two loss terms work together to replace the traditional $L_2$ losses such as MSE and weighted MSE for the spatiotemporal prediction problem on signal-based data. Our method is generic, parameter-free and efficient. Extensive experiments using one synthetic dataset and three radar echo datasets demonstrate that our method improves perceptual metrics and meteorology skill scores, with a small trade-off to pixel-wise accuracy and structural similarity. Moreover, to improve the error margin in meteorological skill scores such as Critical Success Index (CSI) and Fractions Skill Score (FSS), we propose and adopt the Regional Histogram Divergence (RHD), a distance metric that considers the patch-wise similarity between signal-based imagery patterns with tolerance to local transforms. Code is available at https://github.com/argenycw/FACL",
        "published": "2024-10-31 04:00:00",
        "id": "62ead62d-7214-490f-aaea-581ecddc0fc5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新的傅里叶幅度和相关损失（FACL），包含傅里叶幅度损失（FAL）和傅里叶相关损失（FCL）来改进降水临近预报，实验表明该方法可提高感知指标和气象技能得分，还提出区域直方图散度（RHD）以改善气象技能分数中的误差范围并提供了代码链接。"
        },
        "tokens": 922
    },
    {
        "title": "FlexTSF: A Universal Forecasting Model for Time Series with Variable Regularities",
        "link": "https://arxiv.org/abs/2410.23160",
        "description": "arXiv:2410.23160v1 Announce Type: new \nAbstract: Developing a foundation model for time series forecasting across diverse domains has attracted significant attention in recent years. Existing works typically assume regularly sampled, well-structured data, limiting their applicability to more generalized scenarios where time series often contain missing values, unequal sequence lengths, and irregular time intervals between measurements. To cover diverse domains and handle variable regularities, we propose FlexTSF, a universal time series forecasting model that possesses better generalization and natively support both regular and irregular time series. FlexTSF produces forecasts in an autoregressive manner and incorporates three novel designs: VT-Norm, a normalization strategy to ablate data domain barriers, IVP Patcher, a patching module to learn representations from flexibly structured time series, and LED attention, an attention mechanism to seamlessly integrate these two and propagate forecasts with awareness of domain and time information. Experiments on 12 datasets show that FlexTSF outperforms state-of-the-art forecasting models respectively designed for regular and irregular time series. Furthermore, after self-supervised pre-training, FlexTSF shows exceptional performance in both zero-shot and few-show settings for time series forecasting.",
        "published": "2024-10-31 04:00:00",
        "id": "44a7daf2-9a7b-4866-81d8-60d09939180d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出FlexTSF这一通用时间序列预测模型，它具有更好的泛化能力，原生支持规则和不规则时间序列，包含三种新设计，在12个数据集的实验中表现优于现有模型，自监督预训练后在零样本和少样本设置下表现出色。"
        },
        "tokens": 853
    },
    {
        "title": "Does equivariance matter at scale?",
        "link": "https://arxiv.org/abs/2410.23179",
        "description": "arXiv:2410.23179v1 Announce Type: new \nAbstract: Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.",
        "published": "2024-10-31 04:00:00",
        "id": "fe300df9-e678-473c-8956-03ceb15555f0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究通过刚体交互基准问题和通用变压器架构的实验，探究等变和非等变网络在计算和训练样本上的扩展情况，得出等变可提高数据效率、等变模型在计算扩展方面表现更好以及计算预算在模型大小和训练时长上的最优分配在等变和非等变模型中存在差异这三个结论。"
        },
        "tokens": 809
    },
    {
        "title": "Directional anomaly detection",
        "link": "https://arxiv.org/abs/2410.23158",
        "description": "arXiv:2410.23158v1 Announce Type: new \nAbstract: Semi-supervised anomaly detection is based on the principle that potential anomalies are those records that look different from normal training data. However, in some cases we are specifically interested in anomalies that correspond to high attribute values (or low, but not both). We present two asymmetrical distance measures that take this directionality into account: ramp distance and signed distance. Through experiments on synthetic and real-life datasets we show that ramp distance performs as well or better than the absolute distance traditionally used in anomaly detection. While signed distance also performs well on synthetic data, it performs substantially poorer on real-life datasets. We argue that this reflects the fact that in practice, good scores on some attributes should not be allowed to compensate for bad scores on others.",
        "published": "2024-10-31 04:00:00",
        "id": "4d1e0632-fafa-43d8-b68f-1304edad8832",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出两种考虑方向性的不对称距离度量（斜坡距离和有符号距离）用于半监督异常检测，实验表明斜坡距离在真实数据集上表现良好，有符号距离在真实数据集上表现较差。"
        },
        "tokens": 738
    },
    {
        "title": "Reliability of Topic Modeling",
        "link": "https://arxiv.org/abs/2410.23186",
        "description": "arXiv:2410.23186v1 Announce Type: new \nAbstract: Topic models allow researchers to extract latent factors from text data and use those variables in downstream statistical analyses. However, these methodologies can vary significantly due to initialization differences, randomness in sampling procedures, or noisy data. Reliability of these methods is of particular concern as many researchers treat learned topic models as ground truth for subsequent analyses. In this work, we show that the standard practice for quantifying topic model reliability fails to capture essential aspects of the variation in two widely-used topic models. Drawing from a extensive literature on measurement theory, we provide empirical and theoretical analyses of three other metrics for evaluating the reliability of topic models. On synthetic and real-world data, we show that McDonald's $\\omega$ provides the best encapsulation of reliability. This metric provides an essential tool for validation of topic model methodologies that should be a standard component of any topic model-based research.",
        "published": "2024-10-31 04:00:00",
        "id": "7f8c5c84-f656-4e1c-bd2a-634b585399bc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文指出主题模型因多种因素存在差异，可靠性值得关注，通过实证和理论分析发现麦当劳ω指标在评估主题模型可靠性上表现最佳，可作为相关研究的标准验证工具。"
        },
        "tokens": 769
    },
    {
        "title": "HEX: Hierarchical Emergence Exploitation in Self-Supervised Algorithms",
        "link": "https://arxiv.org/abs/2410.23200",
        "description": "arXiv:2410.23200v1 Announce Type: new \nAbstract: In this paper, we propose an algorithm that can be used on top of a wide variety of self-supervised (SSL) approaches to take advantage of hierarchical structures that emerge during training. SSL approaches typically work through some invariance term to ensure consistency between similar samples and a regularization term to prevent global dimensional collapse. Dimensional collapse refers to data representations spanning a lower-dimensional subspace. Recent work has demonstrated that the representation space of these algorithms gradually reflects a semantic hierarchical structure as training progresses. Data samples of the same hierarchical grouping tend to exhibit greater dimensional collapse locally compared to the dataset as a whole due to sharing features in common with each other. Ideally, SSL algorithms would take advantage of this hierarchical emergence to have an additional regularization term to account for this local dimensional collapse effect. However, the construction of existing SSL algorithms does not account for this property. To address this, we propose an adaptive algorithm that performs a weighted decomposition of the denominator of the InfoNCE loss into two terms: local hierarchical and global collapse regularization respectively. This decomposition is based on an adaptive threshold that gradually lowers to reflect the emerging hierarchical structure of the representation space throughout training. It is based on an analysis of the cosine similarity distribution of samples in a batch. We demonstrate that this hierarchical emergence exploitation (HEX) approach can be integrated across a wide variety of SSL algorithms. Empirically, we show performance improvements of up to 5.6% relative improvement over baseline SSL approaches on classification accuracy on Imagenet with 100 epochs of training.",
        "published": "2024-10-31 04:00:00",
        "id": "78f9876b-4b0b-488d-bdb1-2cf724381780",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种可用于多种自监督算法的算法HEX，利用训练时出现的层次结构，将InfoNCE损失分母加权分解为两项，经验证可提升在Imagenet上的分类准确率。"
        },
        "tokens": 917
    },
    {
        "title": "Resilient-By-Design: A Resiliency Framework for Future Wireless Networks",
        "link": "https://arxiv.org/abs/2410.23203",
        "description": "arXiv:2410.23203v1 Announce Type: new \nAbstract: Our future society will be increasingly digitalised, hyper-connected and globally data driven. The sixth generation (6G) and beyond 6G wireless networks are expected to bridge the digital and physical worlds by providing wireless connectivity as a service to different vertical sectors, including industries, smart cities, eHealth and autonomous transportation. Such far reaching integration will render the society increasingly reliant on wireless networks. While this has the potential to greatly enhance our quality and ease of life, any disruption to these networks would also have significant impact with overreaching consequences. Disruptions can happen due to a variety of reasons, including planned outages, failures due to the nature of wireless propagation, natural disasters, and deliberate cybersecurity attacks. Hence, 6G and beyond 6G networks should not only provide near instant and virtually unlimited connectivity, but also be resilient against internal and external disruptions. This paper proposes a resilient-by-design framework towards this end. First, we provide an overview of the disruption landscape. Thereafter, we comprehensively outline the main features of the proposed concept. Finally, we detail the four key steps of the framework, namely predict, preempt, protect and progress. A simple but illustrative preliminary simulation result is also presented to highlight the potential advantages and efficiency of the proposed approach in addressing outages.",
        "published": "2024-10-31 04:00:00",
        "id": "17b6a917-4701-42a5-a3ec-cce0482a787b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一个面向未来无线网络的弹性设计框架，先概述破坏情况，再阐述概念主要特征，最后详述框架的预测、抢先、保护和进步四个关键步骤，并给出初步模拟结果以展示优势。"
        },
        "tokens": 869
    },
    {
        "title": "Enhancing Autonomous Driving Safety Analysis with Generative AI: A Comparative Study on Automated Hazard and Risk Assessment",
        "link": "https://arxiv.org/abs/2410.23207",
        "description": "arXiv:2410.23207v1 Announce Type: new \nAbstract: The advent of autonomous driving technology has accentuated the need for comprehensive hazard analysis and risk assessment (HARA) to ensure the safety and reliability of vehicular systems. Traditional HARA processes, while meticulous, are inherently time-consuming and subject to human error, necessitating a transformative approach to fortify safety engineering. This paper presents an integrative application of generative artificial intelligence (AI) as a means to enhance HARA in autonomous driving safety analysis. Generative AI, renowned for its predictive modeling and data generation capabilities, is leveraged to automate the labor-intensive elements of HARA, thus expediting the process and augmenting the thoroughness of the safety analyses. Through empirical research, the study contrasts conventional HARA practices conducted by safety experts with those supplemented by generative AI tools. The benchmark comparisons focus on critical metrics such as analysis time, error rates, and scope of risk identification. By employing generative AI, the research demonstrates a significant upturn in efficiency, evidenced by reduced timeframes and expanded analytical coverage. The AI-augmented processes also deliver enhanced brainstorming support, stimulating creative problem-solving and identifying previously unrecognized risk factors.",
        "published": "2024-10-31 04:00:00",
        "id": "a4c4a425-7678-40b0-b45e-340612bf32ac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出将生成式人工智能应用于自动驾驶安全分析中的危害分析和风险评估（HARA），通过实证研究对比传统方法和生成式AI辅助方法，结果表明AI辅助可提高效率、扩大分析范围并激发创造性解决问题能力。"
        },
        "tokens": 853
    },
    {
        "title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks",
        "link": "https://arxiv.org/abs/2410.23208",
        "description": "arXiv:2410.23208v1 Announce Type: new \nAbstract: While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge. In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control. To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework. Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training. Our trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*. This includes solving some environments that standard RL training completely fails at. We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.",
        "published": "2024-10-31 04:00:00",
        "id": "5f97a19d-b9a9-48d9-a526-351687dc2fde",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Kinetix，通过生成大量2D物理任务训练通用强化学习（RL）代理，其使用新的物理引擎Jax2D，训练后的代理有很强的物理推理能力，微调效果优于从头训练。"
        },
        "tokens": 868
    },
    {
        "title": "ELMGS: Enhancing memory and computation scaLability through coMpression for 3D Gaussian Splatting",
        "link": "https://arxiv.org/abs/2410.23213",
        "description": "arXiv:2410.23213v1 Announce Type: new \nAbstract: 3D models have recently been popularized by the potentiality of end-to-end training offered first by Neural Radiance Fields and most recently by 3D Gaussian Splatting models. The latter has the big advantage of naturally providing fast training convergence and high editability. However, as the research around these is still in its infancy, there is still a gap in the literature regarding the model's scalability. In this work, we propose an approach enabling both memory and computation scalability of such models. More specifically, we propose an iterative pruning strategy that removes redundant information encoded in the model. We also enhance compressibility for the model by including in the optimization strategy a differentiable quantization and entropy coding estimator. Our results on popular benchmarks showcase the effectiveness of the proposed approach and open the road to the broad deployability of such a solution even on resource-constrained devices.",
        "published": "2024-10-31 04:00:00",
        "id": "5bae209a-a168-4f6a-bf2a-ed596285c4a8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "3D高斯散斑模型在可扩展性方面存在研究空白，本文提出一种迭代剪枝策略及在优化策略中纳入可微量化和熵编码估计器来提高模型的内存和计算可扩展性，实验结果展示了方法的有效性。"
        },
        "tokens": 801
    },
    {
        "title": "OS-ATLAS: A Foundation Action Model for Generalist GUI Agents",
        "link": "https://arxiv.org/abs/2410.23218",
        "description": "arXiv:2410.23218v1 Announce Type: new \nAbstract: Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "52c20e27-3ae3-45cf-85af-e3aa129e3204",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为推动GUI代理研究，开发了OS - Atlas基础GUI动作模型，开发开源工具包合成多平台GUI基础数据并发布最大开源跨平台GUI基础语料库，经多平台基准评估性能优于之前模型。"
        },
        "tokens": 874
    },
    {
        "title": "DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET",
        "link": "https://arxiv.org/abs/2410.23219",
        "description": "arXiv:2410.23219v1 Announce Type: new \nAbstract: Diagnosing dementia, particularly for Alzheimer's Disease (AD) and frontotemporal dementia (FTD), is complex due to overlapping symptoms. While magnetic resonance imaging (MRI) and positron emission tomography (PET) data are critical for the diagnosis, integrating these modalities in deep learning faces challenges, often resulting in suboptimal performance compared to using single modalities. Moreover, the potential of multi-modal approaches in differential diagnosis, which holds significant clinical importance, remains largely unexplored. We propose a novel framework, DiaMond, to address these issues with vision Transformers to effectively integrate MRI and PET. DiaMond is equipped with self-attention and a novel bi-attention mechanism that synergistically combine MRI and PET, alongside a multi-modal normalization to reduce redundant dependency, thereby boosting the performance. DiaMond significantly outperforms existing multi-modal methods across various datasets, achieving a balanced accuracy of 92.4% in AD diagnosis, 65.2% for AD-MCI-CN classification, and 76.5% in differential diagnosis of AD and FTD. We also validated the robustness of DiaMond in a comprehensive ablation study. The code is available at https://github.com/ai-med/DiaMond.",
        "published": "2024-10-31 04:00:00",
        "id": "3a535d4c-8333-46c5-9bda-54ce03bebaac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出DiaMond框架，用视觉变换器整合MRI和PET数据诊断痴呆症，在多种数据集上表现优于现有多模态方法且经消融研究验证稳健，代码已开源。"
        },
        "tokens": 847
    },
    {
        "title": "Partial Channel Dependence with Channel Masks for Time Series Foundation Models",
        "link": "https://arxiv.org/abs/2410.23222",
        "description": "arXiv:2410.23222v1 Announce Type: new \nAbstract: Recent advancements in foundation models have been successfully extended to the time series (TS) domain, facilitated by the emergence of large-scale TS datasets. However, previous efforts have primarily focused on designing model architectures to address explicit heterogeneity among datasets such as various numbers of channels, while often overlooking implicit heterogeneity such as varying dependencies between channels. In this work, we introduce the concept of partial channel dependence (PCD), which enables a more sophisticated adjustment of channel dependencies based on dataset-specific information. To achieve PCD, we propose a channel mask that captures the relationships between channels within a dataset using two key components: 1) a correlation matrix that encodes relative dependencies between channels, and 2) domain parameters that learn the absolute dependencies specific to each dataset, refining the correlation matrix. We validate the effectiveness of PCD across four tasks in TS including forecasting, classification, imputation, and anomaly detection, under diverse settings, including few-shot and zero-shot scenarios with both TS foundation models and single-task models. Code is available at https://github.com/seunghan96/CM.",
        "published": "2024-10-31 04:00:00",
        "id": "80104608-0698-497f-b6bb-63d73507295c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "在将基础模型的进展扩展到时间序列领域的背景下，提出部分通道依赖概念，通过包含相关矩阵和领域参数的通道掩码实现，验证其在时间序列的四项任务中的有效性并提供代码。"
        },
        "tokens": 825
    },
    {
        "title": "LGU-SLAM: Learnable Gaussian Uncertainty Matching with Deformable Correlation Sampling for Deep Visual SLAM",
        "link": "https://arxiv.org/abs/2410.23231",
        "description": "arXiv:2410.23231v1 Announce Type: new \nAbstract: Deep visual Simultaneous Localization and Mapping (SLAM) techniques, e.g., DROID, have made significant advancements by leveraging deep visual odometry on dense flow fields. In general, they heavily rely on global visual similarity matching. However, the ambiguous similarity interference in uncertain regions could often lead to excessive noise in correspondences, ultimately misleading SLAM in geometric modeling. To address this issue, we propose a Learnable Gaussian Uncertainty (LGU) matching. It mainly focuses on precise correspondence construction. In our scheme, a learnable 2D Gaussian uncertainty model is designed to associate matching-frame pairs. It could generate input-dependent Gaussian distributions for each correspondence map. Additionally, a multi-scale deformable correlation sampling strategy is devised to adaptively fine-tune the sampling of each direction by a priori look-up ranges, enabling reliable correlation construction. Furthermore, a KAN-bias GRU component is adopted to improve a temporal iterative enhancement for accomplishing sophisticated spatio-temporal modeling with limited parameters. The extensive experiments on real-world and synthetic datasets are conducted to validate the effectiveness and superiority of our method.",
        "published": "2024-10-31 04:00:00",
        "id": "556a02a5-65e4-4739-a41a-d444cbe92297",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决深度视觉SLAM中不确定区域的相似性干扰问题，提出可学习的高斯不确定性匹配方法，包含可学习2D高斯不确定性模型、多尺度可变形相关采样策略和KAN - bias GRU组件，经实验验证其有效性和优越性。"
        },
        "tokens": 856
    },
    {
        "title": "(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning",
        "link": "https://arxiv.org/abs/2410.23227",
        "description": "arXiv:2410.23227v1 Announce Type: new \nAbstract: Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.",
        "published": "2024-10-31 04:00:00",
        "id": "49d1a5e8-6619-4e93-8a12-58e1edc4057a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为$(FL)^2$的联邦半监督学习训练方法，以解决联邦学习中标签不足问题，实验证明该方法能显著提升性能并缩小与集中式半监督学习的差距。"
        },
        "tokens": 851
    },
    {
        "title": "Emergence of meta-stable clustering in mean-field transformer models",
        "link": "https://arxiv.org/abs/2410.23228",
        "description": "arXiv:2410.23228v1 Announce Type: new \nAbstract: We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in (Geshkovski et al., 2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials.",
        "published": "2024-10-31 04:00:00",
        "id": "0a19b953-c074-43b3-9adf-bf878ce9394d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过基于(Geshkovski等人，2023)框架的平均场相互作用粒子系统对Transformer层内的token演化建模，研究相应的平均场偏微分方程，进行微扰分析并明确元稳定流形结构。"
        },
        "tokens": 828
    },
    {
        "title": "Aligning Audio-Visual Joint Representations with an Agentic Workflow",
        "link": "https://arxiv.org/abs/2410.23230",
        "description": "arXiv:2410.23230v1 Announce Type: new \nAbstract: Visual content and accompanied audio signals naturally formulate a joint representation to improve audio-visual (AV) related applications. While studies develop various AV representation learning frameworks, the importance of AV data alignment is usually undermined for achieving high-quality representation. We observe that an audio signal may contain background noise interference. Also, non-synchronization may appear between audio and video streams. These non-strict data alignment limits representation quality and downgrade application performance. In this paper, we propose to improve AV joint representations from a data-centric perspective by aligning audio signals to visual data. Our alignment is conducted in an agentic workflow controlled by an LLM-based assistant named AVAgent. For each input AV data pair, our AVAgent uses a multi-modal LLM to convert audio and visual data into language descriptions separately (i.e., tool use). Then, AVAgent reasons whether this paired data is aligned well and plans to edit the audio signal if needed (i.e., planning). The audio editing is executed by predefined actions that filter noise or augment data. Moreover, we use a VLM to evaluate how modified audio signals match the visual content and provide feedback to AVAgent (i.e., reflection). The tool use, planning, and reflection steps operate cyclically to become an agentic workflow where audio signals are gradually aligned to visual content. To this end, existing methods can directly leverage the aligned AV data via our agentic workflow to improve AV joint representations. The experimental results comprehensively demonstrate the state-of-the-art performance of the proposed approach against previous baselines in diverse downstream tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "83194514-8f18-4675-94dc-7ae6291de3d3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出通过名为AVAgent的工作流程将音频信号与视觉数据对齐来改进视听联合表示，工作流程包括工具使用、规划、反思步骤，实验结果表明该方法在下游任务中有良好表现。"
        },
        "tokens": 921
    },
    {
        "title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences",
        "link": "https://arxiv.org/abs/2410.23223",
        "description": "arXiv:2410.23223v1 Announce Type: new \nAbstract: Many alignment methods, including reinforcement learning from human feedback (RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to capture the full range of general human preferences. To achieve robust alignment with general preferences, we model the alignment problem as a two-player zero-sum game, where the Nash equilibrium policy guarantees a 50% win rate against any competing policy. However, previous algorithms for finding the Nash policy either diverge or converge to a Nash policy in a modified game, even in a simple synthetic setting, thereby failing to maintain the 50% win rate guarantee against all other policies. We propose a meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), for language model alignment with general preferences, inspired by convergent algorithms in game theory. Theoretically, we prove that our meta-algorithm converges to an exact Nash policy in the last iterate. Additionally, our meta-algorithm is simple and can be integrated with many existing methods designed for RLHF and preference optimization with minimal changes. Experimental results demonstrate the effectiveness of the proposed framework when combined with existing preference policy optimization methods.",
        "published": "2024-10-31 04:00:00",
        "id": "5e767b9c-92b3-40fa-a161-59e4802732c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为使语言模型与人类一般偏好对齐，COMAL将对齐问题建模为双人零和博弈，理论上能收敛到精确的纳什均衡策略，且易于与现有方法集成，实验证明其与现有偏好策略优化方法结合时有效。"
        },
        "tokens": 848
    },
    {
        "title": "Attribute-to-Delete: Machine Unlearning via Datamodel Matching",
        "link": "https://arxiv.org/abs/2410.23232",
        "description": "arXiv:2410.23232v1 Announce Type: new \nAbstract: Machine unlearning -- efficiently removing the effect of a small \"forget set\" of training data on a pre-trained machine learning model -- has recently attracted significant research interest. Despite this interest, however, recent work shows that existing machine unlearning techniques do not hold up to thorough evaluation in non-convex settings. In this work, we introduce a new machine unlearning technique that exhibits strong empirical performance even in such challenging settings. Our starting point is the perspective that the goal of unlearning is to produce a model whose outputs are statistically indistinguishable from those of a model re-trained on all but the forget set. This perspective naturally suggests a reduction from the unlearning problem to that of data attribution, where the goal is to predict the effect of changing the training set on a model's outputs. Thus motivated, we propose the following meta-algorithm, which we call Datamodel Matching (DMM): given a trained model, we (a) use data attribution to predict the output of the model if it were re-trained on all but the forget set points; then (b) fine-tune the pre-trained model to match these predicted outputs. In a simple convex setting, we show how this approach provably outperforms a variety of iterative unlearning algorithms. Empirically, we use a combination of existing evaluations and a new metric based on the KL-divergence to show that even in non-convex settings, DMM achieves strong unlearning performance relative to existing algorithms. An added benefit of DMM is that it is a meta-algorithm, in the sense that future advances in data attribution translate directly into better unlearning algorithms, pointing to a clear direction for future progress in unlearning.",
        "published": "2024-10-31 04:00:00",
        "id": "ae756b85-a2ca-4eb9-a3eb-7db12d26b078",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一种新的机器遗忘技术Datamodel Matching (DMM)，在非凸设置下表现良好，通过数据归因预测模型输出并微调预训练模型，实证显示其性能优于现有算法。"
        },
        "tokens": 945
    },
    {
        "title": "EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning",
        "link": "https://arxiv.org/abs/2410.23234",
        "description": "arXiv:2410.23234v1 Announce Type: new \nAbstract: This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in humanlike non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.",
        "published": "2024-10-31 04:00:00",
        "id": "d4436ea9-bcff-4dad-8a78-53e2c781d0b8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍EMOTION框架用于生成人形机器人的表达性运动序列，利用大型语言模型的情境学习能力动态生成社交手势动作序列，经用户研究，其在特定场景下生成机器人动作的表现可匹配或超越人类。"
        },
        "tokens": 831
    },
    {
        "title": "A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment",
        "link": "https://arxiv.org/abs/2410.23242",
        "description": "arXiv:2410.23242v1 Announce Type: new \nAbstract: As general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "c1641f48-67c8-4b77-af87-eca88e60c7c3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章倡导通过让大型语言模型（LLMs）控制3D环境中的代理来评估其物理常识推理能力，使用Animal - AI环境和测试平台研究LLMs的距离估计、追踪看不见的物体和工具使用等物理推理能力，发现当前LLMs在这些任务上不如人类儿童。"
        },
        "tokens": 1004
    },
    {
        "title": "EMMA: End-to-End Multimodal Model for Autonomous Driving",
        "link": "https://arxiv.org/abs/2410.23262",
        "description": "arXiv:2410.23262v1 Announce Type: new \nAbstract: We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built on a multi-modal large language model foundation, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. However, EMMA also exhibits certain limitations: it can process only a small amount of image frames, does not incorporate accurate 3D sensing modalities like LiDAR or radar and is computationally expensive. We hope that our results will inspire further research to mitigate these issues and to further evolve the state of the art in autonomous driving model architectures.",
        "published": "2024-10-31 04:00:00",
        "id": "e234d62a-fb5e-4988-9a2b-b4f7644dbca4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了EMMA这一用于自动驾驶的端到端多模态模型，阐述其基于多模态大语言模型基础的工作原理、性能表现、取得的成果、存在的局限性等。"
        },
        "tokens": 913
    },
    {
        "title": "PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D Matching",
        "link": "https://arxiv.org/abs/2410.23245",
        "description": "arXiv:2410.23245v1 Announce Type: new \nAbstract: We propose a novel online, point-based 3D reconstruction method from posed monocular RGB videos. Our model maintains a global point cloud representation of the scene, continuously updating the features and 3D locations of points as new images are observed. It expands the point cloud with newly detected points while carefully removing redundancies. The point cloud updates and depth predictions for new points are achieved through a novel ray-based 2D-3D feature matching technique, which is robust against errors in previous point position predictions. In contrast to offline methods, our approach processes infinite-length sequences and provides real-time updates. Additionally, the point cloud imposes no pre-defined resolution or scene size constraints, and its unified global representation ensures view consistency across perspectives. Experiments on the ScanNet dataset show that our method achieves state-of-the-art quality among online MVS approaches. Project page: https://arthurhero.github.io/projects/pointrecon",
        "published": "2024-10-31 04:00:00",
        "id": "bacfe251-1d35-4141-ae56-8f6cd2d44952",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新的基于点的在线3D重建方法，通过基于光线的2D - 3D特征匹配技术更新点云并预测新点深度，在ScanNet数据集上达到在线多视图立体视觉方法中的最先进质量。"
        },
        "tokens": 806
    },
    {
        "title": "Keypoint Abstraction using Large Models for Object-Relative Imitation Learning",
        "link": "https://arxiv.org/abs/2410.23254",
        "description": "arXiv:2410.23254v1 Announce Type: new \nAbstract: Generalization to novel object configurations and instances across diverse tasks and environments is a critical challenge in robotics. Keypoint-based representations have been proven effective as a succinct representation for capturing essential object features, and for establishing a reference frame in action prediction, enabling data-efficient learning of robot skills. However, their manual design nature and reliance on additional human labels limit their scalability. In this paper, we propose KALM, a framework that leverages large pre-trained vision-language models (LMs) to automatically generate task-relevant and cross-instance consistent keypoints. KALM distills robust and consistent keypoints across views and objects by generating proposals using LMs and verifies them against a small set of robot demonstration data. Based on the generated keypoints, we can train keypoint-conditioned policy models that predict actions in keypoint-centric frames, enabling robots to generalize effectively across varying object poses, camera views, and object instances with similar functional shapes. Our method demonstrates strong performance in the real world, adapting to different tasks and environments from only a handful of demonstrations while requiring no additional labels. Website: https://kalm-il.github.io/",
        "published": "2024-10-31 04:00:00",
        "id": "a4edfbcf-7bef-4d60-a79a-1dc89ae2ea69",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出KALM框架，利用大预训练视觉 - 语言模型自动生成任务相关且跨实例一致的关键点，基于这些关键点训练策略模型，提升机器人在不同对象姿态、视图和实例下的泛化能力。"
        },
        "tokens": 840
    },
    {
        "title": "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources",
        "link": "https://arxiv.org/abs/2410.23261",
        "description": "arXiv:2410.23261v1 Announce Type: new \nAbstract: Pre-training is notoriously compute-intensive and academic researchers are notoriously under-resourced. It is, therefore, commonly assumed that academics can't pre-train models. In this paper, we seek to clarify this assumption. We first survey academic researchers to learn about their available compute and then empirically measure the time to replicate models on such resources. We introduce a benchmark to measure the time to pre-train models on given GPUs and also identify ideal settings for maximizing training speed. We run our benchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on our experiments. Our results reveal a brighter picture for academic pre-training: for example, although Pythia-1B was originally trained on 64 GPUs for 3 days, we find it is also possible to replicate this model (with the same hyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude with a cost-benefit analysis to help clarify the trade-offs between price and pre-training time. We believe our benchmark will help academic researchers conduct experiments that require training larger models on more data. We fully release our codebase at: https://github.com/apoorvkh/academic-pretraining.",
        "published": "2024-10-31 04:00:00",
        "id": "870f70b7-cc5c-4343-9ece-ec57a6808fd2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨学术资源预训练的权衡，调查学术研究者可用计算资源，通过实验测量在这些资源上复制模型的时间，还引入基准测量预训练时间并确定理想设置，最后进行成本效益分析，代码已开源。"
        },
        "tokens": 880
    },
    {
        "title": "Carrot and Stick: Eliciting Comparison Data and Beyond",
        "link": "https://arxiv.org/abs/2410.23243",
        "description": "arXiv:2410.23243v1 Announce Type: new \nAbstract: Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment. Our design leverages on the strong stochastic transitivity for comparison data to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.\n  We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents' private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.",
        "published": "2024-10-31 04:00:00",
        "id": "29bc51bb-d33d-40a4-b8e8-00f12e4bc8d7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究设计了奖金 - 惩罚支付的同行预测机制来获取比较数据，还扩展该概念到获取网络数据，实验证明了理论发现。"
        },
        "tokens": 805
    },
    {
        "title": "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models",
        "link": "https://arxiv.org/abs/2410.23266",
        "description": "arXiv:2410.23266v1 Announce Type: new \nAbstract: Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.",
        "published": "2024-10-31 04:00:00",
        "id": "d9119a28-95e7-4b74-9a8a-c9905891bab0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究表明现有多模态基础模型视觉时序推理能力可能被高估，提出三个评估原则并构建TOMATO基准测试集，包含1484个问题、1417个视频，评估发现最佳模型与人类性能有57.3%差距，模型在连续帧理解上存在局限。"
        },
        "tokens": 975
    },
    {
        "title": "Commit: Online Groups with Participation Commitments",
        "link": "https://arxiv.org/abs/2410.23267",
        "description": "arXiv:2410.23267v1 Announce Type: new \nAbstract: In spite of efforts to increase participation, many online groups struggle to survive past the initial days, as members leave and activity atrophies. We argue that a main assumption of online group design -- that groups ask nothing of their members beyond lurking -- may be preventing many of these groups from sustaining a critical mass of participation. In this paper, we explore an alternative commitment design for online groups, which requires that all members commit at regular intervals to participating, as a condition of remaining in the group. We instantiate this approach in a mobile group chat platform called Commit, and perform a field study comparing commitment against a control condition of social psychological nudges with N=57 participants over three weeks. Commitment doubled the number of contributions versus the control condition, and resulted in 87% (vs. 19%) of participants remaining active by the third week. Participants reported that commitment provided safe cover for them to post even when they were nervous. Through this work, we argue that more effortful, not less effortful, membership may support many online groups.",
        "published": "2024-10-31 04:00:00",
        "id": "974c8118-7b18-41e9-9321-067d75099edb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一种名为Commit的在线群组承诺设计，在移动群聊平台进行的为期三周、57人参与的实地研究表明，该设计可增加贡献数量、提高活跃参与者比例，认为更积极的成员身份可能有助于在线群组发展。"
        },
        "tokens": 828
    },
    {
        "title": "A Monte Carlo Framework for Calibrated Uncertainty Estimation in Sequence Prediction",
        "link": "https://arxiv.org/abs/2410.23272",
        "description": "arXiv:2410.23272v1 Announce Type: new \nAbstract: Probabilistic prediction of sequences from images and other high-dimensional data is a key challenge, particularly in risk-sensitive applications. In these settings, it is often desirable to quantify the uncertainty associated with the prediction (instead of just determining the most likely sequence, as in language modeling). In this paper, we propose a Monte Carlo framework to estimate probabilities and confidence intervals associated with the distribution of a discrete sequence. Our framework uses a Monte Carlo simulator, implemented as an autoregressively trained neural network, to sample sequences conditioned on an image input. We then use these samples to estimate the probabilities and confidence intervals. Experiments on synthetic and real data show that the framework produces accurate discriminative predictions, but can suffer from miscalibration. In order to address this shortcoming, we propose a time-dependent regularization method, which is shown to produce calibrated predictions.",
        "published": "2024-10-31 04:00:00",
        "id": "b96dbcb8-2f57-4cb3-bf77-b2d05ca06e4e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出蒙特卡洛框架用于估计离散序列分布相关的概率和置信区间，实验表明框架能做出准确判别预测但可能校准失准，为此提出时间相关的正则化方法来做出校准预测。"
        },
        "tokens": 775
    },
    {
        "title": "Proportional Fairness in Non-Centroid Clustering",
        "link": "https://arxiv.org/abs/2410.23273",
        "description": "arXiv:2410.23273v1 Announce Type: new \nAbstract: We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to non-centroid clustering, where the loss of an agent is a function of the other agents in its cluster, by adapting two proportional fairness criteria -- the core and its relaxation, fully justified representation (FJR) -- to this setting.\n  We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a natural loss function. In contrast, we design a new (inefficient) algorithm, GreedyCohesiveClustering, which achieves the relaxation FJR exactly under arbitrary loss functions, and show that the efficient GreedyCapture algorithm achieves a constant approximation of FJR. We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor. Our experiments on real data suggest that traditional clustering algorithms are highly unfair, whereas GreedyCapture is considerably fairer and incurs only a modest loss in common clustering objectives.",
        "published": "2024-10-31 04:00:00",
        "id": "77de3383-693d-4488-8a6b-cb3a84950b89",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文将比例公平框架扩展到非质心聚类，研究两种比例公平标准在该设置下的情况，设计算法并通过实验表明传统聚类算法不公平，而新算法更公平且在常见聚类目标中损失不大。"
        },
        "tokens": 912
    },
    {
        "title": "Multi-student Diffusion Distillation for Better One-step Generators",
        "link": "https://arxiv.org/abs/2410.23274",
        "description": "arXiv:2410.23274v1 Announce Type: new \nAbstract: Diffusion models achieve high-quality sample generation at the cost of a lengthy multistep inference procedure. To overcome this, diffusion distillation techniques produce student generators capable of matching or surpassing the teacher in a single step. However, the student model's inference speed is limited by the size of the teacher architecture, preventing real-time generation for computationally heavy applications. In this work, we introduce Multi-Student Distillation (MSD), a framework to distill a conditional teacher diffusion model into multiple single-step generators. Each student generator is responsible for a subset of the conditioning data, thereby obtaining higher generation quality for the same capacity. MSD trains multiple distilled students, allowing smaller sizes and, therefore, faster inference. Also, MSD offers a lightweight quality boost over single-student distillation with the same architecture. We demonstrate MSD is effective by training multiple same-sized or smaller students on single-step distillation using distribution matching and adversarial distillation techniques. With smaller students, MSD gets competitive results with faster inference for single-step generation. Using 4 same-sized students, MSD sets a new state-of-the-art for one-step image generation: FID 1.20 on ImageNet-64x64 and 8.20 on zero-shot COCO2014.",
        "published": "2024-10-31 04:00:00",
        "id": "f0f584f8-e8f2-4447-951f-367fa234dee2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出多学生扩散蒸馏（MSD）框架，将条件教师扩散模型蒸馏为多个单步生成器，可提高生成质量和推理速度，在单步图像生成上取得新成果。"
        },
        "tokens": 865
    },
    {
        "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
        "link": "https://arxiv.org/abs/2410.23277",
        "description": "arXiv:2410.23277v1 Announce Type: new \nAbstract: Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io",
        "published": "2024-10-31 04:00:00",
        "id": "5aff3e3a-208d-40ad-883c-3dda0cd8e368",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "SlowFast - VGen论文提出一种双速学习系统用于动作驱动的长视频生成，通过慢学习和快学习策略结合，还提出慢 - 快学习循环算法，实验表明其在相关指标上优于基线且在长视频中保持一致性。"
        },
        "tokens": 965
    },
    {
        "title": "OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction",
        "link": "https://arxiv.org/abs/2410.23278",
        "description": "arXiv:2410.23278v1 Announce Type: new \nAbstract: In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.",
        "published": "2024-10-31 04:00:00",
        "id": "eb6a44be-15b9-475b-8bac-1d43bf8142cd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出OpenSatMap卫星数据集，有细粒度实例级注释、高分辨率图像、数据多样性高且规模大，与其他数据集对齐，为基于卫星的地图构建和自动驾驶等下游任务提供高质量基准。"
        },
        "tokens": 811
    },
    {
        "title": "RelationBooth: Towards Relation-Aware Customized Object Generation",
        "link": "https://arxiv.org/abs/2410.23280",
        "description": "arXiv:2410.23280v1 Announce Type: new \nAbstract: Customized image generation is crucial for delivering personalized content based on user-provided image prompts, aligning large-scale text-to-image diffusion models with individual needs. However, existing models often overlook the relationships between customized objects in generated images. Instead, this work addresses that gap by focusing on relation-aware customized image generation, which aims to preserve the identities from image prompts while maintaining the predicate relations described in text prompts. Specifically, we introduce RelationBooth, a framework that disentangles identity and relation learning through a well-curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges: generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on three benchmarks demonstrate the superiority of RelationBooth in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.",
        "published": "2024-10-31 04:00:00",
        "id": "bf748381-30f8-44a9-93df-b376348a0563",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出RelationBooth框架用于关系感知的定制图像生成，通过特定数据集解耦身份和关系学习，有两个关键模块解决相关挑战，在基准测试中表现优异，将公开源代码和训练模型。"
        },
        "tokens": 865
    },
    {
        "title": "Robot Policy Learning with Temporal Optimal Transport Reward",
        "link": "https://arxiv.org/abs/2410.21795",
        "description": "arXiv:2410.21795v1 Announce Type: cross \nAbstract: Reward specification is one of the most tricky problems in Reinforcement Learning, which usually requires tedious hand engineering in practice.\n  One promising approach to tackle this challenge is to adopt existing expert video demonstrations for policy learning.\n  Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations.\n  For example, reward labeling via Optimal Transport (OT) has been shown to be an effective strategy to generate a proxy reward by measuring the alignment between the robot trajectory and the expert demonstrations.\n  However, previous work mostly overlooks that the OT reward is invariant to temporal order information, which could bring extra noise to the reward signal.\n  To address this issue, in this paper, we introduce the Temporal Optimal Transport (TemporalOT) reward to incorporate temporal order information for learning a more accurate OT-based proxy reward.\n  Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method.\n  Code is available at: https://github.com/fuyw/TemporalOT",
        "published": "2024-10-31 04:00:00",
        "id": "05aae2e5-3df5-4cac-a1f2-fad41e12fc6b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章介绍了强化学习中奖励指定是棘手问题，现有从专家视频演示中学习机器人策略的工作，但之前忽视了最优传输奖励对时间顺序信息的不变性会带来噪声，本文提出了时间最优传输奖励以解决该问题，且在Meta - world基准任务中的实验验证了其有效性，代码已开源。"
        },
        "tokens": 838
    },
    {
        "title": "Provable acceleration for diffusion models under minimal assumptions",
        "link": "https://arxiv.org/abs/2410.23285",
        "description": "arXiv:2410.23285v1 Announce Type: new \nAbstract: While score-based diffusion models have achieved exceptional sampling quality, their sampling speeds are often limited by the high computational burden of score function evaluations. Despite the recent remarkable empirical advances in speeding up the score-based samplers, theoretical understanding of acceleration techniques remains largely limited. To bridge this gap, we propose a novel training-free acceleration scheme for stochastic samplers. Under minimal assumptions -- namely, $L^2$-accurate score estimates and a finite second-moment condition on the target distribution -- our accelerated sampler provably achieves $\\varepsilon$-accuracy in total variation within $\\widetilde{O}(d^{5/4}/\\sqrt{\\varepsilon})$ iterations, thereby significantly improving upon the $\\widetilde{O}(d/\\varepsilon)$ iteration complexity of standard score-based samplers. Notably, our convergence theory does not rely on restrictive assumptions on the target distribution or higher-order score estimation guarantees.",
        "published": "2024-10-31 04:00:00",
        "id": "91dd41f6-6ebf-4612-b2bf-d2e9468727be",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种无需训练的随机采样器加速方案，在最小假设下，加速采样器在总变差中的ε - 精度迭代次数显著优于标准基于分数的采样器，收敛理论不依赖于对目标分布或高阶分数估计保证的限制假设。"
        },
        "tokens": 796
    },
    {
        "title": "ReferEverything: Towards Segmenting Everything We Can Speak of in Videos",
        "link": "https://arxiv.org/abs/2410.23287",
        "description": "arXiv:2410.23287v1 Announce Type: new \nAbstract: We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.",
        "published": "2024-10-31 04:00:00",
        "id": "2a753014-28db-414e-872d-7464190ee03d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出REM框架用于在视频中按自然语言描述分割多种概念，利用视频扩散模型的视觉 - 语言表征，经微调可准确分割跟踪稀有和未见对象，还能推广到非对象动态概念，在相关数据集上表现出色。"
        },
        "tokens": 817
    },
    {
        "title": "Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards",
        "link": "https://arxiv.org/abs/2410.23289",
        "description": "arXiv:2410.23289v1 Announce Type: new \nAbstract: Training robots directly from human videos is an emerging area in robotics and computer vision. While there has been notable progress with two-fingered grippers, learning autonomous tasks for multi-fingered robot hands in this way remains challenging. A key reason for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand due to morphology differences. In this work, we present HuDOR, a technique that enables online fine-tuning of policies by directly computing rewards from human videos. Importantly, this reward function is built using object-oriented trajectories derived from off-the-shelf point trackers, providing meaningful learning signals despite the morphology gap and visual differences between human and robot hands. Given a single video of a human solving a task, such as gently opening a music box, HuDOR enables our four-fingered Allegro hand to learn the task with just an hour of online interaction. Our experiments across four tasks show that HuDOR achieves a 4x improvement over baselines. Code and videos are available on our website, https://object-rewards.github.io.",
        "published": "2024-10-31 04:00:00",
        "id": "8591dd20-a8f9-4fbf-8775-617f57467a00",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出HuDOR技术，可从人类视频直接计算奖励来在线微调策略，让四指Allegro手通过一小时在线交互就能学习任务，实验显示该技术比基线提高4倍。"
        },
        "tokens": 829
    },
    {
        "title": "DisCo: Distributed Contact-Rich Trajectory Optimization for Forceful Multi-Robot Collaboration",
        "link": "https://arxiv.org/abs/2410.23283",
        "description": "arXiv:2410.23283v1 Announce Type: new \nAbstract: We present DisCo, a distributed algorithm for contact-rich, multi-robot tasks. DisCo is a distributed contact-implicit trajectory optimization algorithm, which allows a group of robots to optimize a time sequence of forces to objects and to their environment to accomplish tasks such as collaborative manipulation, robot team sports, and modular robot locomotion. We build our algorithm on a variant of the Alternating Direction Method of Multipliers (ADMM), where each robot computes its own contact forces and contact-switching events from a smaller single-robot, contact-implicit trajectory optimization problem, while cooperating with other robots through dual variables, enforcing constraints between robots. Each robot iterates between solving its local problem, and communicating over a wireless mesh network to enforce these consistency constraints with its neighbors, ultimately converging to a coordinated plan for the group. The local problems solved by each robot are significantly less challenging than a centralized problem with all robots' contact forces and switching events, improving the computational efficiency, while also preserving the privacy of some aspects of each robot's operation. We demonstrate the effectiveness of our algorithm in simulations of collaborative manipulation, multi-robot team sports scenarios, and in modular robot locomotion, where DisCo achieves $3$x higher success rates with a 2.5x to 5x faster computation time. Further, we provide results of hardware experiments on a modular truss robot, with three collaborating truss nodes planning individually while working together to produce a punctuated rolling-gate motion of the composite structure. Videos are available on the project page: https://disco-opt.github.io.",
        "published": "2024-10-31 04:00:00",
        "id": "77363620-3dbc-48d0-8ada-6d979de15730",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出名为DisCo的分布式算法，用于多机器人接触丰富型任务，基于ADMM变体构建，各机器人通过求解局部问题并通信以达成协作，经模拟和硬件实验证明其有效性。"
        },
        "tokens": 928
    },
    {
        "title": "Sing it, Narrate it: Quality Musical Lyrics Translation",
        "link": "https://arxiv.org/abs/2410.22066",
        "description": "arXiv:2410.22066v1 Announce Type: cross \nAbstract: Translating lyrics for musicals presents unique challenges due to the need to ensure high translation quality while adhering to singability requirements such as length and rhyme. Existing song translation approaches often prioritize these singability constraints at the expense of translation quality, which is crucial for musicals. This paper aims to enhance translation quality while maintaining key singability features. Our method consists of three main components. First, we create a dataset to train reward models for the automatic evaluation of translation quality. Second, to enhance both singability and translation quality, we implement a two-stage training process with filtering techniques. Finally, we introduce an inference-time optimization framework for translating entire songs. Extensive experiments, including both automatic and human evaluations, demonstrate significant improvements over baseline methods and validate the effectiveness of each component in our approach.",
        "published": "2024-10-31 04:00:00",
        "id": "27059dcd-fb52-47d1-b960-ab144f821a54",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对音乐剧歌词翻译存在的问题，本文提出包含创建数据集训练奖励模型、两阶段训练加过滤技术、推理时优化框架三个主要部分的方法，经自动和人工评估验证其有效性。"
        },
        "tokens": 762
    },
    {
        "title": "MAPUNetR: A Hybrid Vision Transformer and U-Net Architecture for Efficient and Interpretable Medical Image Segmentation",
        "link": "https://arxiv.org/abs/2410.22223",
        "description": "arXiv:2410.22223v1 Announce Type: cross \nAbstract: Medical image segmentation is pivotal in healthcare, enhancing diagnostic accuracy, informing treatment strategies, and tracking disease progression. This process allows clinicians to extract critical information from visual data, enabling personalized patient care. However, developing neural networks for segmentation remains challenging, especially when preserving image resolution, which is essential in detecting subtle details that influence diagnoses. Moreover, the lack of transparency in these deep learning models has slowed their adoption in clinical practice. Efforts in model interpretability are increasingly focused on making these models' decision-making processes more transparent. In this paper, we introduce MAPUNetR, a novel architecture that synergizes the strengths of transformer models with the proven U-Net framework for medical image segmentation. Our model addresses the resolution preservation challenge and incorporates attention maps highlighting segmented regions, increasing accuracy and interpretability. Evaluated on the BraTS 2020 dataset, MAPUNetR achieved a dice score of 0.88 and a dice coefficient of 0.92 on the ISIC 2018 dataset. Our experiments show that the model maintains stable performance and potential as a powerful tool for medical image segmentation in clinical practice.",
        "published": "2024-10-31 04:00:00",
        "id": "3054400b-d1bf-4b2f-9050-2b972324267e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出用于医学图像分割的MAPUNetR架构，结合了Transformer模型和U - Net框架的优势，在特定数据集上取得较好成绩，解决了分辨率保存挑战并提高了可解释性。"
        },
        "tokens": 851
    },
    {
        "title": "The $s$-Energy and Its Applications",
        "link": "https://arxiv.org/abs/2410.22341",
        "description": "arXiv:2410.22341v1 Announce Type: cross \nAbstract: Averaging dynamics drives countless processes in physics, biology, engineering, and the social sciences. In recent years, the $s$-energy has emerged as a useful tool for bounding the convergence rates of time-varying averaging systems. We derive new bounds on the $s$-energy, which we use to resolve a number of open questions in the areas of bird flocking, opinion dynamics, and distributed motion coordination. We also use our results to provide a theoretical validation for the idea of the \"Overton Window\" as an attracting manifold of viable group opinions. Our new bounds on the $s$-energy highlight its dependency on the connectivity of the underlying networks. In this vein, we use the $s$-energy to explain the exponential gap in the convergence rates of stationary and time-varying consensus systems.",
        "published": "2024-10-31 04:00:00",
        "id": "90e2df63-3a94-40b8-992b-8ae12f6d3232",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "新的关于s -能量的边界被推导出来，用于解决鸟类集群、舆论动态和分布式运动协调等领域的问题，并为'奥弗顿之窗'概念提供理论验证，同时还解释了固定和时变一致性系统收敛速度的指数差距。"
        },
        "tokens": 779
    },
    {
        "title": "Solve Mismatch Problem in Compressed Sensing",
        "link": "https://arxiv.org/abs/2410.22354",
        "description": "arXiv:2410.22354v1 Announce Type: cross \nAbstract: This article proposes a novel algorithm for solving mismatch problem in compressed sensing. Its core is to transform mismatch problem into matched by constructing a new measurement matrix to match measurement value under unknown measurement matrix. Therefore, we propose mismatch equation and establish two types of algorithm based on it, which are matched solution of unknown measurement matrix and calibration of unknown measurement matrix. Experiments have shown that when under low gaussian noise levels, the constructed measurement matrix can transform the mismatch problem into matched and recover original images. The code is available: https://github.com/yanglebupt/mismatch-solution",
        "published": "2024-10-31 04:00:00",
        "id": "56b30d55-afc4-4dc0-97b0-00ad1f54e010",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种解决压缩感知中失配问题的新算法，构建新测量矩阵将失配问题转换为匹配问题，并建立两类算法，实验表明在低高斯噪声水平下可恢复原始图像且代码已开源。"
        },
        "tokens": 722
    },
    {
        "title": "MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2410.22362",
        "description": "arXiv:2410.22362v1 Announce Type: cross \nAbstract: Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.",
        "published": "2024-10-31 04:00:00",
        "id": "e8e633a0-f100-4370-a280-36af797c3a50",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一个用于多场景遥感文本到图像生成的多模态、多地面采样距离、多场景数据集MMM - RS及基准，包含约210万文本 - 图像对，可让现成扩散模型生成多种遥感图像。"
        },
        "tokens": 936
    },
    {
        "title": "ET-Flow: Equivariant Flow-Matching for Molecular Conformer Generation",
        "link": "https://arxiv.org/abs/2410.22388",
        "description": "arXiv:2410.22388v1 Announce Type: cross \nAbstract: Predicting low-energy molecular conformations given a molecular graph is an important but challenging task in computational drug discovery. Existing state-of-the-art approaches either resort to large scale transformer-based models that diffuse over conformer fields, or use computationally expensive methods to generate initial structures and diffuse over torsion angles. In this work, we introduce Equivariant Transformer Flow (ET-Flow). We showcase that a well-designed flow matching approach with equivariance and harmonic prior alleviates the need for complex internal geometry calculations and large architectures, contrary to the prevailing methods in the field. Our approach results in a straightforward and scalable method that directly operates on all-atom coordinates with minimal assumptions. With the advantages of equivariance and flow matching, ET-Flow significantly increases the precision and physical validity of the generated conformers, while being a lighter model and faster at inference. Code is available https://github.com/shenoynikhil/ETFlow.",
        "published": "2024-10-31 04:00:00",
        "id": "d597c20c-367d-440a-9c68-ac22d8e114da",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "ET - Flow是一种等变流匹配方法，可用于分子构象生成，相比现有方法更直接、可扩展，能提高生成构象的精度和物理有效性，模型更轻量且推理速度更快，代码已开源。"
        },
        "tokens": 801
    },
    {
        "title": "Vascular Segmentation of Functional Ultrasound Images using Deep Learning",
        "link": "https://arxiv.org/abs/2410.22365",
        "description": "arXiv:2410.22365v1 Announce Type: cross \nAbstract: Segmentation of medical images is a fundamental task with numerous applications. While MRI, CT, and PET modalities have significantly benefited from deep learning segmentation techniques, more recent modalities, like functional ultrasound (fUS), have seen limited progress. fUS is a non invasive imaging method that measures changes in cerebral blood volume (CBV) with high spatio-temporal resolution. However, distinguishing arterioles from venules in fUS is challenging due to opposing blood flow directions within the same pixel. Ultrasound localization microscopy (ULM) can enhance resolution by tracking microbubble contrast agents but is invasive, and lacks dynamic CBV quantification. In this paper, we introduce the first deep learning-based segmentation tool for fUS images, capable of differentiating signals from different vascular compartments, based on ULM automatic annotation and enabling dynamic CBV quantification. We evaluate various UNet architectures on fUS images of rat brains, achieving competitive segmentation performance, with 90% accuracy, a 71% F1 score, and an IoU of 0.59, using only 100 temporal frames from a fUS stack. These results are comparable to those from tubular structure segmentation in other imaging modalities. Additionally, models trained on resting-state data generalize well to images captured during visual stimulation, highlighting robustness. This work offers a non-invasive, cost-effective alternative to ULM, enhancing fUS data interpretation and improving understanding of vessel function. Our pipeline shows high linear correlation coefficients between signals from predicted and actual compartments in both cortical and deeperregions, showcasing its ability to accurately capture blood flow dynamics.",
        "published": "2024-10-31 04:00:00",
        "id": "1bbae666-30d1-4a91-97da-5f41e48db452",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍首个基于深度学习的功能超声（fUS）图像分割工具，在大鼠脑fUS图像上评估多种UNet架构取得较好分割性能，结果可与其他成像模态中的管状结构分割结果相媲美且模型有良好泛化能力，提供了非侵入性、成本效益高的替代方案。"
        },
        "tokens": 950
    },
    {
        "title": "MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language",
        "link": "https://arxiv.org/abs/2410.22367",
        "description": "arXiv:2410.22367v1 Announce Type: cross \nAbstract: Drug discovery typically consists of multiple steps, including identifying a target protein key to a disease's etiology, validating that interacting with this target could prevent symptoms or cure the disease, discovering a small molecule or biologic therapeutic to interact with it, and optimizing the candidate molecule through a complex landscape of required properties. Drug discovery related tasks often involve prediction and generation while considering multiple entities that potentially interact, which poses a challenge for typical AI models. For this purpose we present MAMMAL - Molecular Aligned Multi-Modal Architecture and Language - a method that we applied to create a versatile multi-task foundation model ibm/biomed.omics.bl.sm.ma-ted-458m that learns from large-scale biological datasets (2 billion samples) across diverse modalities, including proteins, small molecules, and genes. We introduce a prompt syntax that supports a wide range of classification, regression, and generation tasks. It allows combining different modalities and entity types as inputs and/or outputs. Our model handles combinations of tokens and scalars and enables the generation of small molecules and proteins, property prediction, and transcriptomic lab test predictions. We evaluated the model on 11 diverse downstream tasks spanning different steps within a typical drug discovery pipeline, where it reaches new SOTA in 9 tasks and is comparable to SOTA in 2 tasks. This performance is achieved while using a unified architecture serving all tasks, in contrast to the original SOTA performance achieved using tailored architectures.\n  The model code and pretrained weights are publicly available at https://github.com/BiomedSciAI/biomed-multi-alignment and https://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m.",
        "published": "2024-10-31 04:00:00",
        "id": "7a2872a5-dedf-44f6-80bd-7a580bfd4afd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "介绍MAMMAL方法用于创建多功能多任务基础模型ibm/biomed.omics.bl.sm.ma - ted - 458m，该模型从大规模生物数据集中学习，引入支持多种任务的提示语法，经11个下游任务评估在9个任务中达到新SOTA，模型代码和预训练权重已公开。"
        },
        "tokens": 989
    },
    {
        "title": "Debiasing Alternative Data for Credit Underwriting Using Causal Inference",
        "link": "https://arxiv.org/abs/2410.22382",
        "description": "arXiv:2410.22382v1 Announce Type: cross \nAbstract: Alternative data provides valuable insights for lenders to evaluate a borrower's creditworthiness, which could help expand credit access to underserved groups and lower costs for borrowers. But some forms of alternative data have historically been excluded from credit underwriting because it could act as an illegal proxy for a protected class like race or gender, causing redlining. We propose a method for applying causal inference to a supervised machine learning model to debias alternative data so that it might be used for credit underwriting. We demonstrate how our algorithm can be used against a public credit dataset to improve model accuracy across different racial groups, while providing theoretically robust nondiscrimination guarantees.",
        "published": "2024-10-31 04:00:00",
        "id": "d841251a-a196-4efc-bbf6-a0f7fa0ac5b4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种利用因果推理对替代数据进行去偏的方法用于信贷承保，以公共信贷数据集展示该算法可提高不同种族群体的模型准确性并提供无歧视保证。"
        },
        "tokens": 727
    },
    {
        "title": "Branch-and-bound algorithm for efficient reliability analysis of general coherent systems",
        "link": "https://arxiv.org/abs/2410.22363",
        "description": "arXiv:2410.22363v1 Announce Type: cross \nAbstract: Branch and bound algorithms have been developed for reliability analysis of coherent systems. They exhibit a set of advantages; in particular, they can find a computationally efficient representation of a system failure or survival event, which can be re-used when the input probability distributions change over time or when new data is available. However, existing branch-and-bound algorithms can handle only a limited set of system performance functions, mostly network connectivity and maximum flow. Furthermore, they run redundant analyses on component vector states whose system state can be inferred from previous analysis results. This study addresses these limitations by proposing branch and bound for reliability analysis of general coherent systems} (BRC) algorithm: an algorithm that automatically finds minimal representations of failure/survival events of general coherent systems. Computational efficiency is attained by dynamically inferring importance of component events from hitherto obtained results. We demonstrate advantages of the BRC method as a real-time risk management tool by application to the Eastern Massachusetts highway benchmark network.",
        "published": "2024-10-31 04:00:00",
        "id": "e685af60-da12-40f7-b9c2-6b16ab931d45",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出一种用于一般相干系统可靠性分析的分支定界（BRC）算法，通过动态推断组件事件重要性提高计算效率，并应用于东马萨诸塞州公路基准网络展示其作为实时风险管理工具的优势。"
        },
        "tokens": 799
    },
    {
        "title": "EfficientNet with Hybrid Attention Mechanisms for Enhanced Breast Histopathology Classification: A Comprehensive Approach",
        "link": "https://arxiv.org/abs/2410.22392",
        "description": "arXiv:2410.22392v1 Announce Type: cross \nAbstract: Breast cancer histopathology image classification is crucial for early cancer detection, offering the potential to reduce mortality rates through timely diagnosis. This paper introduces a novel approach integrating Hybrid EfficientNet models with advanced attention mechanisms, including Convolutional Block Attention Module (CBAM), Self-Attention, and Deformable Attention, to enhance feature extraction and focus on critical image regions. We evaluate the performance of our models across multiple magnification scales using publicly available histopathological datasets. Our method achieves significant improvements, with accuracy reaching 98.42% at 400X magnification, surpassing several state-of-the-art models, including VGG and ResNet architectures. The results are validated using metrics such as accuracy, F1-score, precision, and recall, demonstrating the clinical potential of our model in improving diagnostic accuracy. Furthermore, the proposed method shows increased computational efficiency, making it suitable for integration into real-time diagnostic workflows.",
        "published": "2024-10-31 04:00:00",
        "id": "36fe01ef-7484-4ab1-ba7f-1cf342005851",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出一种将混合EfficientNet模型与多种注意力机制相结合的新方法用于乳腺癌组织病理学图像分类，在多放大倍数下评估模型性能，准确率达98.42%且计算效率提高，有望用于实时诊断工作流程。"
        },
        "tokens": 804
    },
    {
        "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
        "link": "https://arxiv.org/abs/2410.22448",
        "description": "arXiv:2410.22448v1 Announce Type: cross \nAbstract: Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schr\\\"odinger Bridge. We examine how different design choices affect machine and human perception.",
        "published": "2024-10-31 04:00:00",
        "id": "2d13c4da-21c2-4fea-b318-fd3069c9156f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Neural Audio Codecs用于语音生成，文中关注从粗粒度标记更好地重新合成波形的问题，研究了基于标记预测和回归的两种策略，并引入一种新方法，还探讨了不同设计选择对机器和人类感知的影响。"
        },
        "tokens": 808
    },
    {
        "title": "Explainable convolutional neural network model provides an alternative genome-wide association perspective on mutations in SARS-CoV-2",
        "link": "https://arxiv.org/abs/2410.22452",
        "description": "arXiv:2410.22452v1 Announce Type: cross \nAbstract: Identifying mutations of SARS-CoV-2 strains associated with their phenotypic changes is critical for pandemic prediction and prevention. We compared an explainable convolutional neural network (CNN) and the traditional genome-wide association study (GWAS) on the mutations associated with WHO labels of SARS-CoV-2, a proxy for virulence phenotypes. We trained a CNN classification model that can predict genomic sequences into Variants of Concern (VOCs), and then applied Shapley Additive explanations (SHAP) model to identify mutations that are important for the correct predictions. For comparison, we performed traditional GWAS to identify mutations associated with VOCs. Comparison of the two approaches shows that the explainable neural network approach can more effectively reveal known nucleotide substitutions associated with VOCs, such as those in the spike gene regions. Our results suggest that explainable neural networks for genomic sequences offer a promising alternative to the traditional genome wide analysis approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "f49eceb1-235f-4714-94e3-f01a148d1924",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过比较可解释卷积神经网络（CNN）和传统全基因组关联研究（GWAS）对新冠病毒（SARS-CoV-2）突变与毒性表型关联的研究，发现可解释神经网络能更有效揭示与变异株（VOCs）相关的核苷酸替换，为传统全基因组分析提供替代方案。"
        },
        "tokens": 821
    },
    {
        "title": "Ethical Statistical Practice and Ethical AI",
        "link": "https://arxiv.org/abs/2410.22475",
        "description": "arXiv:2410.22475v1 Announce Type: cross \nAbstract: Artificial Intelligence (AI) is a field that utilizes computing and often, data and statistics, intensively together to solve problems or make predictions. AI has been evolving with literally unbelievable speed over the past few years, and this has led to an increase in social, cultural, industrial, scientific, and governmental concerns about the ethical development and use of AI systems worldwide. The ASA has issued a statement on ethical statistical practice and AI (ASA, 2024), which echoes similar statements from other groups. Here we discuss the support for ethical statistical practice and ethical AI that has been established in long-standing human rights law and ethical practice standards for computing and statistics. There are multiple sources of support for ethical statistical practice and ethical AI deriving from these source documents, which are critical for strengthening the operationalization of the \"Statement on Ethical AI for Statistics Practitioners\". These resources are explicated for interested readers to utilize to guide their development and use of AI in, and through, their statistical practice.",
        "published": "2024-10-31 04:00:00",
        "id": "6d6c5b0f-2e3b-4181-b123-6b055c6681f9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨了人工智能中符合伦理的统计实践，阐述了其在人权法和伦理实践标准中的支持依据，以指导统计从业者对人工智能的开发和使用。"
        },
        "tokens": 794
    },
    {
        "title": "Adaptive Aggregation Weights for Federated Segmentation of Pancreas MRI",
        "link": "https://arxiv.org/abs/2410.22530",
        "description": "arXiv:2410.22530v1 Announce Type: cross \nAbstract: Federated learning (FL) enables collaborative model training across institutions without sharing sensitive data, making it an attractive solution for medical imaging tasks. However, traditional FL methods, such as Federated Averaging (FedAvg), face difficulties in generalizing across domains due to variations in imaging protocols and patient demographics across institutions. This challenge is particularly evident in pancreas MRI segmentation, where anatomical variability and imaging artifacts significantly impact performance. In this paper, we conduct a comprehensive evaluation of FL algorithms for pancreas MRI segmentation and introduce a novel approach that incorporates adaptive aggregation weights. By dynamically adjusting the contribution of each client during model aggregation, our method accounts for domain-specific differences and improves generalization across heterogeneous datasets. Experimental results demonstrate that our approach enhances segmentation accuracy and reduces the impact of domain shift compared to conventional FL methods while maintaining privacy-preserving capabilities. Significant performance improvements are observed across multiple hospitals (centers).",
        "published": "2024-10-31 04:00:00",
        "id": "314cdf79-45f6-4c1c-b203-8c7277cedb0d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文对联邦学习算法在胰腺MRI分割方面进行评估，提出含自适应聚合权重的新方法，实验证明其可提高分割精度、减少域偏移影响并保持隐私。"
        },
        "tokens": 779
    },
    {
        "title": "Privacy-Preserving Dynamic Assortment Selection",
        "link": "https://arxiv.org/abs/2410.22488",
        "description": "arXiv:2410.22488v1 Announce Type: cross \nAbstract: With the growing demand for personalized assortment recommendations, concerns over data privacy have intensified, highlighting the urgent need for effective privacy-preserving strategies. This paper presents a novel framework for privacy-preserving dynamic assortment selection using the multinomial logit (MNL) bandits model. Our approach employs a perturbed upper confidence bound method, integrating calibrated noise into user utility estimates to balance between exploration and exploitation while ensuring robust privacy protection. We rigorously prove that our policy satisfies Joint Differential Privacy (JDP), which better suits dynamic environments than traditional differential privacy, effectively mitigating inference attack risks. This analysis is built upon a novel objective perturbation technique tailored for MNL bandits, which is also of independent interest. Theoretically, we derive a near-optimal regret bound of $\\tilde{O}(\\sqrt{T})$ for our policy and explicitly quantify how privacy protection impacts regret. Through extensive simulations and an application to the Expedia hotel dataset, we demonstrate substantial performance enhancements over the benchmark method.",
        "published": "2024-10-31 04:00:00",
        "id": "1eece16d-19d8-433b-b4d0-31ed4c20598d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于多项式Logit (MNL) bandits模型的隐私保护动态分类选择框架，采用扰动上置信界方法，证明该策略满足联合差分隐私，理论上推导出近似最优遗憾界，通过模拟和应用展示性能提升。"
        },
        "tokens": 807
    },
    {
        "title": "Evaluating utility in synthetic banking microdata applications",
        "link": "https://arxiv.org/abs/2410.22519",
        "description": "arXiv:2410.22519v1 Announce Type: cross \nAbstract: Financial regulators such as central banks collect vast amounts of data, but access to the resulting fine-grained banking microdata is severely restricted by banking secrecy laws. Recent developments have resulted in mechanisms that generate faithful synthetic data, but current evaluation frameworks lack a focus on the specific challenges of banking institutions and microdata. We develop a framework that considers the utility and privacy requirements of regulators, and apply this to financial usage indices, term deposit yield curves, and credit card transition matrices. Using the Central Bank of Paraguay's data, we provide the first implementation of synthetic banking microdata using a central bank's collected information, with the resulting synthetic datasets for all three domain applications being publicly available and featuring information not yet released in statistical disclosure. We find that applications less susceptible to post-processing information loss, which are based on frequency tables, are particularly suited for this approach, and that marginal-based inference mechanisms to outperform generative adversarial network models for these applications. Our results demonstrate that synthetic data generation is a promising privacy-enhancing technology for financial regulators seeking to complement their statistical disclosure, while highlighting the crucial role of evaluating such endeavors in terms of utility and privacy requirements.",
        "published": "2024-10-31 04:00:00",
        "id": "a24c4a7e-2237-4614-9918-ec978c0f3049",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": true,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "金融监管机构数据获取受银行保密法限制，有机制生成可靠的合成数据，但缺乏针对银行机构和微数据特定挑战的评估框架，本文构建框架并应用于相关金融数据，用巴拉圭央行数据首次实现合成银行微数据，发现基于频率表的应用适合该方法，边际推理机制优于生成对抗网络模型，表明合成数据生成对金融监管机构有前景。"
        },
        "tokens": 875
    },
    {
        "title": "Towards Neural-Network-based optical temperature sensing of Semiconductor Membrane External Cavity Laser",
        "link": "https://arxiv.org/abs/2410.22528",
        "description": "arXiv:2410.22528v1 Announce Type: cross \nAbstract: A machine-learning non-contact method to determine the temperature of a laser gain medium via its laser emission with a trained few-layer neural net model is presented. The training of the feed-forward Neural Network (NN) enables the prediction of the device's properties solely from spectral data, here recorded by visible-/nearinfrared-light compact micro-spectrometers for both a diode pump laser and optically-pumped gain membrane of a semiconductor disk laser. Fiber spectrometers are used for the acquisition of large quantities of labelled intensity data, which can afterwards be used for the prediction process. Such pretrained deep NNs enable a fast, reliable and easy way to infer the temperature of a laser system such as our Membrane External Cavity Laser, at a later monitoring stage without the need of additional optical diagnostics or read-out temperature sensors. With the miniature mobile spectrometer and the remote detection ability, the temperature inference capability can be adapted for various laser diodes using transfer learning methods with pretrained models. Here, mean-square-error values for the temperature inference corresponding to sub-percent accuracy of our sensor scheme are reached, while computational cost can be saved by reducing the network depth at the here displayed cost of accuracy, as appropriate for different application scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "41310f03-2861-4a3d-958d-d4deb3783877",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于机器学习的非接触式方法，通过预训练的神经网络模型从光谱数据预测激光增益介质温度，可用于半导体膜外腔激光器，达到亚百分比精度的均方误差值且可通过迁移学习适配不同激光二极管，还可通过减少网络深度节省计算成本。"
        },
        "tokens": 876
    },
    {
        "title": "Bayesian Counterfactual Prediction Models for HIV Care Retention with Incomplete Outcome and Covariate Information",
        "link": "https://arxiv.org/abs/2410.22481",
        "description": "arXiv:2410.22481v1 Announce Type: cross \nAbstract: Like many chronic diseases, human immunodeficiency virus (HIV) is managed over time at regular clinic visits. At each visit, patient features are assessed, treatments are prescribed, and a subsequent visit is scheduled. There is a need for data-driven methods for both predicting retention and recommending scheduling decisions that optimize retention. Prediction models can be useful for estimating retention rates across a range of scheduling options. However, training such models with electronic health records (EHR) involves several complexities. First, formal causal inference methods are needed to adjust for observed confounding when estimating retention rates under counterfactual scheduling decisions. Second, competing events such as death preclude retention, while censoring events render retention missing. Third, inconsistent monitoring of features such as viral load and CD4 count lead to covariate missingness. This paper presents an all-in-one approach for both predicting HIV retention and optimizing scheduling while accounting for these complexities. We formulate and identify causal retention estimands in terms of potential return-time under a hypothetical scheduling decision. Flexible Bayesian approaches are used to model the observed return-time distribution while accounting for competing and censoring events and form posterior point and uncertainty estimates for these estimands. We address the urgent need for data-driven decision support in HIV care by applying our method to EHR from the Academic Model Providing Access to Healthcare (AMPATH) - a consortium of clinics that treat HIV in Western Kenya.",
        "published": "2024-10-31 04:00:00",
        "id": "29cf085e-58bd-42ea-80a9-f7d8121509ff",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": true,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种综合方法用于预测HIV患者留存率并优化日程安排，采用贝叶斯方法处理电子健康记录数据中的复杂情况，应用于肯尼亚西部的AMPATH诊所的电子健康记录。"
        },
        "tokens": 888
    },
    {
        "title": "Deep Priors for Video Quality Prediction",
        "link": "https://arxiv.org/abs/2410.22566",
        "description": "arXiv:2410.22566v1 Announce Type: cross \nAbstract: In this work, we designed a completely blind video quality assessment algorithm using the deep video prior. This work mainly explores the utility of deep video prior in estimating the visual quality of the video. In our work, we have used a single distorted video and a reference video pair to learn the deep video prior. At inference time, the learned deep prior is used to restore the original videos from the distorted videos. The ability of learned deep video prior to restore the original video from the distorted video is measured to quantify distortion in the video. Our hypothesis is that the learned deep video prior fails in restoring the highly distorted videos. The restoring ability of deep video prior is proportional to the distortion present in the video. Therefore, we propose to use the distance between the distorted video and the restored video as the perceptual quality of the video. Our algorithm is trained using a single video pair and it does not need any labelled data. We show that our proposed algorithm outperforms the existing unsupervised video quality assessment algorithms in terms of LCC and SROCC on a synthetically distorted video quality assessment dataset.",
        "published": "2024-10-31 04:00:00",
        "id": "13490cb6-fc9c-4257-afba-9e0182fb24f3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种使用深度视频先验的完全盲视频质量评估算法，利用单个失真视频与参考视频对学习深度视频先验，以恢复视频的能力衡量视频失真，算法无需标记数据且在合成失真视频质量评估数据集上性能优于现有无监督算法。"
        },
        "tokens": 830
    },
    {
        "title": "Fast Deep Hedging with Second-Order Optimization",
        "link": "https://arxiv.org/abs/2410.22568",
        "description": "arXiv:2410.22568v1 Announce Type: cross \nAbstract: Hedging exotic options in presence of market frictions is an important risk management task. Deep hedging can solve such hedging problems by training neural network policies in realistic simulated markets. Training these neural networks may be delicate and suffer from slow convergence, particularly for options with long maturities and complex sensitivities to market parameters. To address this, we propose a second-order optimization scheme for deep hedging. We leverage pathwise differentiability to construct a curvature matrix, which we approximate as block-diagonal and Kronecker-factored to efficiently precondition gradients. We evaluate our method on a challenging and practically important problem: hedging a cliquet option on a stock with stochastic volatility by trading in the spot and vanilla options. We find that our second-order scheme can optimize the policy in 1/4 of the number of steps that standard adaptive moment-based optimization takes.",
        "published": "2024-10-31 04:00:00",
        "id": "dedb0103-6950-415e-a7fa-d3e47d3b6b29",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "为解决含市场摩擦下的奇异期权套期保值中神经网络训练收敛慢的问题，提出二阶优化方案，经在股票棘轮期权套期保值问题上评估，该方案优化步数仅为标准自适应矩优化的1/4。"
        },
        "tokens": 783
    },
    {
        "title": "Orb: A Fast, Scalable Neural Network Potential",
        "link": "https://arxiv.org/abs/2410.22570",
        "description": "arXiv:2410.22570v1 Announce Type: cross \nAbstract: We introduce Orb, a family of universal interatomic potentials for atomistic modelling of materials. Orb models are 3-6 times faster than existing universal potentials, stable under simulation for a range of out of distribution materials and, upon release, represented a 31% reduction in error over other methods on the Matbench Discovery benchmark. We explore several aspects of foundation model development for materials, with a focus on diffusion pretraining. We evaluate Orb as a model for geometry optimization, Monte Carlo and molecular dynamics simulations.",
        "published": "2024-10-31 04:00:00",
        "id": "25275de7-3b7f-4f12-8201-5cced998e118",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Orb作为原子间势的家族模型被引入，它比现有通用势更快、更稳定且误差更小，文章探讨其基础模型开发的多方面，重点是扩散预训练，并评估其在多种模拟中的应用。"
        },
        "tokens": 713
    },
    {
        "title": "Continuous-Time Line-of-Sight Constrained Trajectory Planning for 6-Degree of Freedom Systems",
        "link": "https://arxiv.org/abs/2410.22596",
        "description": "arXiv:2410.22596v1 Announce Type: cross \nAbstract: Perception algorithms are ubiquitous in modern autonomy stacks, providing necessary environmental information to operate in the real world. Many of these algorithms depend on the visibility of keypoints, which must remain within the robot's line-of-sight (LoS), for reliable operation. This paper tackles the challenge of maintaining LoS on such keypoints during robot movement. We propose a novel method that addresses these issues by ensuring applicability to various sensor footprints, adaptability to arbitrary nonlinear dynamics, and constant enforcement of LoS throughout the robot's path. Through our experiments, we show that the proposed approach achieves significantly reduced LoS violation and runtime compared to existing state-of-the-art methods in several representative and challenging scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "e26ce803-152c-4cda-86de-448f3595903f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新方法确保机器人运动时保持关键点的视线，实验表明该方法在减少视线违规和运行时间方面优于现有方法。"
        },
        "tokens": 738
    },
    {
        "title": "Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse",
        "link": "https://arxiv.org/abs/2410.22598",
        "description": "arXiv:2410.22598v1 Announce Type: cross \nAbstract: Machine learning models are often used to automate or support decisions in applications such as lending and hiring. In such settings, consumer protection rules mandate that we provide a list of \"principal reasons\" to consumers who receive adverse decisions. In practice, lenders and employers identify principal reasons by returning the top-scoring features from a feature attribution method. In this work, we study how such practices align with one of the underlying goals of consumer protection - recourse - i.e., educating individuals on how they can attain a desired outcome. We show that standard attribution methods can mislead individuals by highlighting reasons without recourse - i.e., by presenting consumers with features that cannot be changed to achieve recourse. We propose to address these issues by scoring features on the basis of responsiveness - i.e., the probability that an individual can attain a desired outcome by changing a specific feature. We develop efficient methods to compute responsiveness scores for any model and any dataset under complex actionability constraints. We present an extensive empirical study on the responsiveness of explanations in lending and demonstrate how responsiveness scores can be used to construct feature-highlighting explanations that lead to recourse and mitigate harm by flagging instances with fixed predictions.",
        "published": "2024-10-31 04:00:00",
        "id": "b1b63b8d-7189-4773-b852-7ef62e8eef9d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": true,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究机器学习模型用于决策场景时，指出标准归因方法在提供主要原因时存在误导个体的问题，提出基于响应性对特征进行评分以实现诉求，并开发计算方法，通过借贷方面的实证研究展示该评分可构建有助于诉求的解释。"
        },
        "tokens": 850
    },
    {
        "title": "Efficient Feature Extraction and Classification Architecture for MRI-Based Brain Tumor Detection",
        "link": "https://arxiv.org/abs/2410.22619",
        "description": "arXiv:2410.22619v1 Announce Type: cross \nAbstract: Uncontrolled cell division in the brain is what gives rise to brain tumors. If the tumor size increases by more than half, there is little hope for the patient's recovery. This emphasizes the need of rapid and precise brain tumor diagnosis. When it comes to analyzing, diagnosing, and planning therapy for brain tumors, MRI imaging plays a crucial role. A brain tumor's development history is crucial information for doctors to have. When it comes to distinguishing between human soft tissues, MRI scans are superior. In order to get reliable classification results from MRI scans quickly, deep learning is one of the most practical methods. Early human illness diagnosis has been demonstrated to be more accurate when deep learning methods are used. In the case of diagnosing a brain tumor, when even a little misdiagnosis might have serious consequences, accuracy is especially important. Disclosure of brain tumors in medical images is still a difficult task. Brain MRIs are notoriously imprecise in revealing the presence or absence of tumors. Using MRI scans of the brain, a Convolutional Neural Network (CNN) was trained to identify the presence of a tumor in this research. Results from the CNN model showed an accuracy of 99.17%. The CNN model's characteristics were also retrieved. In order to evaluate the CNN model's capability for processing images, we applied the features via the following machine learning models: KNN, Logistic regression, SVM, Random Forest, Naive Bayes, and Perception. CNN and machine learning models were also evaluated using the standard metrics of Precision, Recall, Specificity, and F1 score. The significance of the doctor's diagnosis enhanced the accuracy of the CNN model's assistance in identifying the existence of tumor and treating the patient.",
        "published": "2024-10-31 04:00:00",
        "id": "877ab6a7-5ec0-4a65-8e58-e7687e7c45f2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究利用脑MRI图像训练卷积神经网络识别脑肿瘤，模型准确率达99.17%，并通过多种机器学习模型评估该CNN模型处理图像的能力，同时阐述了脑肿瘤早期准确诊断的重要性。"
        },
        "tokens": 960
    },
    {
        "title": "st-DTPM: Spatial-Temporal Guided Diffusion Transformer Probabilistic Model for Delayed Scan PET Image Prediction",
        "link": "https://arxiv.org/abs/2410.22732",
        "description": "arXiv:2410.22732v1 Announce Type: cross \nAbstract: PET imaging is widely employed for observing biological metabolic activities within the human body. However, numerous benign conditions can cause increased uptake of radiopharmaceuticals, confounding differentiation from malignant tumors. Several studies have indicated that dual-time PET imaging holds promise in distinguishing between malignant and benign tumor processes. Nevertheless, the hour-long distribution period of radiopharmaceuticals post-injection complicates the determination of optimal timing for the second scan, presenting challenges in both practical applications and research. Notably, we have identified that delay time PET imaging can be framed as an image-to-image conversion problem. Motivated by this insight, we propose a novel spatial-temporal guided diffusion transformer probabilistic model (st-DTPM) to solve dual-time PET imaging prediction problem. Specifically, this architecture leverages the U-net framework that integrates patch-wise features of CNN and pixel-wise relevance of Transformer to obtain local and global information. And then employs a conditional DDPM model for image synthesis. Furthermore, on spatial condition, we concatenate early scan PET images and noisy PET images on every denoising step to guide the spatial distribution of denoising sampling. On temporal condition, we convert diffusion time steps and delay time to a universal time vector, then embed it to each layer of model architecture to further improve the accuracy of predictions. Experimental results demonstrated the superiority of our method over alternative approaches in preserving image quality and structural information, thereby affirming its efficacy in predictive task.",
        "published": "2024-10-31 04:00:00",
        "id": "dd344009-b6f8-4fc2-ba34-3435bb5bf2dd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的时空引导扩散变换器概率模型（st - DTPM）解决双时相PET成像预测问题，该模型利用U - net框架并采用条件DDPM模型进行图像合成，实验结果显示该方法在预测任务中的有效性。"
        },
        "tokens": 910
    },
    {
        "title": "An Overview of Causal Inference using Kernel Embeddings",
        "link": "https://arxiv.org/abs/2410.22754",
        "description": "arXiv:2410.22754v1 Announce Type: cross \nAbstract: Kernel embeddings have emerged as a powerful tool for representing probability measures in a variety of statistical inference problems. By mapping probability measures into a reproducing kernel Hilbert space (RKHS), kernel embeddings enable flexible representations of complex relationships between variables. They serve as a mechanism for efficiently transferring the representation of a distribution downstream to other tasks, such as hypothesis testing or causal effect estimation. In the context of causal inference, the main challenges include identifying causal associations and estimating the average treatment effect from observational data, where confounding variables may obscure direct cause-and-effect relationships. Kernel embeddings provide a robust nonparametric framework for addressing these challenges. They allow for the representations of distributions of observational data and their seamless transformation into representations of interventional distributions to estimate relevant causal quantities. We overview recent research that leverages the expressiveness of kernel embeddings in tandem with causal inference.",
        "published": "2024-10-31 04:00:00",
        "id": "b4f14930-563c-4df3-a629-2f7bde72828b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章概述了利用核嵌入进行因果推断的研究，核嵌入可将概率测度映射到再生核希尔伯特空间以灵活表示变量间关系，为因果推断中的挑战提供了非参数框架。"
        },
        "tokens": 775
    },
    {
        "title": "Inexact Augmented Lagrangian Methods for Conic Programs: Quadratic Growth and Linear Convergence",
        "link": "https://arxiv.org/abs/2410.22683",
        "description": "arXiv:2410.22683v1 Announce Type: cross \nAbstract: Augmented Lagrangian Methods (ALMs) are widely employed in solving constrained optimizations, and some efficient solvers are developed based on this framework. Under the quadratic growth assumption, it is known that the dual iterates and the Karush-Kuhn-Tucker (KKT) residuals of ALMs applied to semidefinite programs (SDPs) converge linearly. In contrast, the convergence rate of the primal iterates has remained elusive. In this paper, we resolve this challenge by establishing new $\\textit{quadratic growth}$ and $\\textit{error bound}$ properties for primal and dual SDPs under the strict complementarity condition. Our main results reveal that both primal and dual iterates of the ALMs converge linearly contingent solely upon the assumption of strict complementarity and a bounded solution set. This finding provides a positive answer to an open question regarding the asymptotically linear convergence of the primal iterates of ALMs applied to semidefinite optimization.",
        "published": "2024-10-31 04:00:00",
        "id": "3d5d70fa-bdee-4b68-bf72-49ab1de904e5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "在严格互补条件下，通过建立新的二次增长和误差界性质，证明增广拉格朗日方法应用于半定规划时，原问题和对偶问题的迭代均线性收敛，回答了关于原问题迭代渐近线性收敛的开放性问题。"
        },
        "tokens": 822
    },
    {
        "title": "Amplitude Expansion Phase Field Crystal (APFC) Modeling based Efficient Dislocation Simulations using Fourier Pseudospectral Method",
        "link": "https://arxiv.org/abs/2410.22720",
        "description": "arXiv:2410.22720v1 Announce Type: cross \nAbstract: Crystalline defects play a critical role in determining the properties of crystalline solids, underscoring the need for accurate computational methods to study them. Lattice deformation in dislocation simulations, which involves changes in atomic positions, can be described either microscopically by specific atomic configurations or macroscopically by continuum elasticity, each with inherent limitations. The complex amplitude expansion of the phase field crystal (APFC) model provides a mesoscopic approach that bridges these scales. In this paper, we introduce a Fourier pseudospectral method for efficiently solving the APFC model in the context of crystalline defect simulations. This study marks the first application of the Fourier pseudospectral method to the APFC model. The method fully exploits the system's periodicity and facilitates the implementation of periodic boundary conditions, thanks to its high accuracy and computational efficiency. Numerical experiments conducted on two-dimensional triangular lattices and three-dimensional body-centered cubic lattices for edge dislocation geometry optimization have produced strain field images that align well with by continuum elasticity predictions. The findings demonstrate the potential of the APFC model to accurately capture the complex strain fields associated with dislocations at the mesoscopic scales, a key step toward modeling more intricate crystalline defect structures and dynamics.",
        "published": "2024-10-31 04:00:00",
        "id": "ecf57020-8481-468c-b913-b37840d4b3ee",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "介绍一种基于傅里叶伪谱方法的振幅扩展相场晶体（APFC）模型用于晶体缺陷模拟中的位错模拟，该方法首次应用于APFC模型，数值实验表明APFC模型能在介观尺度准确捕捉位错相关应变场。"
        },
        "tokens": 869
    },
    {
        "title": "SleepNetZero: Zero-Burden Zero-Shot Reliable Sleep Staging With Neural Networks Based on Ballistocardiograms",
        "link": "https://arxiv.org/abs/2410.22646",
        "description": "arXiv:2410.22646v1 Announce Type: cross \nAbstract: Sleep monitoring plays a crucial role in maintaining good health, with sleep staging serving as an essential metric in the monitoring process. Traditional methods, utilizing medical sensors like EEG and ECG, can be effective but often present challenges such as unnatural user experience, complex deployment, and high costs. Ballistocardiography~(BCG), a type of piezoelectric sensor signal, offers a non-invasive, user-friendly, and easily deployable alternative for long-term home monitoring. However, reliable BCG-based sleep staging is challenging due to the limited sleep monitoring data available for BCG. A restricted training dataset prevents the model from generalization across populations. Additionally, transferring to BCG faces difficulty ensuring model robustness when migrating from other data sources. To address these issues, we introduce SleepNetZero, a zero-shot learning based approach for sleep staging. To tackle the generalization challenge, we propose a series of BCG feature extraction methods that align BCG components with corresponding respiratory, cardiac, and movement channels in PSG. This allows models to be trained on large-scale PSG datasets that are diverse in population. For the migration challenge, we employ data augmentation techniques, significantly enhancing generalizability. We conducted extensive training and testing on large datasets~(12393 records from 9637 different subjects), achieving an accuracy of 0.803 and a Cohen's Kappa of 0.718. ZeroSleepNet was also deployed in real prototype~(monitoring pads) and tested in actual hospital settings~(265 users), demonstrating an accuracy of 0.697 and a Cohen's Kappa of 0.589. To the best of our knowledge, this work represents the first known reliable BCG-based sleep staging effort and marks a significant step towards in-home health monitoring.",
        "published": "2024-10-31 04:00:00",
        "id": "adf96bbe-d9e2-4eb7-8da2-4f80a411414f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "为解决基于BCG的睡眠分期因数据有限存在的挑战，提出SleepNetZero零样本学习方法，经大量数据训练测试并在实际医院测试，是首个可靠的基于BCG的睡眠分期成果。"
        },
        "tokens": 983
    },
    {
        "title": "Property Estimation in Geotechnical Databases Using Labeled Random Finite Sets",
        "link": "https://arxiv.org/abs/2410.22659",
        "description": "arXiv:2410.22659v1 Announce Type: cross \nAbstract: The sufficiency of accurate data is a core element in data-centric geotechnics. However, geotechnical datasets are essentially uncertain, whereupon engineers have difficulty with obtaining precise information for making decisions. This challenge is more apparent when the performance of data-driven technologies solely relies on imperfect databases or even when it is sometimes difficult to investigate sites physically. This paper introduces geotechnical property estimation from noisy and incomplete data within the labeled random finite set (LRFS) framework. We leverage the ability of the generalized labeled multi-Bernoulli (GLMB) filter, a fundamental solution for multi-object estimation, to deal with measurement uncertainties from a Bayesian perspective. In particular, this work focuses on the similarity between LRFSs and big indirect data (BID) in geotechnics as those characteristics resemble each other, which enables GLMB filtering to be exploited potentially for data-centric geotechnical engineering. Experiments for numerical study are conducted to evaluate the proposed method using a public clay database.",
        "published": "2024-10-31 04:00:00",
        "id": "29f2de1e-8118-41b6-9c61-31fb375819eb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出在标记随机有限集框架内从有噪声和不完整数据中进行岩土工程特性估计，利用广义标记多伯努利滤波器处理测量不确定性，通过数值实验用公共黏土数据库评估该方法。"
        },
        "tokens": 810
    },
    {
        "title": "Unfolding Target Detection with State Space Model",
        "link": "https://arxiv.org/abs/2410.22774",
        "description": "arXiv:2410.22774v1 Announce Type: cross \nAbstract: Target detection is a fundamental task in radar sensing, serving as the precursor to any further processing for various applications. Numerous detection algorithms have been proposed. Classical methods based on signal processing, e.g., the most widely used CFAR, are challenging to tune and sensitive to environmental conditions. Deep learning-based methods can be more accurate and robust, yet usually lack interpretability and physical relevance. In this paper, we introduce a novel method that combines signal processing and deep learning by unfolding the CFAR detector with a state space model architecture. By reserving the CFAR pipeline yet turning its sophisticated configurations into trainable parameters, our method achieves high detection performance without manual parameter tuning, while preserving model interpretability. We implement a lightweight model of only 260K parameters and conduct real-world experiments for human target detection using FMCW radars. The results highlight the remarkable performance of the proposed method, outperforming CFAR and its variants by 10X in detection rate and false alarm rate. Our code is open-sourced here: https://github.com/aiot-lab/NeuroDet.",
        "published": "2024-10-31 04:00:00",
        "id": "e3161655-aeac-4164-9aa7-4944f370e830",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种将信号处理与深度学习结合的目标检测新方法，通过展开CFAR探测器与状态空间模型架构，实现高性能检测且无需手动调参，代码已开源，实验结果显示其性能远超CFAR及其变体。"
        },
        "tokens": 829
    },
    {
        "title": "Machine Learning Nonadiabatic Dynamics: Eliminating Phase Freedom of Nonadiabatic Couplings with the State-Intraction State-Averaged Spin-Restricted Ensemble-Referenced Kohn-Sham Approach",
        "link": "https://arxiv.org/abs/2410.22801",
        "description": "arXiv:2410.22801v1 Announce Type: cross \nAbstract: Excited-state molecular dynamics (ESMD) simulations near conical intersections (CIs) pose significant challenges when using machine learning potentials (MLPs). Although MLPs have gained recognition for their integration into mixed quantum-classical (MQC) methods, such as trajectory surface hopping (TSH), and their capacity to model correlated electron-nuclear dynamics efficiently, difficulties persist in managing nonadiabatic dynamics. Specifically, singularities at CIs and double-valued coupling elements result in discontinuities that disrupt the smoothness of predictive functions. Partial solutions have been provided by learning diabatic Hamiltonians with phaseless loss functions to these challenges. However, a definitive method for addressing the discontinuities caused by CIs and double-valued coupling elements has yet to be developed. Here, we introduce the phaseless coupling term, $\\Delta^2$, derived from the square of the off-diagonal elements of the diabatic Hamiltonian in the SSR(2,2) formalism. This approach improves the stability and accuracy of the MLP model by addressing the issues arising from CI singularities and double-valued coupling functions. We apply this method to the penta-2,4-dieniminium cation (PSB3), demonstrating its effectiveness in improving MLP training for ML-based nonadiabatic dynamics. Our results show that the $\\Delta^2$ based ML-ESMD method can reproduce ab initio ESMD simulations, underscoring its potential and efficiency for broader applications, particularly in large-scale and long-timescale ESMD simulations.",
        "published": "2024-10-31 04:00:00",
        "id": "556af972-9e3b-445d-8de7-6e115169d106",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了一种基于SSR(2,2)形式主义的无相位耦合项∆²的方法，以提高机器学习势模型在处理非绝热动力学时的稳定性和准确性，并应用于PSB3以展示其有效性。"
        },
        "tokens": 955
    },
    {
        "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
        "link": "https://arxiv.org/abs/2410.22807",
        "description": "arXiv:2410.22807v1 Announce Type: cross \nAbstract: This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
        "published": "2024-10-31 04:00:00",
        "id": "b90e021b-af22-48c2-839e-a6ddc4459134",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种名为APCodec+的新型神经音频编解码器，采用两阶段联合 - 单独训练范式，实验结果表明在低比特率下可达到与高比特率下的基准编解码器相当的性能。"
        },
        "tokens": 823
    },
    {
        "title": "Latent Diffusion, Implicit Amplification: Efficient Continuous-Scale Super-Resolution for Remote Sensing Images",
        "link": "https://arxiv.org/abs/2410.22830",
        "description": "arXiv:2410.22830v1 Announce Type: cross \nAbstract: Recent advancements in diffusion models have significantly improved performance in super-resolution (SR) tasks. However, previous research often overlooks the fundamental differences between SR and general image generation. General image generation involves creating images from scratch, while SR focuses specifically on enhancing existing low-resolution (LR) images by adding typically missing high-frequency details. This oversight not only increases the training difficulty but also limits their inference efficiency. Furthermore, previous diffusion-based SR methods are typically trained and inferred at fixed integer scale factors, lacking flexibility to meet the needs of up-sampling with non-integer scale factors. To address these issues, this paper proposes an efficient and elastic diffusion-based SR model (E$^2$DiffSR), specially designed for continuous-scale SR in remote sensing imagery. E$^2$DiffSR employs a two-stage latent diffusion paradigm. During the first stage, an autoencoder is trained to capture the differential priors between high-resolution (HR) and LR images. The encoder intentionally ignores the existing LR content to alleviate the encoding burden, while the decoder introduces an SR branch equipped with a continuous scale upsampling module to accomplish the reconstruction under the guidance of the differential prior. In the second stage, a conditional diffusion model is learned within the latent space to predict the true differential prior encoding. Experimental results demonstrate that E$^2$DiffSR achieves superior objective metrics and visual quality compared to the state-of-the-art SR methods. Additionally, it reduces the inference time of diffusion-based SR methods to a level comparable to that of non-diffusion methods.",
        "published": "2024-10-31 04:00:00",
        "id": "88645338-38f8-47fd-b2c1-3f0b20916f78",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决超分辨率任务中扩散模型的训练、推理效率和灵活性问题，本文提出用于遥感图像连续尺度超分辨率的E²DiffSR模型，采用两阶段潜在扩散范式并取得良好效果。"
        },
        "tokens": 926
    },
    {
        "title": "Erd\\H{o}s-Gy\\'arf\\'as conjecture on graphs without long induced paths",
        "link": "https://arxiv.org/abs/2410.22842",
        "description": "arXiv:2410.22842v1 Announce Type: cross \nAbstract: In 1994, Erd\\H{o}s and Gy\\'arf\\'as conjectured that every graph with minimum degree at least 3 has a cycle of length a power of 2. In 2022, Gao and Shan (Graphs and Combinatorics) proved that the conjecture is true for $P_8$-free graphs, i.e., graphs without any induced copies of a path on 8 vertices. In 2024, Hu and Shen (Discrete Mathematics) improved this result by proving that the conjecture is true for $P_{10}$-free graphs. With the aid of a computer search, we improve this further by proving that the conjecture is true for $P_{13}$-free graphs.",
        "published": "2024-10-31 04:00:00",
        "id": "178796c7-f9a5-4296-b85b-3cfa70bbc5a3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "Erdős-Gyárfás猜想认为最小度至少为3的图存在长度为2的幂的圈，2022年和2024年分别有研究证明该猜想对P8 - 自由图和P10 - 自由图成立，借助计算机搜索，本文进一步证明该猜想对P13 - 自由图成立。"
        },
        "tokens": 809
    },
    {
        "title": "Graph Integration for Diffusion-Based Manifold Alignment",
        "link": "https://arxiv.org/abs/2410.22978",
        "description": "arXiv:2410.22978v1 Announce Type: cross \nAbstract: Data from individual observations can originate from various sources or modalities but are often intrinsically linked. Multimodal data integration can enrich information content compared to single-source data. Manifold alignment is a form of data integration that seeks a shared, underlying low-dimensional representation of multiple data sources that emphasizes similarities between alternative representations of the same entities. Semi-supervised manifold alignment relies on partially known correspondences between domains, either through shared features or through other known associations. In this paper, we introduce two semi-supervised manifold alignment methods. The first method, Shortest Paths on the Union of Domains (SPUD), forms a unified graph structure using known correspondences to establish graph edges. By learning inter-domain geodesic distances, SPUD creates a global, multi-domain structure. The second method, MASH (Manifold Alignment via Stochastic Hopping), learns local geometry within each domain and forms a joint diffusion operator using known correspondences to iteratively learn new inter-domain correspondences through a random-walk approach. Through the diffusion process, MASH forms a coupling matrix that links heterogeneous domains into a unified structure. We compare SPUD and MASH with existing semi-supervised manifold alignment methods and show that they outperform competing methods in aligning true correspondences and cross-domain classification. In addition, we show how these methods can be applied to transfer label information between domains.",
        "published": "2024-10-31 04:00:00",
        "id": "03be9927-4b2f-40bf-84c4-a8111169c485",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出两种半监督流形对齐方法SPUD和MASH，与现有方法比较，在对齐真实对应关系和跨域分类方面表现更优，还展示了这些方法如何用于跨域传递标签信息。"
        },
        "tokens": 881
    },
    {
        "title": "Hyperparameter Optimization in Machine Learning",
        "link": "https://arxiv.org/abs/2410.22854",
        "description": "arXiv:2410.22854v1 Announce Type: cross \nAbstract: Hyperparameters are configuration variables controlling the behavior of machine learning algorithms. They are ubiquitous in machine learning and artificial intelligence and the choice of their values determine the effectiveness of systems based on these technologies. Manual hyperparameter search is often unsatisfactory and becomes unfeasible when the number of hyperparameters is large. Automating the search is an important step towards automating machine learning, freeing researchers and practitioners alike from the burden of finding a good set of hyperparameters by trial and error. In this survey, we present a unified treatment of hyperparameter optimization, providing the reader with examples and insights into the state-of-the-art. We cover the main families of techniques to automate hyperparameter search, often referred to as hyperparameter optimization or tuning, including random and quasi-random search, bandit-, model- and gradient- based approaches. We further discuss extensions, including online, constrained, and multi-objective formulations, touch upon connections with other fields such as meta-learning and neural architecture search, and conclude with open questions and future research directions.",
        "published": "2024-10-31 04:00:00",
        "id": "1b8f928b-45a1-46a8-8def-ad4133cf3a40",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文介绍了机器学习中的超参数优化，涵盖自动化超参数搜索的主要技术家族，还讨论了扩展内容、与其他领域的联系以及未来研究方向。"
        },
        "tokens": 797
    },
    {
        "title": "Towards Population Scale Testis Volume Segmentation in DIXON MRI",
        "link": "https://arxiv.org/abs/2410.22866",
        "description": "arXiv:2410.22866v1 Announce Type: cross \nAbstract: Testis size is known to be one of the main predictors of male fertility, usually assessed in clinical workup via palpation or imaging. Despite its potential, population-level evaluation of testicular volume using imaging remains underexplored. Previous studies, limited by small and biased datasets, have demonstrated the feasibility of machine learning for testis volume segmentation. This paper presents an evaluation of segmentation methods for testicular volume using Magnet Resonance Imaging data from the UKBiobank. The best model achieves a median dice score of $0.87$, compared to median dice score of $0.83$ for human interrater reliability on the same dataset, enabling large-scale annotation on a population scale for the first time. Our overall aim is to provide a trained model, comparative baseline methods, and annotated training data to enhance accessibility and reproducibility in testis MRI segmentation research.",
        "published": "2024-10-31 04:00:00",
        "id": "7a25fe3f-6ed6-4b81-9442-0af73dc62f88",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文评估了利用英国生物银行磁共振成像数据进行睾丸体积分割的方法，最佳模型的骰子得分中位数为0.87，超过人类评分者间信度的0.83，旨在提供模型、基线方法和注释训练数据以提升睾丸MRI分割研究的可及性和再现性。"
        },
        "tokens": 804
    },
    {
        "title": "Augmenting Polish Automatic Speech Recognition System With Synthetic Data",
        "link": "https://arxiv.org/abs/2410.22903",
        "description": "arXiv:2410.22903v1 Announce Type: cross \nAbstract: This paper presents a system developed for submission to Poleval 2024, Task 3: Polish Automatic Speech Recognition Challenge. We describe Voicebox-based speech synthesis pipeline and utilize it to augment Conformer and Whisper speech recognition models with synthetic data. We show that addition of synthetic speech to training improves achieved results significantly. We also present final results achieved by our models in the competition.",
        "published": "2024-10-31 04:00:00",
        "id": "17dc1809-67f3-4100-bc3c-cbad4509868b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 4
            },
            "keyFacts": "论文介绍为参加Poleval 2024任务3开发的系统，描述基于Voicebox的语音合成管道，用合成数据增强语音识别模型，表明合成语音有助于改善结果并展示模型在竞赛中的最终成绩。"
        },
        "tokens": 688
    },
    {
        "title": "Dataset of polarimetric images of mechanically generated water surface waves coupled with surface elevation records by wave gauges linear array",
        "link": "https://arxiv.org/abs/2410.22849",
        "description": "arXiv:2410.22849v1 Announce Type: cross \nAbstract: Effective spatio-temporal measurements of water surface elevation (water waves) in laboratory experiments are essential for scientific and engineering research. Existing techniques are often cumbersome, computationally heavy and generally suffer from limited wavenumber/frequency response. To address these challenges a novel method was developed, using polarization filter equipped camera as the main sensor and Machine Learning (ML) algorithms for data processing [1,2]. The developed method training and evaluation was based on in-house made supervised dataset. Here we present this supervised dataset of polarimetric images of the water surface coupled with the water surface elevation measurements made by a linear array of resistance-type wave gauges (WG). The water waves were mechanically generated in a laboratory waves basin, and the polarimetric images were captured under an artificial light source. Meticulous camera and WGs calibration and instruments synchronization supported high spatio-temporal resolution. The data set covers several wavefield conditions, from simple monochromatic wave trains of various steepness, to irregular wavefield of JONSWAP prescribed spectral shape and several wave breaking scenarios. The dataset contains measurements repeated in several camera positions relative to the wave field propagation direction.",
        "published": "2024-10-31 04:00:00",
        "id": "6f58682f-0f6a-4589-b8d4-8f47d7c0eb55",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍一个由极化图像和水面高度测量数据组成的监督数据集，其数据由新方法获取，新方法以带偏振滤镜的相机为传感器并使用机器学习算法处理数据，数据覆盖多种波场情况。"
        },
        "tokens": 848
    },
    {
        "title": "Deduction, Constrained Zero Forcing, and Constrained Searching",
        "link": "https://arxiv.org/abs/2410.22962",
        "description": "arXiv:2410.22962v1 Announce Type: cross \nAbstract: Deduction is a recently introduced graph searching process in which searchers clear the vertex set of a graph with one move each, with each searcher's movement determined by which of its neighbors are protected by other searchers. In this paper, we show that the minimum number of searchers required to clear the graph is the same in deduction as in constrained versions of other previously studied graph processes, namely zero forcing and fast-mixed search. We give a structural characterization, new bounds and a spectrum result on the number of searchers required. We consider the complexity of computing this parameter, giving an NP-completeness result for arbitrary graphs, and exhibiting families of graphs for which the parameter can be computed in polynomial time. We also describe properties of the deduction process related to the timing of searcher movement and the success of terminal layouts.",
        "published": "2024-10-31 04:00:00",
        "id": "727882e9-9dcf-46c2-a20f-d36c3a8aaca1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "论文阐述了演绎（Deduction）这种图搜索过程与约束零强制（constrained zero forcing）和约束搜索（constrained searching）在所需搜索者最小数量上相同，给出了结构特征、新界限和频谱结果，讨论了计算此参数的复杂性并描述了演绎过程与搜索者移动时间和终端布局成功相关的特性。"
        },
        "tokens": 806
    },
    {
        "title": "Efficient Routing on Quantum Networks using Adaptive Clustering",
        "link": "https://arxiv.org/abs/2410.23007",
        "description": "arXiv:2410.23007v1 Announce Type: cross \nAbstract: We introduce QuARC, Quantum Adaptive Routing using Clusters, a novel clustering-based entanglement routing protocol that leverages redundant, multi-path routing through multi-particle projective quantum measurements to enable high-throughput, low-overhead, starvation-free entanglement distribution. At its core, QuARC periodically reconfigures the underlying quantum network into clusters of different sizes, where each cluster acts as a small network that distributes entanglement across itself, and the end-to-end entanglement is established by further distributing between clusters. QuARC does not require a-priori knowledge of any physical parameters, and is able to adapt the network configuration using static topology information, and using local (within-cluster) measurements only. We present a comprehensive simulation-based evaluation that shows QuARC is robust against changes to physical network parameters, and maintains high throughput without starvation even as network sizes scale and physical parameters degrade.",
        "published": "2024-10-31 04:00:00",
        "id": "ec7bbe46-a927-4baa-8e80-16ae5c25c81b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了QuARC这一基于聚类的量子纠缠路由协议，通过多粒子投影量子测量实现高吞吐量、低开销、无饥饿的纠缠分布，可利用静态拓扑信息自适应网络配置，经模拟评估对物理网络参数变化具有鲁棒性。"
        },
        "tokens": 787
    },
    {
        "title": "Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality",
        "link": "https://arxiv.org/abs/2410.23015",
        "description": "arXiv:2410.23015v1 Announce Type: cross \nAbstract: Virtual reality (VR) environments are frequently used in auditory and cognitive research to imitate real-life scenarios, presumably enhancing state-of-the-art approaches with traditional computer screens. However, the effects of different display technologies on audiovisual processing remain underexplored. This study investigated how VR displayed with an head-mounted display (HMD) affects serial recall performance compared to traditional computer monitors, focusing on their effects on audiovisual processing in cognitive tasks. For that matter, we conducted two experiments with both an HMD and a computer monitor as display devices and two types of audiovisual incongruences: angle (Exp. 1) and voice (Exp. 2) incongruence. To quantify cognitive performance an audiovisual verbal serial recall (avVSR) task was developed where an embodied conversational agent (ECA) was animated to speak the target digit sequence. Even though subjective evaluations showed a higher sense of presence in the HMD condition, we found no effect of the display device on the proportion of correctly recalled digits. For the extreme conditions of angle incongruence in the computer monitor presentation the proportion of correctly recalled digits increased marginally, presumably due to raised attention, but the effect is likely too small to be meaningful. Response times were not affected by incongruences in either display device across both experiments. These findings suggest that the avVSR task is robust against angular and voice audiovisual incongruences, irrespective of the display device, at least for the conditions studied here. Hence, the study introduces the avVSR task in VR and contributes to the understanding of audiovisual integration.",
        "published": "2024-10-31 04:00:00",
        "id": "7007bc59-72d9-4c1e-893c-da3583131447",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现虚拟现实（VR）中的视听角度和声音不一致不影响视听语言短期记忆，头戴式显示器（HMD）与传统电脑显示器在视听处理任务中的效果无差异。"
        },
        "tokens": 924
    },
    {
        "title": "Beyond hypergraph acyclicity: limits of tractability for pseudo-Boolean optimization",
        "link": "https://arxiv.org/abs/2410.23045",
        "description": "arXiv:2410.23045v1 Announce Type: cross \nAbstract: In this paper, we study the problem of minimizing a polynomial function with literals over all binary points, often referred to as pseudo-Boolean optimization. We investigate the fundamental limits of computation for this problem by providing new necessary conditions and sufficient conditions for tractability. On one hand, we obtain the first intractability results for pseudo-Boolean optimization problems on signed hypergraphs with bounded rank, in terms of the treewidth of the intersection graph. On the other hand, we introduce the nest-set gap, a new hypergraph-theoretic notion that enables us to move beyond hypergraph acyclicity, and obtain a polynomial-size extended formulation for the pseudo-Boolean polytope of a class of signed hypergraphs whose underlying hypergraphs contain beta-cycles.",
        "published": "2024-10-31 04:00:00",
        "id": "bd52ff47-11c7-40f2-9728-f1fef8a56a06",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究伪布尔优化的计算极限，给出可处理性的必要和充分条件，包括有界秩的有符号超图上的难解性结果和新超图理论概念下的多项式规模扩展公式。"
        },
        "tokens": 769
    },
    {
        "title": "AI-assisted prostate cancer detection and localisation on biparametric MR by classifying radiologist-positives",
        "link": "https://arxiv.org/abs/2410.23084",
        "description": "arXiv:2410.23084v1 Announce Type: cross \nAbstract: Prostate cancer diagnosis through MR imaging have currently relied on radiologists' interpretation, whilst modern AI-based methods have been developed to detect clinically significant cancers independent of radiologists. In this study, we propose to develop deep learning models that improve the overall cancer diagnostic accuracy, by classifying radiologist-identified patients or lesions (i.e. radiologist-positives), as opposed to the existing models that are trained to discriminate over all patients. We develop a single voxel-level classification model, with a simple percentage threshold to determine positive cases, at levels of lesions, Barzell-zones and patients. Based on the presented experiments from two clinical data sets, consisting of histopathology-labelled MR images from more than 800 and 500 patients in the respective UCLA and UCL PROMIS studies, we show that the proposed strategy can improve the diagnostic accuracy, by augmenting the radiologist reading of the MR imaging. Among varying definition of clinical significance, the proposed strategy, for example, achieved a specificity of 44.1% (with AI assistance) from 36.3% (by radiologists alone), at a controlled sensitivity of 80.0% on the publicly available UCLA data set. This provides measurable clinical values in a range of applications such as reducing unnecessary biopsies, lowering cost in cancer screening and quantifying risk in therapies.",
        "published": "2024-10-31 04:00:00",
        "id": "d88d2e21-fc24-4adc-9ea1-faf66d8252ea",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种深度学习模型，通过对放射科医生识别的患者或病变进行分类来提高前列腺癌诊断的准确性，实验显示该策略可提高诊断准确性，有一定临床价值。"
        },
        "tokens": 881
    },
    {
        "title": "Compositional Segmentation of Cardiac Images Leveraging Metadata",
        "link": "https://arxiv.org/abs/2410.23130",
        "description": "arXiv:2410.23130v1 Announce Type: cross \nAbstract: Cardiac image segmentation is essential for automated cardiac function assessment and monitoring of changes in cardiac structures over time. Inspired by coarse-to-fine approaches in image analysis, we propose a novel multitask compositional segmentation approach that can simultaneously localize the heart in a cardiac image and perform part-based segmentation of different regions of interest. We demonstrate that this compositional approach achieves better results than direct segmentation of the anatomies. Further, we propose a novel Cross-Modal Feature Integration (CMFI) module to leverage the metadata related to cardiac imaging collected during image acquisition. We perform experiments on two different modalities, MRI and ultrasound, using public datasets, Multi-disease, Multi-View, and Multi-Centre (M&amp;Ms-2) and Multi-structure Ultrasound Segmentation (CAMUS) data, to showcase the efficiency of the proposed compositional segmentation method and Cross-Modal Feature Integration module incorporating metadata within the proposed compositional segmentation network. The source code is available: https://github.com/kabbas570/CompSeg-MetaData.",
        "published": "2024-10-31 04:00:00",
        "id": "8acfd6d1-cb4a-4e0d-a360-0f5fdf6d7bb1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的多任务组合分割方法用于心脏图像分割，效果优于直接分割，还提出跨模态特征集成模块利用图像采集时的元数据，在MRI和超声两种模态数据上进行实验，源代码已公开。"
        },
        "tokens": 822
    },
    {
        "title": "When can classical neural networks represent quantum states?",
        "link": "https://arxiv.org/abs/2410.23152",
        "description": "arXiv:2410.23152v1 Announce Type: cross \nAbstract: A naive classical representation of an n-qubit state requires specifying exponentially many amplitudes in the computational basis. Past works have demonstrated that classical neural networks can succinctly express these amplitudes for many physically relevant states, leading to computationally powerful representations known as neural quantum states. What underpins the efficacy of such representations? We show that conditional correlations present in the measurement distribution of quantum states control the performance of their neural representations. Such conditional correlations are basis dependent, arise due to measurement-induced entanglement, and reveal features not accessible through conventional few-body correlations often examined in studies of phases of matter. By combining theoretical and numerical analysis, we demonstrate how the state's entanglement and sign structure, along with the choice of measurement basis, give rise to distinct patterns of short- or long-range conditional correlations. Our findings provide a rigorous framework for exploring the expressive power of neural quantum states.",
        "published": "2024-10-31 04:00:00",
        "id": "c16c6f4e-d99b-4be2-82d6-473d69ed05c3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究表明量子态测量分布中的条件相关性控制其神经表示的性能，结合理论和数值分析，展示了量子态的纠缠和符号结构以及测量基的选择如何产生不同模式的短或长程条件相关性，为探索神经量子态的表达能力提供框架。"
        },
        "tokens": 788
    },
    {
        "title": "Improved convergence rate of kNN graph Laplacians",
        "link": "https://arxiv.org/abs/2410.23212",
        "description": "arXiv:2410.23212v1 Announce Type: cross \nAbstract: In graph-based data analysis, $k$-nearest neighbor ($k$NN) graphs are widely used due to their adaptivity to local data densities. Allowing weighted edges in the graph, the kernelized graph affinity provides a more general type of $k$NN graph where the $k$NN distance is used to set the kernel bandwidth adaptively. In this work, we consider a general class of $k$NN graph where the graph affinity is $W_{ij} = \\epsilon^{-d/2} \\; k_0 ( \\| x_i - x_j \\|^2 / \\epsilon \\phi( \\widehat{\\rho}(x_i), \\widehat{\\rho}(x_j) )^2 ) $, with $\\widehat{\\rho}(x)$ being the (rescaled) $k$NN distance at the point $x$, $\\phi$ a symmetric bi-variate function, and $k_0$ a non-negative function on $[0,\\infty)$. Under the manifold data setting, where $N$ i.i.d. samples $x_i$ are drawn from a density $p$ on a $d$-dimensional unknown manifold embedded in a high dimensional Euclidean space, we prove the point-wise convergence of the $k$NN graph Laplacian to the limiting manifold operator (depending on $p$) at the rate of $O(N^{-2/(d+6)}\\,)$, up to a log factor, when $k_0$ and $\\phi$ have $C^3$ regularity and satisfy other technical conditions. This fast rate is obtained when $\\epsilon \\sim N^{-2/(d+6)}\\,$ and $k \\sim N^{6/(d+6)}\\,$, both at the optimal order to balance the theoretical bias and variance errors. When $k_0$ and $\\phi$ have lower regularities, including when $k_0$ is a compactly supported function as in the standard $k$NN graph, the convergence rate degenerates to $O(N^{-1/(d+4)}\\,)$. Our improved convergence rate is based on a refined analysis of the $k$NN estimator, which can be of independent interest. We validate our theory by numerical experiments on simulated data.",
        "published": "2024-10-31 04:00:00",
        "id": "2560d60e-71ac-41ee-b7f4-158616513114",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "在基于图的数据分析中，针对一类k近邻（kNN）图，在流形数据设置下，证明kNN图拉普拉斯算子以特定收敛速率向极限流形算子收敛，改进的收敛率基于对kNN估计量的精细分析，通过数值实验验证了理论。"
        },
        "tokens": 1098
    },
    {
        "title": "Crosstalk Attack Resilient RNS Quantum Addition",
        "link": "https://arxiv.org/abs/2410.23217",
        "description": "arXiv:2410.23217v1 Announce Type: cross \nAbstract: As quantum computers scale, the rise of multi-user and cloud-based quantum platforms can lead to new security challenges. Attacks within shared execution environments become increasingly feasible due to the crosstalk noise that, in combination with quantum computer's hardware specifications, can be exploited in form of crosstalk attack. Our work pursues crosstalk attack implementation in ion-trap quantum computers. We propose three novel quantum crosstalk attacks designed for ion trap qubits: (i) Alternate CNOT attack (ii) Superposition Alternate CNOT (SAC) attack (iii) Alternate Phase Change (APC) attack. We demonstrate the effectiveness of proposed attacks by conducting noise-based simulations on a commercial 20-qubit ion-trap quantum computer. The proposed attacks achieve an impressive reduction of up to 42.2% in output probability for Quantum Full Adders (QFA) having 6 to 9-qubit output. Finally, we investigate the possibility of mitigating crosstalk attacks by using Residue Number System (RNS) based Parallel Quantum Addition (PQA). We determine that PQA achieves higher attack resilience against crosstalk attacks in the form of 24.3% to 133.5% improvement in output probability against existing Non Parallel Quantum Addition (NPQA). Through our systematic methodology, we demonstrate how quantum properties such as superposition and phase transition can lead to crosstalk attacks and also how parallel quantum computing can help secure against these attacks.",
        "published": "2024-10-31 04:00:00",
        "id": "5909f881-c428-48fe-9197-fe0185f53b0b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在离子阱量子计算机中研究串扰攻击的实现，提出三种量子串扰攻击方式，通过模拟展示攻击有效性，还探讨使用基于余数系统的并行量子加法缓解串扰攻击的可能性。"
        },
        "tokens": 916
    },
    {
        "title": "Far-right party influence on polarization dynamics in electoral campaign",
        "link": "https://arxiv.org/abs/2410.23177",
        "description": "arXiv:2410.23177v1 Announce Type: cross \nAbstract: Political polarization has attracted increasing attention in recent years, driven by the rise of social media and the global emergence of far-right populist movements. This study investigates the dynamics of structural polarization during electoral campaigns in multi-party systems, with a particular focus on the presence of far-right actors and their influence on polarization patterns and hate speech. Using retweet networks as a measure of structural polarization, we analyze two case studies in Spain: the 2022 Andalusia regional elections, where the far-right party Vox was a significant contender, and the 2019 Barcelona city council elections, where the party had no representation. Our results reveal that the presence of a far-right party intensifies polarization, leading to the formation of two distinct ideological blocks aligned along left-right ideological axes, as observed in Andalusia. In contrast, the Catalan independence movement in Barcelona diluted the alignment of voters, resulting in a more complex, multi-axis polarization landscape. We also explore the relationship between polarization and hate speech, finding an anti-correlation between them in both cases. Our findings underscore the significant role of far-right movements in driving political polarization and the nuanced effects of different political contexts on polarization dynamics.",
        "published": "2024-10-31 04:00:00",
        "id": "01e76805-f448-4f34-a75f-560cb2826828",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "研究调查多党制选举活动期间结构极化的动态，通过西班牙的两个案例研究，发现极右翼政党的存在会加剧极化，还探讨了极化与仇恨言论之间的关系。"
        },
        "tokens": 841
    },
    {
        "title": "Uncertainty quantification for fast reconstruction methods using augmented equivariant bootstrap: Application to radio interferometry",
        "link": "https://arxiv.org/abs/2410.23178",
        "description": "arXiv:2410.23178v1 Announce Type: cross \nAbstract: The advent of next-generation radio interferometers like the Square Kilometer Array promises to revolutionise our radio astronomy observational capabilities. The unprecedented volume of data these devices generate requires fast and accurate image reconstruction algorithms to solve the ill-posed radio interferometric imaging problem. Most state-of-the-art reconstruction methods lack trustworthy and scalable uncertainty quantification, which is critical for the rigorous scientific interpretation of radio observations. We propose an unsupervised technique based on a conformalized version of a radio-augmented equivariant bootstrapping method, which allows us to quantify uncertainties for fast reconstruction methods. Noticeably, we rely on reconstructions from ultra-fast unrolled algorithms. The proposed method brings more reliable uncertainty estimations to our problem than existing alternatives.",
        "published": "2024-10-31 04:00:00",
        "id": "3b7a3d85-900d-4c15-81b8-ab41353f9631",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于无线电增强等变自举法的无监督技术，用于量化快速重建方法的不确定性，相比现有替代方案更可靠，有助于解决射电干涉成像问题。"
        },
        "tokens": 752
    },
    {
        "title": "Nested ResNet: A Vision-Based Method for Detecting the Sensing Area of a Drop-in Gamma Probe",
        "link": "https://arxiv.org/abs/2410.23154",
        "description": "arXiv:2410.23154v1 Announce Type: cross \nAbstract: Purpose: Drop-in gamma probes are widely used in robotic-assisted minimally invasive surgery (RAMIS) for lymph node detection. However, these devices only provide audio feedback on signal intensity, lacking the visual feedback necessary for precise localisation. Previous work attempted to predict the sensing area location using laparoscopic images, but the prediction accuracy was unsatisfactory. Improvements are needed in the deep learning-based regression approach.\n  Methods: We introduce a three-branch deep learning framework to predict the sensing area of the probe. Specifically, we utilise the stereo laparoscopic images as input for the main branch and develop a Nested ResNet architecture. The framework also incorporates depth estimation via transfer learning and orientation guidance through probe axis sampling. The combined features from each branch enhanced the accuracy of the prediction.\n  Results: Our approach has been evaluated on a publicly available dataset, demonstrating superior performance over previous methods. In particular, our method resulted in a 22.10\\% decrease in 2D mean error and a 41.67\\% reduction in 3D mean error. Additionally, qualitative comparisons further demonstrated the improved precision of our approach.\n  Conclusion: With extensive evaluation, our solution significantly enhances the accuracy and reliability of sensing area predictions. This advancement enables visual feedback during the use of the drop-in gamma probe in surgery, providing surgeons with more accurate and reliable localisation.}",
        "published": "2024-10-31 04:00:00",
        "id": "452cc4b7-0974-44fb-b715-14c463c04c0d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种三分支深度学习框架预测探头感应区域，采用Nested ResNet架构，经评估在公开数据集上表现优于以往方法，提高了感应区域预测的准确性和可靠性。"
        },
        "tokens": 889
    },
    {
        "title": "AugTriever: Unsupervised Dense Retrieval and Domain Adaptation by Scalable Data Augmentation",
        "link": "https://arxiv.org/abs/2212.08841",
        "description": "arXiv:2212.08841v4 Announce Type: replace \nAbstract: Dense retrievers have made significant strides in text retrieval and open-domain question answering. However, most of these achievements have relied heavily on extensive human-annotated supervision. In this study, we aim to develop unsupervised methods for improving dense retrieval models. We propose two approaches that enable annotation-free and scalable training by creating pseudo querydocument pairs: query extraction and transferred query generation. The query extraction method involves selecting salient spans from the original document to generate pseudo queries. On the other hand, the transferred query generation method utilizes generation models trained for other NLP tasks, such as summarization, to produce pseudo queries. Through extensive experimentation, we demonstrate that models trained using these augmentation methods can achieve comparable, if not better, performance than multiple strong dense baselines. Moreover, combining these strategies leads to further improvements, resulting in superior performance of unsupervised dense retrieval, unsupervised domain adaptation and supervised finetuning, benchmarked on both BEIR and ODQA datasets. Code and datasets are publicly available at https://github.com/salesforce/AugTriever.",
        "published": "2024-10-31 04:00:00",
        "id": "a083e4ab-35f3-42f5-9ece-e2c15f551dfb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出通过创建伪查询 - 文档对实现无注释和可扩展训练的两种方法（查询提取和转移查询生成），实验证明用这些增强方法训练的模型在密集检索等任务上有较好性能，相关代码和数据集已公开。"
        },
        "tokens": 836
    },
    {
        "title": "Scientific and Technological Information Oriented Semantics-adversarial and Media-adversarial Cross-media Retrieval",
        "link": "https://arxiv.org/abs/2203.08615",
        "description": "arXiv:2203.08615v3 Announce Type: replace \nAbstract: Cross-media retrieval of scientific and technological information is one of the important tasks in the cross-media study. Cross-media scientific and technological information retrieval obtain target information from massive multi-source and heterogeneous scientific and technological resources, which helps to design applications that meet users' needs, including scientific and technological information recommendation, personalized scientific and technological information retrieval, etc. The core of cross-media retrieval is to learn a common subspace, so that data from different media can be directly compared with each other after being mapped into this subspace. In subspace learning, existing methods often focus on modeling the discrimination of intra-media data and the invariance of inter-media data after mapping; however, they ignore the semantic consistency of inter-media data before and after mapping and media discrimination of intra-semantics data, which limit the result of cross-media retrieval. In light of this, we propose a scientific and technological information oriented Semantics-adversarial and Media-adversarial Cross-media Retrieval method (SMCR) to find an effective common subspace. Specifically, SMCR minimizes the loss of inter-media semantic consistency in addition to modeling intra-media semantic discrimination, to preserve semantic similarity before and after mapping. Furthermore, SMCR constructs a basic feature mapping network and a refined feature mapping network to jointly minimize the media discriminative loss within semantics, so as to enhance the feature mapping network's ability to confuse the media discriminant network. Experimental results on two datasets demonstrate that the proposed SMCR outperforms state-of-the-art methods in cross-media retrieval.",
        "published": "2024-10-31 04:00:00",
        "id": "463c5d27-6ef2-462c-8aa8-2372c9268e6c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对科技信息的跨媒体检索提出语义对抗和媒体对抗的跨媒体检索方法SMCR，通过减少跨媒体语义一致性损失等方式找到有效公共子空间，实验证明其在跨媒体检索方面优于现有方法。"
        },
        "tokens": 919
    },
    {
        "title": "Cognitive Load-based Affective Workload Allocation for Multi-human Multi-robot Teams",
        "link": "https://arxiv.org/abs/2303.10465",
        "description": "arXiv:2303.10465v2 Announce Type: replace \nAbstract: The interaction and collaboration between humans and multiple robots represent a novel field of research known as human multi-robot systems. Adequately designed systems within this field allow teams composed of both humans and robots to work together effectively on tasks such as monitoring, exploration, and search and rescue operations. This paper presents a deep reinforcement learning-based affective workload allocation controller specifically for multi-human multi-robot teams. The proposed controller can dynamically reallocate workloads based on the performance of the operators during collaborative missions with multi-robot systems. The operators' performances are evaluated through the scores of a self-reported questionnaire (i.e., subjective measurement) and the results of a deep learning-based cognitive workload prediction algorithm that uses physiological and behavioral data (i.e., objective measurement). To evaluate the effectiveness of the proposed controller, we use a multi-human multi-robot CCTV monitoring task as an example and carry out comprehensive real-world experiments with 32 human subjects for both quantitative measurement and qualitative analysis. Our results demonstrate the performance and effectiveness of the proposed controller and highlight the importance of incorporating both subjective and objective measurements of the operators' cognitive workload as well as seeking consent for workload transitions, to enhance the performance of multi-human multi-robot teams.",
        "published": "2024-10-31 04:00:00",
        "id": "534cc068-5153-4b55-a9e5-908cc3e9de46",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于深度强化学习的情感工作量分配控制器用于多人多机器人团队，通过主观和客观测量评估操作员表现以动态重新分配工作量，并用多人多机器人CCTV监控任务实验验证其有效性。"
        },
        "tokens": 851
    },
    {
        "title": "BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for Graph Continual Learning",
        "link": "https://arxiv.org/abs/2211.14568",
        "description": "arXiv:2211.14568v5 Announce Type: replace \nAbstract: Continual Learning (CL) is the process of learning ceaselessly a sequence of tasks. Most existing CL methods deal with independent data (e.g., images and text) for which many benchmark frameworks and results under standard experimental settings are available. Compared to them, however, CL methods for graph data (graph CL) are relatively underexplored because of (a) the lack of standard experimental settings, especially regarding how to deal with the dependency between instances, (b) the lack of benchmark datasets and scenarios, and (c) high complexity in implementation and evaluation due to the dependency. In this paper, regarding (a) we define four standard incremental settings (task-, class-, domain-, and time-incremental) for node-, link-, and graph-level problems, extending the previously explored scope. Regarding (b), we provide 35 benchmark scenarios based on 24 real-world graphs. Regarding (c), we develop BeGin, an easy and fool-proof framework for graph CL. BeGin is easily extended since it is modularized with reusable modules for data processing, algorithm design, and evaluation. Especially, the evaluation module is completely separated from user code to eliminate potential mistakes. Regarding benchmark results, we cover 3x more combinations of incremental settings and levels of problems than the latest benchmark. All assets for the benchmark framework are publicly available at https://github.com/ShinhwanKang/BeGin.",
        "published": "2024-10-31 04:00:00",
        "id": "ae7c6c29-fd97-48b9-954b-4f56608d5665",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文定义了图数据持续学习（graph CL）的四个标准增量设置，提供35个基准场景、开发出BeGin框架，还给出了基准结果，相关资产已公开。"
        },
        "tokens": 905
    },
    {
        "title": "A Strong Baseline for Semi-Supervised Incremental Few-Shot Learning",
        "link": "https://arxiv.org/abs/2110.11128",
        "description": "arXiv:2110.11128v3 Announce Type: replace \nAbstract: Few-shot learning (FSL) aims to learn models that generalize to novel classes with limited training samples. Recent works advance FSL towards a scenario where unlabeled examples are also available and propose semi-supervised FSL methods. Another line of methods also cares about the performance of base classes in addition to the novel ones and thus establishes the incremental FSL scenario. In this paper, we generalize the above two under a more realistic yet complex setting, named by Semi-Supervised Incremental Few-Shot Learning (S2 I-FSL). To tackle the task, we propose a novel paradigm containing two parts: (1) a well-designed meta-training algorithm for mitigating ambiguity between base and novel classes caused by unreliable pseudo labels and (2) a model adaptation mechanism to learn discriminative features for novel classes while preserving base knowledge using few labeled and all the unlabeled data. Extensive experiments on standard FSL, semi-supervised FSL, incremental FSL, and the firstly built S2 I-FSL benchmarks demonstrate the effectiveness of our proposed method.",
        "published": "2024-10-31 04:00:00",
        "id": "b4d41cab-3f38-4fac-afe5-8ac83b73f2d2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种新范式以解决半监督增量少样本学习任务，包括精心设计的元训练算法和模型适应机制，在多个基准测试中的实验证明了该方法的有效性。"
        },
        "tokens": 813
    },
    {
        "title": "Neural Networks with Sparse Activation Induced by Large Bias: Tighter Analysis with Bias-Generalized NTK",
        "link": "https://arxiv.org/abs/2301.00327",
        "description": "arXiv:2301.00327v3 Announce Type: replace \nAbstract: We study training one-hidden-layer ReLU networks in the neural tangent kernel (NTK) regime, where the networks' biases are initialized to some constant rather than zero. We prove that under such initialization, the neural network will have sparse activation throughout the entire training process, which enables fast training procedures via some sophisticated computational methods. With such initialization, we show that the neural networks possess a different limiting kernel which we call \\textit{bias-generalized NTK}, and we study various properties of the neural networks with this new kernel. We first characterize the gradient descent dynamics. In particular, we show that the network in this case can achieve as fast convergence as the dense network, as opposed to the previous work suggesting that the sparse networks converge slower. In addition, our result improves the previous required width to ensure convergence. Secondly, we study the networks' generalization: we show a width-sparsity dependence, which yields a sparsity-dependent Rademacher complexity and generalization bound. To our knowledge, this is the first sparsity-dependent generalization result via Rademacher complexity. Lastly, we study the smallest eigenvalue of this new kernel. We identify a data-dependent region where we can derive a much sharper lower bound on the NTK's smallest eigenvalue than the worst-case bound previously known. This can lead to improvement in the generalization bound.",
        "published": "2024-10-31 04:00:00",
        "id": "810c2319-9cd7-4e87-a6f0-12f07ab21966",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究在神经正切核(NTK)机制下训练单隐藏层ReLU网络，证明初始化网络偏差为常数时网络在训练中保持稀疏激活，分析了偏差广义NTK的相关性质，包括梯度下降动力学、泛化能力和新内核最小特征值。"
        },
        "tokens": 898
    },
    {
        "title": "Very fast Bayesian Additive Regression Trees on GPU",
        "link": "https://arxiv.org/abs/2410.23244",
        "description": "arXiv:2410.23244v1 Announce Type: cross \nAbstract: Bayesian Additive Regression Trees (BART) is a nonparametric Bayesian regression technique based on an ensemble of decision trees. It is part of the toolbox of many statisticians. The overall statistical quality of the regression is typically higher than other generic alternatives, and it requires less manual tuning, making it a good default choice. However, it is a niche method compared to its natural competitor XGBoost, due to the longer running time, making sample sizes above 10,000-100,000 a nuisance. I present a GPU-enabled implementation of BART, faster by up to 200x relative to a single CPU core, making BART competitive in running time with XGBoost. This implementation is available in the Python package bartz.",
        "published": "2024-10-31 04:00:00",
        "id": "b1ecc18c-174d-4fa0-90f7-258fdb30dc57",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一个GPU启用的贝叶斯加法回归树（BART）的实现，比单CPU核心快200倍，使BART在运行时间上与XGBoost有竞争力，该实现可在Python包bartz中使用。"
        },
        "tokens": 775
    },
    {
        "title": "bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction",
        "link": "https://arxiv.org/abs/2410.23247",
        "description": "arXiv:2410.23247v1 Announce Type: cross \nAbstract: Quanta image sensors, such as SPAD arrays, are an emerging sensor technology, producing 1-bit arrays representing photon detection events over exposures as short as a few nanoseconds. In practice, raw data are post-processed using heavy spatiotemporal binning to create more useful and interpretable images at the cost of degrading spatiotemporal resolution. In this work, we propose bit2bit, a new method for reconstructing high-quality image stacks at the original spatiotemporal resolution from sparse binary quanta image data. Inspired by recent work on Poisson denoising, we developed an algorithm that creates a dense image sequence from sparse binary photon data by predicting the photon arrival location probability distribution. However, due to the binary nature of the data, we show that the assumption of a Poisson distribution is inadequate. Instead, we model the process with a Bernoulli lattice process from the truncated Poisson. This leads to the proposal of a novel self-supervised solution based on a masked loss function. We evaluate our method using both simulated and real data. On simulated data from a conventional video, we achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06 photons per pixel per frame). We also present a novel dataset containing a wide range of real SPAD high-speed videos under various challenging imaging conditions. The scenes cover strong/weak ambient light, strong motion, ultra-fast events, etc., which will be made available to the community, on which we demonstrate the promise of our approach. Both reconstruction quality and throughput substantially surpass the state-of-the-art methods (e.g., Quanta Burst Photography (QBP)). Our approach significantly enhances the visualization and usability of the data, enabling the application of existing analysis techniques.",
        "published": "2024-10-31 04:00:00",
        "id": "48f356a3-d584-40c9-8e6d-215edce6a173",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出bit2bit方法从稀疏二进制量子图像数据重建高质量图像堆栈，受泊松去噪工作启发，基于截断泊松的伯努利格点过程建模并提出新的自监督解决方案，用模拟和真实数据评估方法且在多方面超越现有技术。"
        },
        "tokens": 985
    },
    {
        "title": "Generalized Short Path Algorithms: Towards Super-Quadratic Speedup over Markov Chain Search for Combinatorial Optimization",
        "link": "https://arxiv.org/abs/2410.23270",
        "description": "arXiv:2410.23270v1 Announce Type: cross \nAbstract: We analyze generalizations of algorithms based on the short-path framework first proposed by Hastings [Quantum 2, 78 (2018)], which has been extended and shown by Dalzell et al. [STOC '22] to achieve super-Grover speedups for certain binary optimization problems. We demonstrate that, under some commonly satisfied technical conditions, an appropriate generalization can achieve super-quadratic speedups not only over unstructured search but also over a classical optimization algorithm that searches for the optimum by drawing samples from the stationary distribution of a Markov Chain. We employ this framework to obtain algorithms for problems including variants of Max-Bisection, Max Independent Set, the Ising Model, and the Sherrington Kirkpatrick Model, whose runtimes are asymptotically faster than those obtainable from previous short path techniques. For random regular graphs of sufficiently high degree, our algorithm is super-quadratically faster than the best rigorously proven classical runtimes for regular graphs. Our results also shed light on the quantum nature of short path algorithms, by identifying a setting where our algorithm is super-quadratically faster than any polynomial time Gibbs sampler, unless NP = RP. We conclude the paper with a numerical analysis that guides the choice of parameters for short path algorithms and raises the possibility of super-quadratic speedups in settings that are currently beyond our theoretical analysis.",
        "published": "2024-10-31 04:00:00",
        "id": "723aa17e-460b-4c1b-ad89-270aeb128c25",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "分析基于短路径框架算法的推广，在满足一定条件下可实现超二次加速，用于多种组合优化问题，运行时间渐近快于以往短路径技术，还通过实例阐明算法的量子性质并进行数值分析。"
        },
        "tokens": 894
    },
    {
        "title": "Functional Gradient Flows for Constrained Sampling",
        "link": "https://arxiv.org/abs/2410.23170",
        "description": "arXiv:2410.23170v1 Announce Type: cross \nAbstract: Recently, through a unified gradient flow perspective of Markov chain Monte Carlo (MCMC) and variational inference (VI), particle-based variational inference methods (ParVIs) have been proposed that tend to combine the best of both worlds. While typical ParVIs such as Stein Variational Gradient Descent (SVGD) approximate the gradient flow within a reproducing kernel Hilbert space (RKHS), many attempts have been made recently to replace RKHS with more expressive function spaces, such as neural networks. While successful, these methods are mainly designed for sampling from unconstrained domains. In this paper, we offer a general solution to constrained sampling by introducing a boundary condition for the gradient flow which would confine the particles within the specific domain. This allows us to propose a new functional gradient ParVI method for constrained sampling, called constrained functional gradient flow (CFG), with provable continuous-time convergence in total variation (TV). We also present novel numerical strategies to handle the boundary integral term arising from the domain constraints. Our theory and experiments demonstrate the effectiveness of the proposed framework.",
        "published": "2024-10-31 04:00:00",
        "id": "cc6bc543-41db-4187-8058-18e94727c8b5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种约束采样的功能梯度ParVI方法，有总变差连续时间收敛性，还给出处理边界积分项的数值策略"
        },
        "tokens": 801
    },
    {
        "title": "Full-waveform earthquake source inversion using simulation-based inference",
        "link": "https://arxiv.org/abs/2410.23238",
        "description": "arXiv:2410.23238v1 Announce Type: cross \nAbstract: This paper presents a novel framework for full-waveform seismic source inversion using simulation-based inference (SBI). Traditional probabilistic approaches often rely on simplifying assumptions about data errors, which we show can lead to inaccurate uncertainty quantification. SBI addresses this limitation by building an empirical probabilistic model of the data errors using machine learning models, known as neural density estimators, which can then be integrated into the Bayesian inference framework. We apply the SBI framework to point-source moment tensor inversions as well as joint moment tensor and time-location inversions. We construct a range of synthetic examples to explore the quality of the SBI solutions, as well as to compare the SBI results with standard Gaussian likelihood-based Bayesian inversions. We then demonstrate that under real seismic noise, common Gaussian likelihood assumptions for treating full-waveform data yield overconfident posterior distributions that underestimate the moment tensor component uncertainties by up to a factor of 3. We contrast this with SBI, which produces well-calibrated posteriors that generally agree with the true seismic source parameters, and offers an order-of-magnitude reduction in the number of simulations required to perform inference compared to standard Monte Carlo techniques. Finally, we apply our methodology to a pair of moderate magnitude earthquakes in the North Atlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean bottom seismometer array as well as by regional land stations in the Azores, comparing full moment tensor and source-time location posteriors between SBI and a Gaussian likelihood approach. We find that our adaptation of SBI can be directly applied to real earthquake sources to efficiently produce high quality posterior distributions that significantly improve upon Gaussian likelihood approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "13fc4bde-0d52-4ad5-a15f-14ad926b0a1e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出基于模拟推理（SBI）的全波形地震源反演框架，展示SBI在点源矩张量反演和联合矩张量与时间 - 位置反演中的应用并与传统方法对比，还应用于北大西洋地震实例。"
        },
        "tokens": 946
    },
    {
        "title": "Selective Reincarnation: Offline-to-Online Multi-Agent Reinforcement Learning",
        "link": "https://arxiv.org/abs/2304.00977",
        "description": "arXiv:2304.00977v2 Announce Type: replace \nAbstract: 'Reincarnation' in reinforcement learning has been proposed as a formalisation of reusing prior computation from past experiments when training an agent in an environment. In this paper, we present a brief foray into the paradigm of reincarnation in the multi-agent (MA) context. We consider the case where only some agents are reincarnated, whereas the others are trained from scratch -- selective reincarnation. In the fully-cooperative MA setting with heterogeneous agents, we demonstrate that selective reincarnation can lead to higher returns than training fully from scratch, and faster convergence than training with full reincarnation. However, the choice of which agents to reincarnate in a heterogeneous system is vitally important to the outcome of the training -- in fact, a poor choice can lead to considerably worse results than the alternatives. We argue that a rich field of work exists here, and we hope that our effort catalyses further energy in bringing the topic of reincarnation to the multi-agent realm.",
        "published": "2024-10-31 04:00:00",
        "id": "26975b5f-c13f-4b3b-af2c-e8615c23c052",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨多智能体（MA）环境中的选择性转世（selective reincarnation），在完全合作的异构MA设置中，它比完全从头训练回报更高、比完全转世训练收敛更快，但异构系统中选择转世的智能体对训练结果至关重要。"
        },
        "tokens": 818
    },
    {
        "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
        "link": "https://arxiv.org/abs/2305.12715",
        "description": "arXiv:2305.12715v4 Announce Type: replace \nAbstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.",
        "published": "2024-10-31 04:00:00",
        "id": "324d9313-ccf9-4f5d-a8ea-22e51f9395c1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了不精确标签学习（ILL）框架，它利用期望最大化（EM）对不精确标签信息建模，可适应多种不精确标签设置，性能优于现有处理不精确标签的技术。"
        },
        "tokens": 873
    },
    {
        "title": "Interpretable Mesomorphic Networks for Tabular Data",
        "link": "https://arxiv.org/abs/2305.13072",
        "description": "arXiv:2305.13072v2 Announce Type: replace \nAbstract: Even though neural networks have been long deployed in applications involving tabular data, still existing neural architectures are not explainable by design. In this paper, we propose a new class of interpretable neural networks for tabular data that are both deep and linear at the same time (i.e. mesomorphic). We optimize deep hypernetworks to generate explainable linear models on a per-instance basis. As a result, our models retain the accuracy of black-box deep networks while offering free-lunch explainability for tabular data by design. Through extensive experiments, we demonstrate that our explainable deep networks have comparable performance to state-of-the-art classifiers on tabular data and outperform current existing methods that are explainable by design.",
        "published": "2024-10-31 04:00:00",
        "id": "c94c876d-a639-4802-ae10-e0de2bd9b8ba",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一类用于表格数据的可解释神经网络，优化深度超网络为每个实例生成可解释线性模型，实验表明其性能可比肩现有先进分类器并超越现有可解释方法。"
        },
        "tokens": 745
    },
    {
        "title": "Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment Triplet Extraction",
        "link": "https://arxiv.org/abs/2305.14434",
        "description": "arXiv:2305.14434v2 Announce Type: replace \nAbstract: Aspect Sentiment Triplet Extraction (ASTE) is a challenging task in sentiment analysis, aiming to provide fine-grained insights into human sentiments. However, existing benchmarks are limited to two domains and do not evaluate model performance on unseen domains, raising concerns about the generalization of proposed methods. Furthermore, it remains unclear if large language models (LLMs) can effectively handle complex sentiment tasks like ASTE. In this work, we address the issue of generalization in ASTE from both a benchmarking and modeling perspective. We introduce a domain-expanded benchmark by annotating samples from diverse domains, enabling evaluation of models in both in-domain and out-of-domain settings. Additionally, we propose CASE, a simple and effective decoding strategy that enhances trustworthiness and performance of LLMs in ASTE. Through comprehensive experiments involving multiple tasks, settings, and models, we demonstrate that CASE can serve as a general decoding strategy for complex sentiment tasks. By expanding the scope of evaluation and providing a more reliable decoding strategy, we aim to inspire the research community to reevaluate the generalizability of benchmarks and models for ASTE. Our code, data, and models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste.",
        "published": "2024-10-31 04:00:00",
        "id": "7fac60e8-ec5f-445e-babb-d4cedf0c9aac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出从基准测试和建模角度解决Aspect Sentiment Triplet Extraction (ASTE)中的泛化问题，引入领域扩展基准，提出CASE解码策略，相关代码、数据和模型可在特定网址获取。"
        },
        "tokens": 859
    },
    {
        "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
        "link": "https://arxiv.org/abs/2305.15265",
        "description": "arXiv:2305.15265v3 Announce Type: replace \nAbstract: With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones. By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7$\\times$ peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size. Under the same hardware, WTA-CRS enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.",
        "published": "2024-10-31 04:00:00",
        "id": "31e4c4cc-1f35-44e1-b64a-b54f45f4df1d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对大型预训练语言模型微调时内存占用大的问题，提出一种新的无偏估计器WTA - CRS，理论和实验证明其在调整变压器时方差更低，能减少峰值内存且几乎不降低精度，还能增大批量大小提升下游任务性能。"
        },
        "tokens": 891
    },
    {
        "title": "Metric Based Few-Shot Graph Classification",
        "link": "https://arxiv.org/abs/2206.03695",
        "description": "arXiv:2206.03695v3 Announce Type: replace \nAbstract: Many modern deep-learning techniques do not work without enormous datasets. At the same time, several fields demand methods working in scarcity of data. This problem is even more complex when the samples have varying structures, as in the case of graphs. Graph representation learning techniques have recently proven successful in a variety of domains. Nevertheless, the employed architectures perform miserably when faced with data scarcity. On the other hand, few-shot learning allows employing modern deep learning models in scarce data regimes without waiving their effectiveness. In this work, we tackle the problem of few-shot graph classification, showing that equipping a simple distance metric learning baseline with a state-of-the-art graph embedder allows to obtain competitive results on the task. While the simplicity of the architecture is enough to outperform more complex ones, it also allows straightforward additions. To this end, we show that additional improvements may be obtained by encouraging a task-conditioned embedding space. Finally, we propose a MixUp-based online data augmentation technique acting in the latent space and show its effectiveness on the task.",
        "published": "2024-10-31 04:00:00",
        "id": "1a2e56b6-9017-47d7-acb7-fa69d3b06eb6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究解决少样本图分类问题，表明用先进图嵌入器装备简单距离度量学习基线可在任务中获竞争力成果，还可通过鼓励任务条件嵌入空间改进，提出潜空间的MixUp在线数据增强技术并证明有效性。"
        },
        "tokens": 815
    },
    {
        "title": "Computation of a Unified Graph-Based Rate Optimization Problem",
        "link": "https://arxiv.org/abs/2306.04981",
        "description": "arXiv:2306.04981v3 Announce Type: replace \nAbstract: We define a graph-based rate optimization problem and consider its computation, which provides a unified approach to the computation of various theoretical limits, including the (conditional) graph entropy, rate-distortion functions and capacity-cost functions with side information. Compared with their classical counterparts, theoretical limits with side information are much more difficult to compute since their characterizations as optimization problems have larger and more complex feasible regions. Following the unified approach, we develop effective methods to resolve the difficulty. On the theoretical side, we derive graph characterizations for rate-distortion and capacity-cost functions with side information and simplify the characterizations in special cases by reducing the number of decision variables. On the computational side, we design an efficient alternating minimization algorithm for the graph-based problem, which deals with the inequality constraint by a flexible multiplier update strategy. Moreover, simplified graph characterizations are exploited and deflation techniques are introduced, so that the computing time is greatly reduced. Theoretical analysis shows that the algorithm converges to an optimal solution. By numerical experiments, the accuracy and efficiency of the algorithm are illustrated and its significant advantage over existing methods is demonstrated.",
        "published": "2024-10-31 04:00:00",
        "id": "cd976533-cd28-4f2d-bd3d-30eb86f5e8f4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "定义基于图的速率优化问题并考虑其计算，开发有效方法解决带边信息的理论极限计算困难，设计交替最小化算法并进行理论分析和数值实验。"
        },
        "tokens": 816
    },
    {
        "title": "Efficient distributed representations with linear-time attention scores normalization",
        "link": "https://arxiv.org/abs/2303.17475",
        "description": "arXiv:2303.17475v3 Announce Type: replace \nAbstract: The attention score matrix ${\\rm SoftMax}(XY^T)$ encodes relational similarity patterns between objects and is extremely popular in machine learning. However, the complexity required to calculate it runs quadratically with the problem size, making it a computationally heavy solution. In this article, we propose a linear-time approximation of the attention score normalization constants for embedding vectors with bounded norms. We show on several pre-trained embeddings that the accuracy of our estimation formula surpasses competing kernel methods by even orders of magnitude. From this result, we design a linear-time and task-agnostic embedding algorithm based on the optimization of the attention scores. The proposed algorithm is highly interpretable and easily adapted to an arbitrary embedding problem. We consider a few use-cases and observe similar or higher performances and a lower computational time with respect to comparable embedding algorithms.",
        "published": "2024-10-31 04:00:00",
        "id": "f4b85130-24ad-4f61-99ef-537369ba7e95",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种对有界范数嵌入向量的注意力分数归一化常数的线性时间近似，其估计公式精度超竞争核方法，据此设计线性时间、任务无关的嵌入算法，性能类似或更高且计算时间更低。"
        },
        "tokens": 774
    },
    {
        "title": "Integrating One-Shot View Planning with a Single Next-Best View via Long-Tail Multiview Sampling",
        "link": "https://arxiv.org/abs/2304.00910",
        "description": "arXiv:2304.00910v4 Announce Type: replace \nAbstract: Existing view planning systems either adopt an iterative paradigm using next-best views (NBV) or a one-shot pipeline relying on the set-covering view-planning (SCVP) network. However, neither of these methods can concurrently guarantee both high-quality and high-efficiency reconstruction of 3D unknown objects. To tackle this challenge, we introduce a crucial hypothesis: with the availability of more information about the unknown object, the prediction quality of the SCVP network improves. There are two ways to provide extra information: (1) leveraging perception data obtained from NBVs, and (2) training on an expanded dataset of multiview inputs. In this work, we introduce a novel combined pipeline that incorporates a single NBV before activating the proposed multiview-activated (MA-)SCVP network. The MA-SCVP is trained on a multiview dataset generated by our long-tail sampling method, which addresses the issue of unbalanced multiview inputs and enhances the network performance. Extensive simulated experiments substantiate that our system demonstrates a significant surface coverage increase and a substantial 45% reduction in movement cost compared to state-of-the-art systems. Real-world experiments justify the capability of our system for generalization and deployment.",
        "published": "2024-10-31 04:00:00",
        "id": "d8f0644b-13db-4f73-8eb6-2818ffe57287",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决现有视图规划系统难以兼顾3D未知物体高质量和高效重建的问题，提出在激活多视图激活（MA-）集覆盖视图规划（SCVP）网络前加入单个次优视图（NBV）的新组合管道，MA - SCVP通过长尾采样方法生成的多视图数据集训练，实验表明该系统有优势。"
        },
        "tokens": 894
    },
    {
        "title": "Set-based Neural Network Encoding Without Weight Tying",
        "link": "https://arxiv.org/abs/2305.16625",
        "description": "arXiv:2305.16625v2 Announce Type: replace \nAbstract: We propose a neural network weight encoding method for network property prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a model zoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \\textbf{S}et-based \\textbf{N}eural network \\textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks. To respect symmetries inherent in network weight space, we utilize Logit Invariance to learn the required minimal invariance properties. Additionally, we introduce a \\textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network property prediction: cross-dataset and cross-architecture. In cross-dataset property prediction, we evaluate how well property predictors generalize across model zoos trained on different datasets but of the same architecture. In cross-architecture property prediction, we evaluate how well property predictors transfer to model zoos of different architecture not seen during training. We show that SNE outperforms the relevant baselines on standard benchmarks.",
        "published": "2024-10-31 04:00:00",
        "id": "04263273-380c-4b36-a2af-05a73b839d5e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种神经网络权重编码方法SNE，可对不同架构网络编码，考虑网络层次结构、利用Logit不变性，还有‘pad - chunk - encode’管道，引入两个神经网络属性预测新任务，SNE在标准基准测试中优于相关基线。"
        },
        "tokens": 864
    },
    {
        "title": "Combining Robust Control and Machine Learning for Uncertain Nonlinear Systems Subject to Persistent Disturbances",
        "link": "https://arxiv.org/abs/2303.11890",
        "description": "arXiv:2303.11890v2 Announce Type: replace \nAbstract: This paper proposes a control strategy consisting of a robust controller and an Echo State Network (ESN) based control law for stabilizing a class of uncertain nonlinear discrete-time systems subject to persistent disturbances. Firstly, the robust controller is designed to ensure that the closed-loop system is Input-to-State Stable (ISS) with a guaranteed stability region regardless of the ESN control action and exogenous disturbances. Then, the ESN based controller is trained in order to mitigate the effects of disturbances on the system output. A numerical example demonstrates the potentials of the proposed control design method.",
        "published": "2024-10-31 04:00:00",
        "id": "407b9b00-9625-492f-bb01-843f67e25c12",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种由鲁棒控制器和基于回声状态网络（ESN）的控制律组成的控制策略，用于稳定受持续扰动的不确定非线性离散时间系统，鲁棒控制器确保闭环系统稳定，ESN控制器减轻扰动影响，数值示例展示该方法潜力。"
        },
        "tokens": 742
    },
    {
        "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution",
        "link": "https://arxiv.org/abs/2306.06755",
        "description": "arXiv:2306.06755v4 Announce Type: replace \nAbstract: In this paper, we present an LLM-based code translation method and an associated tool called CoTran, that translates whole-programs from one high-level programming language to another. Existing LLM-based code translation methods lack training to ensure that the translated code reliably compiles or bears substantial functional equivalence to the input code. In our work, we fine-tune an LLM using reinforcement learning, incorporating compiler feedback, and symbolic execution (symexec)-based testing feedback to assess functional equivalence between the input and output programs. The idea is to guide an LLM during fine-tuning, via compiler and symexec-based testing feedback, by letting it know how far it is from producing perfect translations. We conduct extensive experiments comparing CoTran with 14 other code translation tools, including human-written transpilers, LLM-based translation tools, and ChatGPT. Using a benchmark of over \\num{57000} code pairs in Java and Python, we demonstrate that CoTran outperforms the other tools on relevant metrics such as compilation accuracy (CompAcc) and functional equivalence accuracy (FEqAcc). For example, in Python-to-Java translation, CoTran achieves 48.68% FEqAcc and 76.98% CompAcc, whereas the nearest competing tool (PLBART-base) gets 38.26% and 75.77% respectively. Additionally, CoTran, built on top of CodeT5, improves FEqAcc by +14.89% and CompAcc by +8.14% for Python-to-Java (resp., +12.94% and +4.30% for Java-to-Python).",
        "published": "2024-10-31 04:00:00",
        "id": "18ee6e29-5fa0-40c9-8ec3-da93737ff1f1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍基于LLM的代码翻译方法CoTran，用强化学习、编译器反馈和符号执行测试反馈微调LLM，实验表明它在代码对基准测试中，比其他14种工具在编译准确性和功能等效准确性等指标上表现更好。"
        },
        "tokens": 980
    },
    {
        "title": "Towards Heterogeneous Long-tailed Learning: Benchmarking, Metrics, and Toolbox",
        "link": "https://arxiv.org/abs/2307.08235",
        "description": "arXiv:2307.08235v2 Announce Type: replace \nAbstract: Long-tailed data distributions pose challenges for a variety of domains like e-commerce, finance, biomedical science, and cyber security, where the performance of machine learning models is often dominated by head categories while tail categories are inadequately learned. This work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. We develop HeroLT, a comprehensive long-tailed learning benchmark integrating 18 state-of-the-art algorithms, 10 evaluation metrics, and 17 real-world datasets across 6 tasks and 4 data modalities. HeroLT with novel angles and extensive experiments (315 in total) enables effective and fair evaluation of newly proposed methods compared with existing baselines on varying dataset types. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://github.com/SSSKJ/HeroLT.",
        "published": "2024-10-31 04:00:00",
        "id": "cfdfa1f4-161b-4c8a-8a75-cfcc6b60b895",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍了HeroLT这一长尾学习基准，从数据长尾性、数据复杂性和新兴任务的异质性三个角度提供长尾学习的系统视图，包含18种算法、10种评估指标、17个数据集，进行315次实验，且开源相关内容。"
        },
        "tokens": 854
    },
    {
        "title": "Zero-Shot Reinforcement Learning from Low Quality Data",
        "link": "https://arxiv.org/abs/2309.15178",
        "description": "arXiv:2309.15178v3 Announce Type: replace \nAbstract: Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline, reward-free pre-training phase. Methods leveraging successor measures and successor features have shown strong performance in this setting, but require access to large heterogenous datasets for pre-training which cannot be expected for most real problems. Here, we explore how the performance of zero-shot RL methods degrades when trained on small homogeneous datasets, and propose fixes inspired by conservatism, a well-established feature of performant single-task offline RL algorithms. We evaluate our proposals across various datasets, domains and tasks, and show that conservative zero-shot RL algorithms outperform their non-conservative counterparts on low quality datasets, and perform no worse on high quality datasets. Somewhat surprisingly, our proposals also outperform baselines that get to see the task during training. Our code is available via https://enjeeneer.io/projects/zero-shot-rl/ .",
        "published": "2024-10-31 04:00:00",
        "id": "d6b26d0a-8172-40cf-9b39-cf4cd888b21e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章探讨零-shot强化学习在小而同质数据集上训练时性能如何下降，并提出受保守性启发的修复方法，经多种数据集、领域和任务评估，保守算法在低质量数据集上表现更好，在高质量数据集上表现不差，甚至优于训练时能看到任务的基线。"
        },
        "tokens": 811
    },
    {
        "title": "Fairness in Ranking under Disparate Uncertainty",
        "link": "https://arxiv.org/abs/2309.01610",
        "description": "arXiv:2309.01610v4 Announce Type: replace \nAbstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even in the presence of disparate uncertainty. EOR optimizes for an even cost burden on all groups, unlike the conventional Probability Ranking Principle, and is fundamentally different from existing notions of fairness in rankings, such as demographic parity and proportional Rooney rule constraints that are motivated by proportional representation relative to group size. To make EOR ranking practical, we present an efficient algorithm for computing it in time $O(n \\log(n))$ and prove its close approximation guarantee to the globally optimal solution. In a comprehensive empirical evaluation on synthetic data, a US Census dataset, and a real-world audit of Amazon search queries, we find that the algorithm reliably guarantees EOR fairness while providing effective rankings.",
        "published": "2024-10-31 04:00:00",
        "id": "d5610d05-5099-4891-bfa7-8b4087ca3b05",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出平等机会排名（EOR）作为排名的新公平标准，阐述其特性，给出高效计算算法并证明近似最优解，经多种数据评估表明算法能保证EOR公平且提供有效排名。"
        },
        "tokens": 907
    },
    {
        "title": "StyleAdapter: A Unified Stylized Image Generation Model",
        "link": "https://arxiv.org/abs/2309.01770",
        "description": "arXiv:2309.01770v2 Announce Type: replace \nAbstract: This work focuses on generating high-quality images with specific style of reference images and content of provided textual descriptions. Current leading algorithms, i.e., DreamBooth and LoRA, require fine-tuning for each style, leading to time-consuming and computationally expensive processes. In this work, we propose StyleAdapter, a unified stylized image generation model capable of producing a variety of stylized images that match both the content of a given prompt and the style of reference images, without the need for per-style fine-tuning. It introduces a two-path cross-attention (TPCA) module to separately process style information and textual prompt, which cooperate with a semantic suppressing vision model (SSVM) to suppress the semantic content of style images. In this way, it can ensure that the prompt maintains control over the content of the generated images, while also mitigating the negative impact of semantic information in style references. This results in the content of the generated image adhering to the prompt, and its style aligning with the style references. Besides, our StyleAdapter can be integrated with existing controllable synthesis methods, such as T2I-adapter and ControlNet, to attain a more controllable and stable generation process. Extensive experiments demonstrate the superiority of our method over previous works.",
        "published": "2024-10-31 04:00:00",
        "id": "1a78526a-9454-4317-bbee-e624f1761bb5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "StyleAdapter是一种统一的风格化图像生成模型，无需对每种风格进行微调就能生成符合给定提示内容和参考图像风格的图像，通过TPCA模块和SSVM实现，可与现有方法集成，实验证明其优于以前的工作。"
        },
        "tokens": 865
    },
    {
        "title": "Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks",
        "link": "https://arxiv.org/abs/2309.12927",
        "description": "arXiv:2309.12927v3 Announce Type: replace \nAbstract: Recurrent neural networks (RNNs) in the brain and in silico excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale). However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$ by simultaneously optimizing recurrent weights and $\\tau$s. We find that for both tasks RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\\tau$ with $N$ and are able to solve tasks for large $N$, but they suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\\tau$ constant and develop longer timescales through recurrent connectivity. Moreover, we show that the multi-head curriculum increases training speed and network stability to ablations and perturbations, and allows RNNs to generalize better to tasks beyond their training regime. This curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.",
        "published": "2024-10-31 04:00:00",
        "id": "a9ed02f1-cef0-4ebb-b106-0058c0e5f5fc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究人员训练循环神经网络（RNNs）解决记忆相关任务，发现不同学习目标（单头、多头）下RNNs发展时长尺度的机制不同，多头课程可提高训练速度、网络稳定性和泛化能力。"
        },
        "tokens": 956
    },
    {
        "title": "Integration of Large Language Models and Federated Learning",
        "link": "https://arxiv.org/abs/2307.08925",
        "description": "arXiv:2307.08925v3 Announce Type: replace \nAbstract: As the parameter size of Large Language Models (LLMs) continues to expand, there is an urgent need to address the scarcity of high-quality data. In response, existing research has attempted to make a breakthrough by incorporating Federated Learning (FL) into LLMs. Conversely, considering the outstanding performance of LLMs in task generalization, researchers have also tried applying LLMs within FL to tackle challenges in relevant domains. The complementarity between LLMs and FL has already ignited widespread research interest. In this paper, we aim to deeply explore the integration of LLMs and FL. We propose a research framework, dividing the fusion of LLMs and FL into three parts: the combination of LLM sub-technologies with FL, the integration of FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We first provide a comprehensive review of the current state of research in the domain of LLMs combined with FL, including their typical applications, integration advantages, challenges faced, and future directions for resolution. Subsequently, we discuss the practical applications of the combination of LLMs and FL in critical scenarios such as healthcare, finance, and education, and provide new perspectives and insights into future research directions for LLMs and FL.",
        "published": "2024-10-31 04:00:00",
        "id": "7e68309c-b4bc-4e4c-b84e-8ed169591e1f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出一个研究框架，将大型语言模型与联邦学习的融合分为三部分，对二者结合的研究现状进行综述，包括典型应用、优势、挑战和未来方向，并讨论其在医疗、金融和教育等场景中的实际应用。"
        },
        "tokens": 860
    },
    {
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "link": "https://arxiv.org/abs/2310.01801",
        "description": "arXiv:2310.01801v4 Announce Type: replace \nAbstract: In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
        "published": "2024-10-31 04:00:00",
        "id": "d97a5ae9-a97e-458e-ab18-afd85dd94dd0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍自适应KV缓存压缩方法以减少大型语言模型生成推理的内存占用，通过针对性分析构建自适应KV缓存，FastGen无需大量资源微调或重新训练即可部署，实验中FastGen减少GPU内存消耗且生成质量损失可忽略不计"
        },
        "tokens": 813
    },
    {
        "title": "Mapping the DeFi Crime Landscape: An Evidence-based Picture",
        "link": "https://arxiv.org/abs/2310.04356",
        "description": "arXiv:2310.04356v2 Announce Type: replace \nAbstract: Decentralized finance (DeFi) has been the target of numerous profit-driven crimes, but the prevalence and cumulative impact of these crimes have not yet been assessed. This study provides a comprehensive assessment of profit-driven crimes targeting the DeFi sector. We collected data on 1153 crime events from 2017 to 2022. Of these, 1,048 were related to DeFi (the main focus of this study) and 105 to centralized finance (CeFi). The findings show that the entire cryptoasset industry has suffered a minimum loss of US$30B, with two thirds related to CeFi and one third to DeFi. Focusing on DeFi, a taxonomy was developed to clarify the similarities and differences among these crimes. All events were mapped onto the DeFi stack to assess the impacted technical layers, and the financial damages were quantified to gauge their scale. The results highlight that during an attack, a DeFi actor (an entity developing a DeFi technology) can serve as a direct target (due to technical vulnerabilities or exploitation of human risks), as a perpetrator (through malicious uses of contracts or market manipulations), or as an intermediary (by being imitated through, for example, phishing scams). The findings also show that DeFi actors are the first victims of crimes targeting the DeFi industry: 52.2% of events targeted them, primarily due to technical vulnerabilities at the protocol layer, and these events accounted for 83% of all financial damages. Alternatively, in 40.7% of events, DeFi actors were themselves malicious perpetrators, predominantly misusing contracts at the cryptoasset layer (e.g., rug pull scams). However, these events accounted for only 17% of all financial damages. The study offers a preliminary assessment of the size and scope of crime events within the DeFi sector and highlights the vulnerable position of DeFi actors in the ecosystem.",
        "published": "2024-10-31 04:00:00",
        "id": "75ccf02f-efc2-4a49-8caa-60575773c57f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该研究对2017 - 2022年1153起针对金融（主要是去中心化金融DeFi）的盈利性犯罪事件进行数据收集，分析了DeFi犯罪的类型、目标、损害规模等情况。"
        },
        "tokens": 1016
    },
    {
        "title": "Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction",
        "link": "https://arxiv.org/abs/2310.05185",
        "description": "arXiv:2310.05185v3 Announce Type: replace \nAbstract: Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs remains at a coarse-grained level, which is always in a single schema, ignoring the order and variable arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging and output merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. The experimental results demonstrate that Text2NKG achieves state-of-the-art performance in F1 scores on the fine-grained n-ary relation extraction benchmark. Our code and datasets are publicly available.",
        "published": "2024-10-31 04:00:00",
        "id": "3450b65c-7481-4b61-95f0-bcdf6286860a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出Text2NKG框架用于构建n元关系知识图谱，通过跨元组分类方法实现细粒度的n元关系抽取，支持四种典型模式，实验结果在F1分数上达到最先进水平且代码和数据集公开。"
        },
        "tokens": 842
    },
    {
        "title": "Predictive auxiliary objectives in deep RL mimic learning in the brain",
        "link": "https://arxiv.org/abs/2310.06089",
        "description": "arXiv:2310.06089v3 Announce Type: replace \nAbstract: The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain -- that of an auxiliary learning system that benefits representation learning in other regions.",
        "published": "2024-10-31 04:00:00",
        "id": "089afa33-41b5-4714-ba5a-a7d7a8fcb97a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究深度强化学习中预测辅助目标对表示学习的影响及其与大脑表征变化的相似性，发现预测目标在资源有限架构中改善和稳定学习，且特定设置下长预测范围更利于表征转移。"
        },
        "tokens": 869
    },
    {
        "title": "IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training",
        "link": "https://arxiv.org/abs/2310.07355",
        "description": "arXiv:2310.07355v5 Announce Type: replace \nAbstract: In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furthermore, a new clinical-informed contrastive loss is introduced for cross-modal learning, which accounts for clinical prior knowledge in formulating sample correlations in contrastive learning. The proposed model, IMITATE, outperforms baseline VLP methods across six different datasets, spanning five medical imaging downstream tasks. Comprehensive experimental results highlight the advantages of integrating the hierarchical structure of medical reports for vision-language alignment. The code related to this paper is available at https://github.com/cheliu-computation/IMITATE-TMI2024.",
        "published": "2024-10-31 04:00:00",
        "id": "b0a934e9-1b03-4426-aafb-8d255f3aa135",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为IMITATE的临床先验引导的视觉 - 语言预训练框架，通过分层视觉 - 语言对齐从医疗报告中学习结构信息，引入新的临床感知对比损失，在六个数据集上优于基线方法。"
        },
        "tokens": 892
    },
    {
        "title": "Optimal Linear Decay Learning Rate Schedules and Further Refinements",
        "link": "https://arxiv.org/abs/2310.07831",
        "description": "arXiv:2310.07831v2 Announce Type: replace \nAbstract: Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our main technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). When considering only worst-case analysis, our theory predicts that the optimal choice is the linear decay schedule where the step-size is set proportional to 1 - t/T, where t is the current iteration and T is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate annealing near the end of training. Ours is the first systematic approach to automatically yield both of these properties. We perform the most comprehensive evaluation of learning rate schedules to date, evaluating across 10 diverse deep learning problems, a series of LLMs, and a suite of logistic regression problems. We validate that overall, the linear-decay schedule outperforms all commonly used default schedules including cosine annealing. Our adaptive schedule refinement method gives further improvements.",
        "published": "2024-10-31 04:00:00",
        "id": "d4d7d041-3bfe-4376-a3ab-b31fb45cf847",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文分析优化算法学习率调度，提出线性衰减调度为最优选择，还可根据梯度范数得出自适应调度，经多种问题评估验证其有效性。"
        },
        "tokens": 835
    },
    {
        "title": "Yuga: Automatically Detecting Lifetime Annotation Bugs in the Rust Language",
        "link": "https://arxiv.org/abs/2310.08507",
        "description": "arXiv:2310.08507v2 Announce Type: replace \nAbstract: The Rust programming language is becoming increasingly popular among systems programmers due to its efficient performance and robust memory safety guarantees. Rust employs an ownership model to ensure this guarantee by allowing each value to be owned by only one identifier at a time. Additionally, it introduces the concept of borrowing and lifetimes to enable other variables to borrow the values under certain conditions temporarily. Despite its benefits, security vulnerabilities have been reported in Rust projects, often attributed to the use of \"unsafe\" Rust code. These vulnerabilities, in part, arise from incorrect lifetime annotations on function signatures. However, existing tools fail to detect these bugs, primarily because such bugs are rare, challenging to detect through dynamic analysis, and require explicit memory models. To overcome these limitations, first, we characterize incorrect lifetime annotations as a source of memory safety bugs and leverage this understanding to devise a novel static analysis tool, Yuga, to detect potential lifetime annotation bugs. Yuga uses a multi-phase analysis approach, starting with a quick pattern-matching algorithm to identify potential buggy components and then conducting a flow and field-sensitive alias analysis to confirm the bugs. We also curate new datasets of lifetime annotation bugs. Yuga successfully detects bugs with good precision on these datasets, and we make the code and datasets publicly available for review.",
        "published": "2024-10-31 04:00:00",
        "id": "bab00f8f-a6be-480c-a12d-9134ab71b7e9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Yuga是一种新的静态分析工具，可检测Rust语言中潜在的生命周期注释错误，其采用多阶段分析方法且已在新数据集上成功检测出错误并公开代码和数据集。"
        },
        "tokens": 859
    },
    {
        "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
        "link": "https://arxiv.org/abs/2310.08975",
        "description": "arXiv:2310.08975v3 Announce Type: replace \nAbstract: Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: inefficient knowledge retrieval, mistakes of retrieval adversely impacting semantic parsing, and the complexity of previous KBQA methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly. Experimental results show that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering. Our code is publicly available.",
        "published": "2024-10-31 04:00:00",
        "id": "bdee7b38-895c-4efa-ba6d-69aed612d61a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "ChatKBQA是一种新的知识库问答框架，先使用微调的大型语言模型生成逻辑形式，再用无监督检索方法检索和替换实体与关系，在标准KBQA数据集上达到新的最优性能，其代码已公开。"
        },
        "tokens": 819
    },
    {
        "title": "On Unsupervised Partial Shape Correspondence",
        "link": "https://arxiv.org/abs/2310.14692",
        "description": "arXiv:2310.14692v3 Announce Type: replace \nAbstract: While dealing with matching shapes to their parts, we often apply a tool known as functional maps. The idea is to translate the shape matching problem into \"convenient\" spaces by which matching is performed algebraically by solving a least squares problem. Here, we argue that such formulations, though popular in this field, introduce errors in the estimated match when partiality is invoked. Such errors are unavoidable even for advanced feature extraction networks, and they can be shown to escalate with increasing degrees of shape partiality, adversely affecting the learning capability of such systems. To circumvent these limitations, we propose a novel approach for partial shape matching. Our study of functional maps led us to a novel method that establishes direct correspondence between partial and full shapes through feature matching bypassing the need for functional map intermediate spaces. The Gromov Distance between metric spaces leads to the construction of the first part of our loss functions. For regularization we use two options: a term based on the area preserving property of the mapping, and a relaxed version that avoids the need to resort to functional maps. The proposed approach shows superior performance on the SHREC'16 dataset, outperforming existing unsupervised methods for partial shape matching.Notably, it achieves state-of-the-art results on the SHREC'16 HOLES benchmark, superior also compared to supervised methods. We demonstrate the benefits of the proposed unsupervised method when applied to a new dataset PFAUST for part-to-full shape correspondence.",
        "published": "2024-10-31 04:00:00",
        "id": "7c944638-89de-4726-8824-0dfc60c259c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的部分形状匹配方法，通过特征匹配建立部分和完整形状间的直接对应关系，在SHREC'16数据集上表现优越并在PFAUST新数据集展示优势。"
        },
        "tokens": 894
    },
    {
        "title": "Sui Lutris: A Blockchain Combining Broadcast and Consensus",
        "link": "https://arxiv.org/abs/2310.18042",
        "description": "arXiv:2310.18042v5 Announce Type: replace \nAbstract: Sui Lutris is the first smart-contract platform to sustainably achieve sub-second finality. It achieves this significant decrease by employing consensusless agreement not only for simple payments but for a large variety of transactions. Unlike prior work, Sui Lutris neither compromises expressiveness nor throughput and can run perpetually without restarts. Sui Lutris achieves this by safely integrating consensuless agreement with a high-throughput consensus protocol that is invoked out of the critical finality path but ensures that when a transaction is at risk of inconsistent concurrent accesses, its settlement is delayed until the total ordering is resolved. Building such a hybrid architecture is especially delicate during reconfiguration events, where the system needs to preserve the safety of the consensusless path without compromising the long-term liveness of potentially misconfigured clients. We thus develop a novel reconfiguration protocol, the first to provably show the safe and efficient reconfiguration of a consensusless blockchain. Sui Lutris is currently running in production and underpins the Sui smart-contract platform. Combined with the use of Objects instead of accounts it enables the safe execution of smart contracts that expose objects as a first-class resource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds for throughput up to 5,000 certificates per second (150k ops/s with transaction blocks), compared to the state-of-the-art real-world consensus latencies of 3 seconds. Furthermore, it gracefully handles validators crash-recovery and does not suffer visible performance degradation during reconfiguration.",
        "published": "2024-10-31 04:00:00",
        "id": "d779c903-1b5f-42ce-bcf4-69672bc72091",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Sui Lutris是首个可持续实现亚秒级最终性的智能合约平台，它将无共识协议与高吞吐量共识协议安全集成，在实验中实现低于0.5秒的延迟（每秒吞吐量达5000个证书），目前已用于生产并支撑Sui智能合约平台。"
        },
        "tokens": 942
    },
    {
        "title": "FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction",
        "link": "https://arxiv.org/abs/2310.19453",
        "description": "arXiv:2310.19453v4 Announce Type: replace \nAbstract: Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality, which capture the collaborative signals via feature interaction modeling. But the one-hot encoding discards the semantic information included in the textual features. Recently, the emergence of Pretrained Language Models(PLMs) has given rise to another paradigm, which takes as inputs the sentences of textual modality obtained by hard prompt templates and adopts PLMs to extract the semantic knowledge. However, PLMs often face challenges in capturing field-wise collaborative signals and distinguishing features with subtle textual differences. In this paper, to leverage the benefits of both paradigms and meanwhile overcome their limitations, we propose to conduct Fine-grained feature-level ALignment between ID-based Models and Pretrained Language Models(FLIP) for CTR prediction. Unlike most methods that solely rely on global views through instance-level contrastive learning, we design a novel jointly masked tabular/language modeling task to learn fine-grained alignment between tabular IDs and word tokens. Specifically, the masked data of one modality (IDs and tokens) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose to jointly finetune the ID-based model and PLM by adaptively combining the output of both models, thus achieving superior performance in downstream CTR prediction tasks. Extensive experiments on three real-world datasets demonstrate that FLIP outperforms SOTA baselines, and is highly compatible with various ID-based models and PLMs. The code is at \\url{https://github.com/justarter/FLIP}.",
        "published": "2024-10-31 04:00:00",
        "id": "0c1ddbef-7856-4055-8413-07cbafa6ac8f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决传统基于ID的CTR预测模型和预训练语言模型（PLMs）各自的局限性，提出FLIP，通过联合掩码表格/语言建模任务实现细粒度对齐，自适应组合模型输出以在CTR预测任务中取得更好性能。"
        },
        "tokens": 973
    },
    {
        "title": "Orion: A Fully Homomorphic Encryption Framework for Deep Learning",
        "link": "https://arxiv.org/abs/2311.03470",
        "description": "arXiv:2311.03470v2 Announce Type: replace \nAbstract: Fully Homomorphic Encryption (FHE) has the potential to substantially improve privacy and security by enabling computation directly on encrypted data. This is especially true with deep learning, as today, many popular user services are powered by neural networks in the cloud. One of the major challenges facing wide-scale deployment of FHE-secured neural inference is effectively mapping these networks to FHE primitives. FHE poses many programming challenges including packing large vectors, automatically managing noise via bootstrapping, and translating arbitrary and general-purpose programs to the limited instruction set provided by FHE. These challenges make building large FHE neural networks intractable using the tools available today.\n  In this paper we address these challenges with Orion, a fully-automated framework for private neural inference in FHE. Orion accepts deep neural networks written in PyTorch and translates them into efficient FHE programs. We achieve this by proposing a novel single-shot multiplexed packing strategy for arbitrary convolutions and through a new, efficient technique to automate bootstrap placement. We evaluate Orion on common benchmarks used by the FHE deep learning community and outperform state-of-the-art by $2.38 \\times$ on ResNet-20, the largest network they report. Orion extends naturally to larger networks. We demonstrate this by evaluating ResNet-50 on ImageNet and present the first high-resolution homomorphic object detection experiments using a YOLO-v1 model with 139 million parameters. Finally, we open-source our framework Orion at the following repository: https://github.com/baahl-nyu/orion",
        "published": "2024-10-31 04:00:00",
        "id": "e1be256f-dc36-4678-b0df-19bda7b00da2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Orion是一个用于全同态加密（FHE）中私有神经推理的全自动框架，它能将PyTorch编写的深度神经网络转换为高效的FHE程序，在常见基准测试中表现优于现有技术，已开源。"
        },
        "tokens": 932
    },
    {
        "title": "Structure and inference in hypergraphs with node attributes",
        "link": "https://arxiv.org/abs/2311.03857",
        "description": "arXiv:2311.03857v2 Announce Type: replace \nAbstract: Many networked datasets with units interacting in groups of two or more, encoded with hypergraphs, are accompanied by extra information about nodes, such as the role of an individual in a workplace. Here we show how these node attributes can be used to improve our understanding of the structure resulting from higher-order interactions. We consider the problem of community detection in hypergraphs and develop a principled model that combines higher-order interactions and node attributes to better represent the observed interactions and to detect communities more accurately than using either of these types of information alone. The method learns automatically from the input data the extent to which structure and attributes contribute to explain the data, down weighing or discarding attributes if not informative. Our algorithmic implementation is efficient and scales to large hypergraphs and interactions of large numbers of units. We apply our method to a variety of systems, showing strong performance in hyperedge prediction tasks and in selecting community divisions that correlate with attributes when these are informative, but discarding them otherwise. Our approach illustrates the advantage of using informative node attributes when available with higher-order data.",
        "published": "2024-10-31 04:00:00",
        "id": "458009c1-9941-4a8e-8b4c-493706ac9ca0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究提出一种结合高阶交互和节点属性的原则模型，用于超图中的社区检测，自动学习结构和属性对解释数据的贡献程度，算法实现高效且可扩展，在多种系统中有良好表现。"
        },
        "tokens": 825
    },
    {
        "title": "Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning",
        "link": "https://arxiv.org/abs/2311.08110",
        "description": "arXiv:2311.08110v3 Announce Type: replace \nAbstract: Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining, a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.",
        "published": "2024-10-31 04:00:00",
        "id": "a13eea06-fd65-490b-a85b-ea1ada7b0c10",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出通过检索引导对比训练构建仇恨感知嵌入空间以改进仇恨模因检测，新方法在HatefulMemes数据集上达到最先进性能，还展示了基于检索的检测系统可利用未见过的数据识别仇恨性。"
        },
        "tokens": 786
    },
    {
        "title": "Banyan: Fast Rotating Leader BFT",
        "link": "https://arxiv.org/abs/2312.05869",
        "description": "arXiv:2312.05869v2 Announce Type: replace \nAbstract: This paper presents Banyan, the first rotating leader state machine replication (SMR) protocol that allows transactions to be confirmed in just a single round-trip time in the Byzantine fault tolerance (BFT) setting. Based on minimal alterations to the Internet Computer Consensus (ICC) protocol and with negligible communication overhead, we introduce a novel dual mode mechanism that enables optimal block finalization latency in the fast path. Crucially, the modes of operation are integrated, such that even if the fast path is not effective, no penalties are incurred. Moreover, our algorithm maintains the core attributes of the ICC protocol it is based on, including optimistic responsiveness and rotating leaders without the necessity for a view-change protocol. We prove the correctness of our protocol and provide an open-source implementation of it. Banyan is compared to its predecessor ICC, as well as other well known BFT protocols, in a globally distributed wide-area network. Our evaluation reveals that Banyan reduces latency by up to 30% compared to state-of-the-art protocols, without requiring additional security assumptions.",
        "published": "2024-10-31 04:00:00",
        "id": "a6be5a05-822b-41cd-881c-48dcac6281ca",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍Banyan协议，它是首个旋转领导者状态机复制协议，基于对互联网计算机共识协议的微小改动，有双模式机制，能减少延迟，已被证明正确且有开源实现，经评估与其他协议相比可减少30%延迟。"
        },
        "tokens": 826
    },
    {
        "title": "Successive Bayesian Reconstructor for Channel Estimation in Fluid Antenna Systems",
        "link": "https://arxiv.org/abs/2312.06551",
        "description": "arXiv:2312.06551v4 Announce Type: replace \nAbstract: Fluid antenna systems (FASs) can reconfigure their antenna locations freely within a spatially continuous space. To keep favorable antenna positions, the channel state information (CSI) acquisition for FASs is essential. While some techniques have been proposed, most existing FAS channel estimators require several channel assumptions, such as slow variation and angular-domain sparsity. When these assumptions are not reasonable, the model mismatch may lead to unpredictable performance losses. In this paper, we propose the successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS channels. Unlike model-based estimators, the proposed S-BAR is prior-aided, which builds the experiential kernel for CSI acquisition. Inspired by Bayesian regression, the key idea of S-BAR is to model the FAS channels as a stochastic process, whose uncertainty can be successively eliminated by kernel-based sampling and regression. In this way, the predictive mean of the regressed stochastic process can be viewed as a Bayesian channel estimator. Simulation results verify that, in both model-mismatched and model-matched cases, the proposed S-BAR can achieve higher estimation accuracy than the existing schemes.",
        "published": "2024-10-31 04:00:00",
        "id": "02d1038c-db80-4dff-b1d8-547c9a4f53ee",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对流体天线系统中的信道估计问题，提出先验辅助的连续贝叶斯重构器（S - BAR），仿真结果表明其在模型匹配和不匹配情况下比现有方案估计精度更高。"
        },
        "tokens": 838
    },
    {
        "title": "Tight Bounds for The Price of Fairness",
        "link": "https://arxiv.org/abs/2311.18339",
        "description": "arXiv:2311.18339v2 Announce Type: replace \nAbstract: A central decision maker (CDM), who seeks an efficient allocation of scarce resources among a finite number of players, often has to incorporate fairness criteria to avoid unfair outcomes. Indeed, the Price of Fairness (POF), a term coined in the seminal work by Bertsimas et al. (2011), refers to the efficiency loss due to the incorporation of fairness criteria into the allocation method. Quantifying the POF would help the CDM strike an appropriate balance between efficiency and fairness. In this paper we improve upon existing results in the literature, by providing tight bounds for the POF for the proportional fairness criterion for any $n$, when the maximum achievable utilities of the players are equal or are not equal. Further, while Bertsimas et al. (2011) have already derived a tight bound for the max-min fairness criterion for the case that all players have equal maximum achievable utilities, we also provide a tight bound in scenarios where these utilities are not equal. For both criteria, we characterize the conditions where the POF reaches its peak and provide the supremum bounds of our bounds over all maximum achievable utility vectors, which are shown to be asymptotically strictly smaller than the supremum of the Bertsimas et al. (2011) bounds. Finally, we investigate the sensitivity of our bounds and the bounds in Bertsimas et al. (2011) for the POF to the variability of the maximum achievable utilities.",
        "published": "2024-10-31 04:00:00",
        "id": "a6000474-8117-4d67-87f1-2d812ee2806b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "论文《Tight Bounds for The Price of Fairness》提供了比例公平准则和最大最小公平准则下价格公平性（POF）的紧界，改进了现有结果并分析了相关条件及上确界界的敏感性。"
        },
        "tokens": 910
    },
    {
        "title": "Anti-symmetric and Positivity Preserving Formulation of a Spectral Method for Vlasov-Poisson Equations",
        "link": "https://arxiv.org/abs/2312.05439",
        "description": "arXiv:2312.05439v3 Announce Type: replace \nAbstract: We analyze the anti-symmetric properties of a spectral discretization for the one-dimensional Vlasov-Poisson equations. The discretization is based on a spectral expansion in velocity with the symmetrically weighted Hermite basis functions, central finite differencing in space, and an implicit Runge Kutta integrator in time. The proposed discretization preserves the anti-symmetric structure of the advection operator in the Vlasov equation, resulting in a stable numerical method. We apply such discretization to two formulations: the canonical Vlasov-Poisson equations and their continuously transformed square-root representation. The latter preserves the positivity of the particle distribution function. We derive analytically the conservation properties of both formulations, including particle number, momentum, and energy, which are verified numerically on the following benchmark problems: manufactured solution, linear and nonlinear Landau damping, two-stream instability, bump-on-tail instability, and ion-acoustic wave.",
        "published": "2024-10-31 04:00:00",
        "id": "82e617ef-dd19-42fc-97c6-91d6fad5ce63",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "分析一维弗拉索夫 - 泊松方程频谱离散化的反对称性质，离散化基于速度频谱展开、空间中心有限差分和时间隐式龙格 - 库塔积分器，将其应用于两种公式并推导其守恒性质，通过基准问题进行数值验证。"
        },
        "tokens": 818
    },
    {
        "title": "Learning Dynamic Selection and Pricing of Out-of-Home Deliveries",
        "link": "https://arxiv.org/abs/2311.13983",
        "description": "arXiv:2311.13983v3 Announce Type: replace \nAbstract: Home delivery failures, traffic congestion, and relatively large handling times have a negative impact on the profitability of last-mile logistics. A potential solution is the delivery to parcel lockers or parcel shops, denoted by out-of-home (OOH) delivery. In the academic literature, models for OOH delivery were so far limited to static settings, contrasting with the sequential nature of the problem. We model the sequential decision-making problem of which OOH location to offer against what incentive for each incoming customer, taking into account future customer arrivals and choices. We propose Dynamic Selection and Pricing of OOH (DSPO), an algorithmic pipeline that uses a novel spatial-temporal state encoding as input to a convolutional neural network. We demonstrate the performance of our method by benchmarking it against two state-of-the-art approaches. Our extensive numerical study, guided by real-world data, reveals that DSPO can save 19.9%pt in costs compared to a situation without OOH locations, 7%pt compared to a static selection and pricing policy, and 3.8%pt compared to a state-of-the-art demand management benchmark. We provide comprehensive insights into the complex interplay between OOH delivery dynamics and customer behavior influenced by pricing strategies. The implications of our findings suggest practitioners to adopt dynamic selection and pricing policies.",
        "published": "2024-10-31 04:00:00",
        "id": "88a09588-ded0-462d-b6f1-f516e8e5e05e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为DSPO的算法管道用于动态选择和定价户外配送，通过实验证明其相比其他方法能节省成本，揭示了户外配送动态与客户行为受定价策略影响的复杂相互作用。"
        },
        "tokens": 867
    },
    {
        "title": "The Adoption and Efficacy of Large Language Models: Evidence From Consumer Complaints in the Financial Industry",
        "link": "https://arxiv.org/abs/2311.16466",
        "description": "arXiv:2311.16466v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) are reshaping consumer decision-making, particularly in communication with firms, yet our understanding of their impact remains limited. This research explores the effect of LLMs on consumer complaints submitted to the Consumer Financial Protection Bureau from 2015 to 2024, documenting the adoption of LLMs for drafting complaints and evaluating the likelihood of obtaining relief from financial firms. Utilizing a leading AI detection tool, we analyzed over 1 million complaints and identified a significant increase in LLM usage following the release of ChatGPT. We establish a causal relationship between LLM usage and an increased likelihood of obtaining relief by employing instrumental variables to address endogeneity in LLM adoption. Experimental data further support this link, demonstrating that LLMs enhance the clarity and persuasiveness of consumer narratives. Our findings suggest that facilitating access to LLMs can help firms better understand consumer concerns and level the playing field among consumers. This underscores the importance of policies promoting technological accessibility, enabling all consumers to effectively voice their concerns.",
        "published": "2024-10-31 04:00:00",
        "id": "d2b081b6-cd46-4854-bb29-6359a485e0fe",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究探索2015 - 2024年大型语言模型（LLMs）对消费者向消费者金融保护局提交投诉的影响，发现ChatGPT发布后LLM使用显著增加，使用工具变量确定LLM使用与获得救济可能性增加存在因果关系，实验数据表明LLMs可增强消费者叙述的清晰度和说服力。"
        },
        "tokens": 848
    },
    {
        "title": "Robust Control Barrier Functions using Uncertainty Estimation with Application to Mobile Robots",
        "link": "https://arxiv.org/abs/2401.01881",
        "description": "arXiv:2401.01881v2 Announce Type: replace \nAbstract: This paper proposes a safety-critical control design approach for nonlinear control affine systems in the presence of matched and unmatched uncertainties. Our constructive framework couples control barrier function (CBF) theory with a new uncertainty estimator to ensure robust safety. The estimated uncertainty with a derived upper bound on the estimation error is used for synthesizing CBFs and safety-critical controllers via a quadratic program-based feedback control law that rigorously ensures robust safety while improving disturbance rejection performance. The method is extended to higher-order CBFs (HOCBFs) to achieve safety under unmatched uncertainty, which may cause relative degree differences with respect to control input and disturbances. We assume the relative degree difference is at most one, resulting in a second-order cone constraint. The proposed robust HOCBF method is demonstrated via a simulation of an uncertain elastic actuator control problem. Finally, we experimentally demonstrated the efficacy of our robust CBF framework on a tracked robot with slope-induced matched and unmatched perturbations.",
        "published": "2024-10-31 04:00:00",
        "id": "45323ad4-333d-4fad-906b-34e5e6e50cd9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出一种针对存在匹配和非匹配不确定性的非线性控制仿射系统的安全关键控制设计方法，将控制障碍函数理论与新的不确定性估计器相结合确保鲁棒安全性，扩展到高阶控制障碍函数以在非匹配不确定性下实现安全，并通过模拟和实验进行了验证。"
        },
        "tokens": 823
    },
    {
        "title": "A Concentration Bound for TD(0) with Function Approximation",
        "link": "https://arxiv.org/abs/2312.10424",
        "description": "arXiv:2312.10424v2 Announce Type: replace \nAbstract: We derive a concentration bound of the type `for all $n \\geq n_0$ for some $n_0$' for TD(0) with linear function approximation. We work with online TD learning with samples from a single sample path of the underlying Markov chain. This makes our analysis significantly different from offline TD learning or TD learning with access to independent samples from the stationary distribution of the Markov chain. We treat TD(0) as a contractive stochastic approximation algorithm, with both martingale and Markov noises. Markov noise is handled using the Poisson equation and the lack of almost sure guarantees on boundedness of iterates is handled using the concept of relaxed concentration inequalities.",
        "published": "2024-10-31 04:00:00",
        "id": "24182e65-6571-41de-81ec-618f96c938d5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "推导了带有线性函数逼近的TD(0)的浓度界，将TD(0)视为收缩随机逼近算法，处理鞅和马尔可夫噪声以解决相关问题。"
        },
        "tokens": 739
    },
    {
        "title": "MetaSegNet: Metadata-collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images",
        "link": "https://arxiv.org/abs/2312.12735",
        "description": "arXiv:2312.12735v3 Announce Type: replace \nAbstract: Semantic segmentation of remote sensing images plays a vital role in a wide range of Earth Observation applications, such as land use land cover mapping, environment monitoring, and sustainable development. Driven by rapid developments in artificial intelligence, deep learning (DL) has emerged as the mainstream for semantic segmentation and has achieved many breakthroughs in the field of remote sensing. However, most DL-based methods focus on unimodal visual data while ignoring rich multimodal information involved in the real world. Non-visual data, such as text, can gather extra knowledge from the real world, which can strengthen the interpretability, reliability, and generalization of visual models. Inspired by this, we propose a novel metadata-collaborative segmentation network (MetaSegNet) that applies vision-language representation learning for semantic segmentation of remote sensing images. Unlike the common model structure that only uses unimodal visual data, we extract the key characteristic (e.g. the climate zone) from freely available remote sensing image metadata and transfer it into geographic text prompts via the generic ChatGPT. Then, we construct an image encoder, a text encoder, and a crossmodal attention fusion subnetwork to extract the image and text feature and apply image-text interaction. Benefiting from such a design, the proposed MetaSegNet not only demonstrates superior generalization in zero-shot testing but also achieves competitive accuracy with the state-of-the-art semantic segmentation methods on the large-scale OpenEarthMap dataset (70.4% mIoU) and the Potsdam dataset (93.3% mean F1 score) as well as the LoveDA dataset (52.0% mIoU).",
        "published": "2024-10-31 04:00:00",
        "id": "a8ea05f4-8153-4075-9f8e-6636ca8db753",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出MetaSegNet用于遥感图像语义分割，通过提取遥感图像元数据特征并转化为地理文本提示，构建图像、文本编码器和跨模态注意力融合子网进行图像 - 文本交互，在多个数据集上取得较好成果。"
        },
        "tokens": 950
    },
    {
        "title": "Algebraic Positional Encodings",
        "link": "https://arxiv.org/abs/2312.16045",
        "description": "arXiv:2312.16045v2 Announce Type: replace \nAbstract: We introduce a novel positional encoding strategy for Transformer-style models, addressing the shortcomings of existing, often ad hoc, approaches. Our framework provides a flexible mapping from the algebraic specification of a domain to an interpretation as orthogonal operators. This design preserves the algebraic characteristics of the source domain, ensuring that the model upholds its desired structural properties. Our scheme can accommodate various structures, ncluding sequences, grids and trees, as well as their compositions. We conduct a series of experiments to demonstrate the practical applicability of our approach. Results suggest performance on par with or surpassing the current state-of-the-art, without hyper-parameter optimizations or \"task search\" of any kind. Code is available at https://github.com/konstantinosKokos/ape.",
        "published": "2024-10-31 04:00:00",
        "id": "a8265efa-9d06-4726-99b0-f68d8a49454e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一种用于Transformer式模型的新位置编码策略，该策略可用于多种结构，实验表明其性能与当前最佳水平相当或更优，代码已开源。"
        },
        "tokens": 746
    },
    {
        "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
        "link": "https://arxiv.org/abs/2312.10336",
        "description": "arXiv:2312.10336v2 Announce Type: replace \nAbstract: We study the problem of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the direct Hessian-based conventional Newton update. We develop a new $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a total-Hessian-based complete Newton update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the \"sensitivity\" of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples $n$ and model dimension $d$, it yields the order $\\mathcal O(n/d^{1/4})$, which shows a strict gap over the baseline method of differentially private minimax learning that has $\\mathcal O(n/d^{1/2})$. In addition, our rates of generalization and deletion capacity match the state-of-the-art rates derived previously for standard statistical learning models.",
        "published": "2024-10-31 04:00:00",
        "id": "41343dff-d8f3-467b-a526-ad01b41fb6a7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究了极小极大模型的(ε,δ) - 认证机器遗忘问题，开发了新算法，推导了三种损失函数情况的泛化率并提供了删除容量，其结果相比基线方法有优势且与标准统计学习模型的相关成果匹配。"
        },
        "tokens": 935
    },
    {
        "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
        "link": "https://arxiv.org/abs/2401.02349",
        "description": "arXiv:2401.02349v2 Announce Type: replace \nAbstract: Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
        "published": "2024-10-31 04:00:00",
        "id": "92825fe9-a1d4-4bcf-8ef6-71bd3e195381",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文《A Survey Analyzing Generalization in Deep Reinforcement Learning》将形式化并分析深度强化学习中的泛化问题，解释深度强化学习策略遇到过拟合问题的根本原因，分类并解释多种提高泛化的解决方法。"
        },
        "tokens": 810
    },
    {
        "title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG",
        "link": "https://arxiv.org/abs/2401.10225",
        "description": "arXiv:2401.10225v5 Announce Type: replace \nAbstract: In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4% improvement. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.",
        "published": "2024-10-31 04:00:00",
        "id": "d5e82446-e636-41bf-ba0b-c9032bc43b87",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍ChatQA模型在检索增强生成（RAG）和对话问答（QA）上超越GPT - 4，提出两阶段指令调整方法提升生成能力，引入优化的密集检索器，给出ChatRAG Bench并开源相关资源。"
        },
        "tokens": 947
    },
    {
        "title": "HyperSense: Hyperdimensional Intelligent Sensing for Energy-Efficient Sparse Data Processing",
        "link": "https://arxiv.org/abs/2401.10267",
        "description": "arXiv:2401.10267v4 Announce Type: replace \nAbstract: Introducing HyperSense, our co-designed hardware and software system efficiently controls Analog-to-Digital Converter (ADC) modules' data generation rate based on object presence predictions in sensor data. Addressing challenges posed by escalating sensor quantities and data rates, HyperSense reduces redundant digital data using energy-efficient low-precision ADC, diminishing machine learning system costs. Leveraging neurally-inspired HyperDimensional Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data, offering advantages in handling noise, memory-centricity, and real-time learning. Our proposed HyperSense model combines high-performance software for object detection with real-time hardware prediction, introducing the novel concept of Intelligent Sensor Control. Comprehensive software and hardware evaluations demonstrate our solution's superior performance, evidenced by the highest Area Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC) curve among lightweight models. Hardware-wise, our FPGA-based domain-specific accelerator tailored for HyperSense achieves a 5.6x speedup compared to YOLOv4 on NVIDIA Jetson Orin while showing up to 92.1% energy saving compared to the conventional system. These results underscore HyperSense's effectiveness and efficiency, positioning it as a promising solution for intelligent sensing and real-time data processing across diverse applications.",
        "published": "2024-10-31 04:00:00",
        "id": "64343299-8053-46ab-8f0d-a1b0571d0ad6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "HyperSense是一种软硬件协同设计的系统，基于超维计算处理传感器数据，减少冗余数据，其软件硬件评估表现优秀，硬件加速器性能提升且节能。"
        },
        "tokens": 860
    },
    {
        "title": "AI, insurance, discrimination and unfair differentiation. An overview and research agenda",
        "link": "https://arxiv.org/abs/2401.11892",
        "description": "arXiv:2401.11892v3 Announce Type: replace \nAbstract: Insurers underwrite risks: they calculate risks and decide on the insurance premium. Insurers seem captivated by two trends enabled by Artificial Intelligence (AI). (i) First, insurers could use AI for analysing more and new types of data to assess risks more precisely: data-intensive underwriting. (ii) Second, insurers could use AI to monitor the behaviour of individual consumers in real-time: behaviour-based insurance. For example, some car insurers offer a discount if the consumer agrees to being tracked by the insurer and drives safely. While the two trends bring many advantages, they may also have discriminatory effects on society. This paper focuses on the following question. Which effects related to discrimination and unfair differentiation may occur if insurers follow data-intensive underwriting and behaviour-based insurance? Researchers and policymakers working in other sectors may also find the paper useful, as the insurance sector has decades of experience with statistics and forms of AI. Moreover, some questions that arise in the insurance sector are important in other sectors too.",
        "published": "2024-10-31 04:00:00",
        "id": "6d56eae9-01d7-490c-9d3e-1da95ddc650d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨人工智能背景下，保险公司在数据密集型承保和基于行为的保险中可能出现的歧视和不公平差异影响。"
        },
        "tokens": 790
    },
    {
        "title": "Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy",
        "link": "https://arxiv.org/abs/2401.12129",
        "description": "arXiv:2401.12129v2 Announce Type: replace \nAbstract: As deep neural networks become adopted in high-stakes domains, it is crucial to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence -- ultimately to know when networks' decisions (and their uncertainty in those decisions) should be trusted. In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), an OOD detection method which lowers the False Positive Rate at 95\\% True Positive Rate (FPR@95) by $43.43\\%$ in classification compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to why our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively -- with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ in semantic segmentation compared to previous state of the art.",
        "published": "2024-10-31 04:00:00",
        "id": "bc5734f3-1faf-49c2-9f4c-86b98aecefae",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Ablated Learned Temperature Energy (AbeT)这种离群值检测方法在分类中降低误报率，阐述模型区分分布内和离群样本的原因，并展示其在目标检测和语义分割中识别离群对象的有效性。"
        },
        "tokens": 901
    },
    {
        "title": "Human Expertise in Algorithmic Prediction",
        "link": "https://arxiv.org/abs/2402.00793",
        "description": "arXiv:2402.00793v3 Announce Type: replace \nAbstract: We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or \"look the same\" to predictive algorithms. We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of \"side information\", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly $30\\%$ of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.",
        "published": "2024-10-31 04:00:00",
        "id": "5de25a0e-1546-471e-9446-473fff49153e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一种将人类专业知识融入算法预测的新框架，该框架利用人类判断区分算法无法区分的输入，可提高算法预测性能，经实证发现人类判断能改善特定实例的算法预测。"
        },
        "tokens": 825
    },
    {
        "title": "Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution",
        "link": "https://arxiv.org/abs/2401.15866",
        "description": "arXiv:2401.15866v2 Announce Type: replace \nAbstract: Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and are intractable for large datasets. These methods require efficient approximations, and although amortizing the process by learning a network to directly predict the desired output is a promising solution, training such models with exact labels is often infeasible. We therefore explore training amortized models with noisy labels, and we find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach tolerates high noise levels and significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "1a967628-6d08-41c7-9b4e-d09edfeabb8a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在可解释机器学习中，数据估值和特征归因等任务计算昂贵，用学习网络直接预测输出的摊销过程有训练标签不可行的问题，探索用带噪标签训练摊销模型成本低且效果好，理论分析和实验表明其能加速相关方法。"
        },
        "tokens": 763
    },
    {
        "title": "Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models",
        "link": "https://arxiv.org/abs/2401.16727",
        "description": "arXiv:2401.16727v4 Announce Type: replace \nAbstract: In the evolving landscape of online communication, moderating hate speech (HS) presents an intricate challenge, compounded by the multimodal nature of digital content. This comprehensive survey delves into the recent strides in HS moderation, spotlighting the burgeoning role of large language models (LLMs) and large multimodal models (LMMs). Our exploration begins with a thorough analysis of current literature, revealing the nuanced interplay between textual, visual, and auditory elements in propagating HS. We uncover a notable trend towards integrating these modalities, primarily due to the complexity and subtlety with which HS is disseminated. A significant emphasis is placed on the advances facilitated by LLMs and LMMs, which have begun to redefine the boundaries of detection and moderation capabilities. We identify existing gaps in research, particularly in the context of underrepresented languages and cultures, and the need for solutions to handle low-resource settings. The survey concludes with a forward-looking perspective, outlining potential avenues for future research, including the exploration of novel AI methodologies, the ethical governance of AI in moderation, and the development of more nuanced, context-aware systems. This comprehensive overview aims to catalyze further research and foster a collaborative effort towards more sophisticated, responsible, and human-centric approaches to HS moderation in the digital era. WARNING: This paper contains offensive examples.",
        "published": "2024-10-31 04:00:00",
        "id": "628f4129-996f-44ce-bca2-8424000725db",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "这篇文章对网络交流中的仇恨言论审查的近期进展进行了综述，重点关注大型语言模型和大型多模态模型的作用、多模态元素在仇恨言论传播中的相互作用、现有研究的差距以及未来研究方向。"
        },
        "tokens": 883
    },
    {
        "title": "Comparing Template-based and Template-free Language Model Probing",
        "link": "https://arxiv.org/abs/2402.00123",
        "description": "arXiv:2402.00123v2 Announce Type: replace \nAbstract: The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same answers frequently across prompts for template-based probing, which is less common when employing template-free techniques.",
        "published": "2024-10-31 04:00:00",
        "id": "1ced017e-991a-4476-bf66-fd1d87c4b740",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章评估16种不同的语言模型在10个英语探测数据集上的情况，对比基于模板和无模板的方法，回答三个研究问题并得出相关发现。"
        },
        "tokens": 835
    },
    {
        "title": "Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery",
        "link": "https://arxiv.org/abs/2401.14121",
        "description": "arXiv:2401.14121v2 Announce Type: replace \nAbstract: Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image. There is a kind of methods first training a regression model for this problem, then further optimizing the pretrained regression model for any specific sample individually at test time. However, the pretrained model may not provide an ideal optimization starting point for the test-time optimization. Inspired by meta-learning, we incorporate the test-time optimization into training, performing a step of test-time optimization for each sample in the training batch before really conducting the training optimization over all the training samples. In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization. At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model. Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model. To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives. Our method, armed with meta-learning and the dual networks, outperforms state-of-the-art regression-based and optimization-based HMR approaches, as validated by the extensive experiments. The codes are available at https://github.com/fmx789/Meta-HMR.",
        "published": "2024-10-31 04:00:00",
        "id": "d4f7a5ef-2cd1-4191-b11b-f8eb16edf2e5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受元学习启发，将测试时优化融入训练以提高人体网格恢复（HMR）精度，提出双网络架构统一训练和测试目标，新方法优于现有同类方法且代码已开源。"
        },
        "tokens": 887
    },
    {
        "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
        "link": "https://arxiv.org/abs/2402.05369",
        "description": "arXiv:2402.05369v3 Announce Type: replace \nAbstract: User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By comparing NCA and InfoNCA, we demonstrate that the well-observed decreasing-likelihood trend of DPO/InfoNCA is caused by their focus on adjusting relative likelihood across different responses. In contrast, NCA optimizes the absolute likelihood for each response, thereby effectively preventing the chosen likelihood from decreasing. We evaluate our methods in both reward and preference settings with Mistral-8*7B and 7B models. Experiments suggest that InfoNCA/NCA surpasses various preference baselines when reward datasets are available. We also find NCA significantly outperforms DPO in complex reasoning tasks like math and coding.",
        "published": "2024-10-31 04:00:00",
        "id": "9bed6003-1a60-483a-b70c-f2cec518f6bb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一个语言模型对齐的通用框架，包含NCA和InfoNCA算法，在奖励和偏好设置下评估方法，结果显示InfoNCA/NCA在有奖励数据集时优于偏好基线，NCA在数学和编码等复杂推理任务中显著优于DPO。"
        },
        "tokens": 903
    },
    {
        "title": "Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm",
        "link": "https://arxiv.org/abs/2402.02042",
        "description": "arXiv:2402.02042v3 Announce Type: replace \nAbstract: This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDPs). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual-based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, our proposed algorithm achieves $\\tilde{\\mathcal{O}}({T}^{4/5})$ objective regret and $\\tilde{\\mathcal{O}}({T}^{4/5})$ constraint violation bounds.",
        "published": "2024-10-31 04:00:00",
        "id": "dbf43544-cb13-4f47-92f6-9950b22a1bc9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文研究无限期平均奖励约束马尔可夫决策过程（CMDPs），提出基于原始对偶的策略梯度算法，该算法实现特定的目标后悔和约束违反界限。"
        },
        "tokens": 748
    },
    {
        "title": "Copycats: the many lives of a publicly available medical imaging dataset",
        "link": "https://arxiv.org/abs/2402.06353",
        "description": "arXiv:2402.06353v3 Announce Type: replace \nAbstract: Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",
        "published": "2024-10-31 04:00:00",
        "id": "7ca54d4a-e08e-45a1-bdad-acdea7062f3e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文指出社区贡献平台在管理公开的医学影像数据集时存在诸多问题，影响诊断算法，研究有助于医疗保健领域负责任的数据管理和AI算法发展。"
        },
        "tokens": 829
    },
    {
        "title": "Online Feature Updates Improve Online (Generalized) Label Shift Adaptation",
        "link": "https://arxiv.org/abs/2402.03545",
        "description": "arXiv:2402.03545v2 Announce Type: replace \nAbstract: This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. By carefully designing the algorithm, theoretically OLS-OFU maintains the similar online regret convergence to the results in the literature while taking the improved features into account. Empirically, it achieves substantial improvements over existing methods, which is as significant as the gains existing methods have over the baseline (i.e., without distribution shift adaptations).",
        "published": "2024-10-31 04:00:00",
        "id": "bef56e24-3040-4563-af05-008656fab184",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出OLS - OFU方法，通过自监督学习利用无标签数据优化特征提取过程，解决在线环境下标签转移及标签缺失问题，理论上保持在线后悔收敛，经验上效果优于现有方法。"
        },
        "tokens": 775
    },
    {
        "title": "UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models",
        "link": "https://arxiv.org/abs/2402.11846",
        "description": "arXiv:2402.11846v4 Announce Type: replace \nAbstract: The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at https://unlearn-canvas.netlify.app/.",
        "published": "2024-10-31 04:00:00",
        "id": "85434bdd-9f14-4c5c-bf3f-d241e0d85c84",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决现有机器忘却评估系统的挑战，提出用于评估扩散模型忘却艺术风格及相关对象能力的UnlearnCanvas数据集，通过实验对9种机器忘却方法进行基准测试，相关资源已公开。"
        },
        "tokens": 890
    },
    {
        "title": "Retention Induced Biases in a Recommendation System with Heterogeneous Users",
        "link": "https://arxiv.org/abs/2402.13959",
        "description": "arXiv:2402.13959v3 Announce Type: replace \nAbstract: I examine a conceptual model of a recommendation system (RS) with user inflow and churn dynamics. When inflow and churn balance out, the user distribution reaches a steady state. Changing the recommendation algorithm alters the steady state and creates a transition period. During this period, the RS behaves differently from its new steady state. In particular, A/B experiment metrics obtained in transition periods are biased indicators of the RS's long-term performance. Scholars and practitioners, however, often conduct A/B tests shortly after introducing new algorithms to validate their effectiveness. This A/B experiment paradigm, widely regarded as the gold standard for assessing RS improvements, may consequently yield false conclusions. I also briefly touch on the data bias caused by the user retention dynamics.",
        "published": "2024-10-31 04:00:00",
        "id": "e3e3354a-b68b-4a00-862c-1ef9ee43b29a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究推荐系统中用户流入和流失动态下的留存偏差，指出改变推荐算法时A/B实验指标存在偏差，还提及用户留存动态造成的数据偏差。"
        },
        "tokens": 744
    },
    {
        "title": "Learning Structure-Aware Representations of Dependent Types",
        "link": "https://arxiv.org/abs/2402.02104",
        "description": "arXiv:2402.02104v2 Announce Type: replace \nAbstract: Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, which details proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves promising initial results, surpassing strong baselines.",
        "published": "2024-10-31 04:00:00",
        "id": "d3014a57-d6a5-4ca1-8668-50cf144a6292",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍并发布新的Agda程序 - 证明数据集，提出基于结构的神经架构表示依赖类型程序，在前提选择设置中取得初步成果。"
        },
        "tokens": 750
    },
    {
        "title": "Transductive Learning Is Compact",
        "link": "https://arxiv.org/abs/2402.10360",
        "description": "arXiv:2402.10360v3 Announce Type: replace \nAbstract: We demonstrate a compactness result holding broadly across supervised learning with a general class of loss functions: Any hypothesis class $H$ is learnable with transductive sample complexity $m$ precisely when all of its finite projections are learnable with sample complexity $m$. We prove that this exact form of compactness holds for realizable and agnostic learning with respect to any proper metric loss function (e.g., any norm on $\\mathbb{R}^d$) and any continuous loss on a compact space (e.g., cross-entropy, squared loss). For realizable learning with improper metric losses, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. Furthermore, invoking the equivalence between sample complexities in the PAC and transductive models (up to lower order factors, in the realizable case) permits us to directly port our results to the PAC model, revealing an almost-exact form of compactness holding broadly in PAC learning.",
        "published": "2024-10-31 04:00:00",
        "id": "476bb03f-1e84-4a88-b992-8558936b926c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "证明了监督学习中的一个紧致性结果，即任何假设类H在转导样本复杂度m下可学习等价于其所有有限投影在样本复杂度m下可学习，还给出了一些特殊情况下的结果，并将其延伸到PAC模型。"
        },
        "tokens": 836
    },
    {
        "title": "Score-based Causal Representation Learning: Linear and General Transformations",
        "link": "https://arxiv.org/abs/2402.00849",
        "description": "arXiv:2402.00849v3 Announce Type: replace \nAbstract: This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the identifiability and achievability aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. First, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on general transformations and shows that two stochastic hard interventions per node suffice for identifiability. Notably, one does not need to know which pair of interventional environments have the same node intervened. Finally, the theoretical results are empirically validated via experiments on structured synthetic data and image data.",
        "published": "2024-10-31 04:00:00",
        "id": "32147b25-d120-4383-a085-1400ba38536f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "本文在一般非参数潜在因果模型和将潜在变量映射到观测变量的未知变换下研究干预型因果表示学习（CRL），探究线性和一般变换，从得分函数与CRL的联系设计算法确保可识别性和可实现性，并对线性和一般变换下的干预保证及相关实验进行阐述。"
        },
        "tokens": 899
    },
    {
        "title": "NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks",
        "link": "https://arxiv.org/abs/2402.15393",
        "description": "arXiv:2402.15393v3 Announce Type: replace \nAbstract: We contribute NeuralSolver, a novel recurrent solver that can efficiently and consistently extrapolate, i.e., learn algorithms from smaller problems (in terms of observation size) and execute those algorithms in large problems. Contrary to previous recurrent solvers, NeuralSolver can be naturally applied in both same-size problems, where the input and output sizes are the same, and in different-size problems, where the size of the input and output differ. To allow for this versatility, we design NeuralSolver with three main components: a recurrent module, that iteratively processes input information at different scales, a processing module, responsible for aggregating the previously processed information, and a curriculum-based training scheme, that improves the extrapolation performance of the method. To evaluate our method we introduce a set of novel different-size tasks and we show that NeuralSolver consistently outperforms the prior state-of-the-art recurrent solvers in extrapolating to larger problems, considering smaller training problems and requiring less parameters than other approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "0c103379-7110-42db-ba98-91ffce6a835c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍NeuralSolver这种新型循环求解器，它由三个主要组件构成，能有效且一致地外推，在新的不同规模任务集中评估时，相比之前的循环求解器在向更大问题外推方面表现更优。"
        },
        "tokens": 814
    },
    {
        "title": "Are Fact-Checking Tools Helpful? An Exploration of the Usability of Google Fact Check",
        "link": "https://arxiv.org/abs/2402.13244",
        "description": "arXiv:2402.13244v5 Announce Type: replace \nAbstract: Fact-checking-specific search tools such as Google Fact Check are a promising way to combat misinformation on social media, especially during events bringing significant social influence, such as the COVID-19 pandemic and the U.S. presidential elections. However, the usability of such an approach has not been thoroughly studied. We evaluated the performance of Google Fact Check by analyzing the retrieved fact-checking results regarding 1,000 COVID-19-related false claims and found it able to retrieve the fact-checking results for 15.8% of the input claims, and the rendered results are relatively reliable. We also found that the false claims receiving different fact-checking verdicts (i.e., \"False,\" \"Partly False,\" \"True,\" and \"Unratable\") tend to reflect diverse emotional tones, and fact-checking sources tend to check the claims in different lengths and using dictionary words to various extents. Claim variations addressing the same issue yet described differently are likely to retrieve distinct fact-checking results. We suggest that the quantities of the retrieved fact-checking results could be optimized and that slightly adjusting input wording may be the best practice for users to retrieve more useful information. This study aims to contribute to the understanding of state-of-the-art fact-checking tools and information integrity.",
        "published": "2024-10-31 04:00:00",
        "id": "f22a7101-dc37-49c8-9178-68f779c04dce",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过分析1000个新冠相关虚假声明的检索事实核查结果评估谷歌事实核查的性能，发现其能检索15.8%输入声明的核查结果且较可靠，还发现不同核查结果的声明有不同特点，建议优化检索数量、调整输入措辞"
        },
        "tokens": 890
    },
    {
        "title": "Linear Transformers are Versatile In-Context Learners",
        "link": "https://arxiv.org/abs/2402.14180",
        "description": "arXiv:2402.14180v2 Announce Type: replace \nAbstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.",
        "published": "2024-10-31 04:00:00",
        "id": "5a24fbdc-729c-4050-a07e-9ba19973efe3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文证明线性变换器每层维护隐式线性回归问题的权重向量并可视为预条件梯度下降变体，还研究其在含噪训练数据场景中的表现，发现其能找到包含动量和自适应缩放的优化算法。"
        },
        "tokens": 787
    },
    {
        "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive Edge Caching",
        "link": "https://arxiv.org/abs/2402.14576",
        "description": "arXiv:2402.14576v3 Announce Type: replace \nAbstract: This paper tackles the growing issue of excessive data transmission in networks. With increasing traffic, backhaul links and core networks are under significant traffic, leading to the investigation of caching solutions at edge routers. Many existing studies utilize Markov Decision Processes (MDP) to tackle caching problems, often assuming decision points at fixed intervals; however, real-world environments are characterized by random request arrivals. Additionally, critical file attributes such as lifetime, size, and priority significantly impact the effectiveness of caching policies, yet existing research fails to integrate all these attributes in policy design. In this work, we model the caching problem using a Semi-Markov Decision Process (SMDP) to better capture the continuous-time nature of real-world applications, enabling caching decisions to be triggered by random file requests. We then introduce a Proximal Policy Optimization (PPO)--based caching strategy that fully considers file attributes like lifetime, size, and priority. Simulations show that our method outperforms a recent Deep Reinforcement Learning-based technique. To further advance our research, we improved the convergence rate of PPO by prioritizing transitions within the replay buffer through an attention mechanism. This mechanism evaluates the similarity between the current state and all stored transitions, assigning higher priorities to transitions that exhibit greater similarity.",
        "published": "2024-10-31 04:00:00",
        "id": "4042e78e-e4fc-4e69-a6e4-9855b739da9c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对网络中数据传输过多问题，用半马尔可夫决策过程建模缓存问题，提出基于近端策略优化的缓存策略并考虑文件属性，通过注意力机制改进近端策略优化收敛速度，模拟结果显示该方法优于现有技术。"
        },
        "tokens": 868
    },
    {
        "title": "How to split a tera-polynomial",
        "link": "https://arxiv.org/abs/2402.06083",
        "description": "arXiv:2402.06083v2 Announce Type: replace \nAbstract: This article presents a new algorithm to compute all the roots of two families of polynomials that are of interest for the Mandelbrot set $\\mathcal{M}$ : the roots of those polynomials are respectively the parameters $c\\in\\mathcal{M}$ associated with periodic critical dynamics for $f_c(z)=z^2+c$ (hyperbolic centers) or with pre-periodic dynamics (Misiurewicz-Thurston parameters). The algorithm is based on the computation of discrete level lines that provide excellent starting points for the Newton method. In practice, we observe that these polynomials can be split in linear time of the degree. This article is paired with a code library [Mandel] that implements this algorithm. Using this library and about 723 000 core-hours on the HPC center Rom\\'eo (Reims), we have successfully found all hyperbolic centers of period $\\leq 41$ and all Misiurewicz-Thurston parameters whose period and pre-period sum to $\\leq 35$. Concretely, this task involves splitting a tera-polynomial, i.e. a polynomial of degree $\\sim10^{12}$, which is orders of magnitude ahead of the previous state of the art. It also involves dealing with the certifiability of our numerical results, which is an issue that we address in detail, both mathematically and along the production chain. The certified database is available to the scientific community. For the smaller periods that can be represented using only hardware arithmetic (floating points FP80), the implementation of our algorithm can split the corresponding polynomials of degree $\\sim10^{9}$ in less than one day-core. We complement these benchmarks with a statistical analysis of the separation of the roots, which confirms that no other polynomial in these families can be split without using higher precision arithmetic.",
        "published": "2024-10-31 04:00:00",
        "id": "21e12079-3294-40d5-9216-0dae3e52c37b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "文章提出一种新算法计算两类多项式的所有根，在实践中多项式可按线性时间拆分，通过在高性能计算中心耗时运算，成功找到特定周期内的参数，涉及拆分万亿级多项式，还涉及数值结果的可验证性并提供了认证数据库，同时进行了根分离的统计分析。"
        },
        "tokens": 1016
    },
    {
        "title": "LLMs for Targeted Sentiment in News Headlines: Exploring the Descriptive-Prescriptive Dilemma",
        "link": "https://arxiv.org/abs/2403.00418",
        "description": "arXiv:2403.00418v3 Announce Type: replace \nAbstract: News headlines often evoke sentiment by intentionally portraying entities in particular ways, making targeted sentiment analysis (TSA) of headlines a worthwhile but difficult task. Due to its subjectivity, creating TSA datasets can involve various annotation paradigms, from descriptive to prescriptive, either encouraging or limiting subjectivity. LLMs are a good fit for TSA due to their broad linguistic and world knowledge and in-context learning abilities, yet their performance depends on prompt design. In this paper, we compare the accuracy of state-of-the-art LLMs and fine-tuned encoder models for TSA of news headlines using descriptive and prescriptive datasets across several languages. Exploring the descriptive--prescriptive continuum, we analyze how performance is affected by prompt prescriptiveness, ranging from plain zero-shot to elaborate few-shot prompts. Finally, we evaluate the ability of LLMs to quantify uncertainty via calibration error and comparison to human label variation. We find that LLMs outperform fine-tuned encoders on descriptive datasets, while calibration and F1-score generally improve with increased prescriptiveness, yet the optimal level varies.",
        "published": "2024-10-31 04:00:00",
        "id": "11836067-5a4f-4089-80fb-c33c78b621a1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究比较了先进大型语言模型（LLMs）和微调编码器模型在多种语言新闻标题的目标情感分析（TSA）中的准确性，分析提示规范性对性能的影响，还评估了LLMs量化不确定性的能力。"
        },
        "tokens": 830
    },
    {
        "title": "Anchor-free Clustering based on Anchor Graph Factorization",
        "link": "https://arxiv.org/abs/2402.15688",
        "description": "arXiv:2402.15688v2 Announce Type: replace \nAbstract: Anchor-based methods are a pivotal approach in handling clustering of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor graph. This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free Clustering based on Anchor Graph Factorization (AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means clustering algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the membership matrix between cluster centers and samples in FKM into an anchor graph encompassing multiple anchor points and samples. Employing Non-negative Matrix Factorization (NMF) on this anchor graph allows for the direct derivation of cluster labels, thereby eliminating the requirement for further post-processing steps. To solve the method proposed, we implement an alternating optimization algorithm that ensures convergence. Empirical evaluations on various real-world datasets underscore the superior efficacy of our algorithm compared to traditional approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "4af5047e-4a3f-415c-b9c0-cc61bc1ca913",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于锚图分解的无锚聚类方法AFCAGF，改进模糊k - 均值聚类算法，采用交替优化算法求解，经实证评估其在真实数据集上比传统方法更有效。"
        },
        "tokens": 861
    },
    {
        "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue",
        "link": "https://arxiv.org/abs/2402.17262",
        "description": "arXiv:2402.17262v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to \"jailbreak.\" Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide range of LLMs, indicate current inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our findings expose vulnerabilities of LLMs in complex scenarios involving multi-turn dialogue, presenting new challenges for the safety of LLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "b4790e27-d801-4962-9c29-91aafe036eb1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现人类可利用多轮对话诱导大型语言模型生成有害信息，实验表明当前大型语言模型在多轮对话中的安全机制存在不足，暴露出其在多轮对话场景中的漏洞。"
        },
        "tokens": 831
    },
    {
        "title": "Derivative-enhanced Deep Operator Network",
        "link": "https://arxiv.org/abs/2402.19242",
        "description": "arXiv:2402.19242v2 Announce Type: replace \nAbstract: The deep operator networks (DeepONet), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages derivative information to enhance the solution prediction accuracy and provides a more accurate approximation of solution-to-parameter derivatives, especially when training data are limited. DE-DeepONet explicitly incorporates linear dimension reduction of high dimensional parameter input into DeepONet to reduce training cost and adds derivative loss in the loss function to reduce the number of required parameter-solution pairs. We further demonstrate that the use of derivative loss can be extended to enhance other neural operators, such as the Fourier neural operator (FNO). Numerical experiments validate the effectiveness of our approach.",
        "published": "2024-10-31 04:00:00",
        "id": "f27a9045-1535-4f91-8920-5f0671821273",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出导数增强型深度算子网络（DE - DeepONet）以提高解预测精度并更准确地逼近解 - 参数导数，可减少训练成本并减少所需的参数 - 解对数量，还可扩展到增强其他神经算子，数值实验验证了该方法的有效性。"
        },
        "tokens": 789
    },
    {
        "title": "CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models",
        "link": "https://arxiv.org/abs/2403.02745",
        "description": "arXiv:2403.02745v2 Announce Type: replace \nAbstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), focusing on incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs' resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an $\\epsilon$-optimal ranking with high probability while allowing as large as $O(n)$ perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.",
        "published": "2024-10-31 04:00:00",
        "id": "3ba53268-abae-4646-8636-721013e1bbac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新方法稳健且完整地重新校准偏好数据集中的值以增强大型语言模型对不完整和损坏数据的应对能力，包括设计保证多项式时间的排序算法并展示相关恢复结果，通过实验确认算法在不同场景下能处理对抗性噪声和未观测比较等情况。"
        },
        "tokens": 864
    },
    {
        "title": "CIDGMed: Causal Inference-Driven Medication Recommendation with Enhanced Dual-Granularity Learning",
        "link": "https://arxiv.org/abs/2403.00880",
        "description": "arXiv:2403.00880v2 Announce Type: replace \nAbstract: Medication recommendation aims to integrate patients' long-term health records to provide accurate and safe medication combinations for specific health states. Existing methods often fail to deeply explore the true causal relationships between diseases/procedures and medications, resulting in biased recommendations. Additionally, in medication representation learning, the relationships between information at different granularities of medications, coarse-grained (medication itself) and fine-grained (molecular level), are not effectively integrated, leading to biases in representation learning. To address these limitations, we propose the Causal Inference-driven Dual-Granularity Medication Recommendation method (CIDGMed). Our approach leverages causal inference to uncover the relationships between diseases/procedures and medications, thereby enhancing the rationality and interpretability of recommendations. By integrating coarse-grained medication effects with fine-grained molecular structure information, CIDGMed provides a comprehensive representation of medications. Additionally, we employ a bias correction model during the prediction phase to further refine recommendations, ensuring both accuracy and safety. Through extensive experiments, CIDGMed significantly outperforms current state-of-the-art models across multiple metrics, achieving a 2.54% increase in accuracy, a 3.65% reduction in side effects, and a 39.42% improvement in time efficiency. Additionally, we demonstrate the rationale of CIDGMed through a case study.",
        "published": "2024-10-31 04:00:00",
        "id": "420a12dd-3a67-4947-bfa2-3e5cb43aba41",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出CIDGMed方法解决现有药物推荐的局限，该方法利用因果推理揭示疾病/医疗程序和药物关系、整合粗细粒度药物信息，还有偏差校正模型，实验表现优于现有模型。"
        },
        "tokens": 882
    },
    {
        "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
        "link": "https://arxiv.org/abs/2403.01446",
        "description": "arXiv:2403.01446v2 Announce Type: replace \nAbstract: Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios. Our framework is available at https://github.com/cure-lab/GuardT2I.",
        "published": "2024-10-31 04:00:00",
        "id": "e37101a0-30d1-4dc6-87f3-cb72fe5ab620",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出一种名为GuardT2I的新型审核框架，采用生成式方法增强文本到图像（T2I）模型对对抗性提示的鲁棒性，实验表明其在多种对抗场景下优于其他商业解决方案，框架代码已开源。"
        },
        "tokens": 820
    },
    {
        "title": "Differentially Private Representation Learning via Image Captioning",
        "link": "https://arxiv.org/abs/2403.02506",
        "description": "arXiv:2403.02506v2 Announce Type: replace \nAbstract: Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For example, under a privacy budget of $\\varepsilon=8$ for the LAION dataset, a linear classifier trained on top of learned DP-Cap features attains $65.8\\%$ accuracy on ImageNet-1K, considerably improving the previous SOTA of $56.5\\%$.",
        "published": "2024-10-31 04:00:00",
        "id": "1af069d4-7d91-4743-9a14-c9169eaaa4c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出通过图像字幕进行差分隐私表示学习，通过工程技巧在LAION - 2B的2.33亿子集上训练出DP图像字幕器（DP - Cap），其产生的图像特征可用于下游任务，在隐私预算下提升了在ImageNet - 1K上的准确率。"
        },
        "tokens": 868
    },
    {
        "title": "Minions: Accelerating Large Language Model Inference with Aggregated Speculative Execution",
        "link": "https://arxiv.org/abs/2402.15678",
        "description": "arXiv:2402.15678v2 Announce Type: replace \nAbstract: Large language models (LLM) have recently attracted surging interest due to their outstanding capabilities across various domains. However, enabling efficient LLM inference is challenging due to its autoregressive decoding that generates tokens only one at a time. Although research works apply pruning or quantization to speed up LLM inference, they typically require fine-tuning the LLM, incurring significant time and economic costs. Meanwhile, speculative decoding has been proposed to use small speculative models (SSMs) to accelerate the inference of LLM. However, the low acceptance rate of SSM and the high verification cost of LLM prohibit further performance improvement of inference. In this paper, we propose Minions, an LLM inference system that accelerates LLM inference with a collective and adaptive speculative generation. Specifically, Minions proposes a majority-voted mechanism to leverage multiple SSMs to jointly speculate the outputs of LLM, which improves the inference performance without introducing prohibitive computation costs for LLM. To better trade off the number of tokens speculated from SSM and the verification cost of LLM, Minions proposes an adaptive mechanism to dynamically determine the optimal speculation length of SSM, which can achieve better inference performance across different models, datasets, and hyper-parameters. In addition, Minions decouples the SSM decoding and LLM verification efficiently and adopts a pipelined execution mechanism to further improve the inference performance of LLM. By comparing with the state-of-the-art LLM inference systems, we demonstrate that Minions can achieve higher inference throughput and lower inference time.",
        "published": "2024-10-31 04:00:00",
        "id": "adcf216e-ab70-4940-8cf3-642a380943d5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文《Minions: Accelerating Large Language Model Inference with Aggregated Speculative Execution》提出Minions系统，通过集体和自适应推测生成加速大型语言模型推理，包括采用多数投票机制、自适应机制、解耦和流水线执行机制，性能优于现有系统。"
        },
        "tokens": 931
    },
    {
        "title": "Accelerating the convergence of Newton's method for nonlinear elliptic PDEs using Fourier neural operators",
        "link": "https://arxiv.org/abs/2403.03021",
        "description": "arXiv:2403.03021v2 Announce Type: replace \nAbstract: It is well known that Newton's method can have trouble converging if the initial guess is too far from the solution. Such a problem particularly occurs when this method is used to solve nonlinear elliptic partial differential equations (PDEs) discretized via finite differences. This work focuses on accelerating Newton's method convergence in this context. We seek to construct a mapping from the parameters of the nonlinear PDE to an approximation of its discrete solution, independently of the mesh resolution. This approximation is then used as an initial guess for Newton's method. To achieve these objectives, we elect to use a Fourier neural operator (FNO). The loss function is the sum of a data term (i.e., the comparison between known solutions and outputs of the FNO) and a physical term (i.e., the residual of the PDE discretization). Numerical results, in one and two dimensions, show that the proposed initial guess accelerates the convergence of Newton's method by a large margin compared to a naive initial guess, especially for highly nonlinear and anisotropic problems, with larger gains on coarse grids.",
        "published": "2024-10-31 04:00:00",
        "id": "5ec07304-412b-4de2-a347-c25da2703a2e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出用傅里叶神经算子（FNO）构建非线性偏微分方程参数到离散解近似值的映射，作为牛顿法的初始猜测以加速其收敛，数值结果显示该方法有效。"
        },
        "tokens": 826
    },
    {
        "title": "MamMIL: Multiple Instance Learning for Whole Slide Images with State Space Models",
        "link": "https://arxiv.org/abs/2403.05160",
        "description": "arXiv:2403.05160v3 Announce Type: replace \nAbstract: Recently, pathological diagnosis has achieved superior performance by combining deep learning models with the multiple instance learning (MIL) framework using whole slide images (WSIs). However, the giga-pixeled nature of WSIs poses a great challenge for efficient MIL. Existing studies either do not consider global dependencies among instances, or use approximations such as linear attentions to model the pair-to-pair instance interactions, which inevitably brings performance bottlenecks. To tackle this challenge, we propose a framework named MamMIL for WSI analysis by cooperating the selective structured state space model (i.e., Mamba) with MIL, enabling the modeling of global instance dependencies while maintaining linear complexity. Specifically, considering the irregularity of the tissue regions in WSIs, we represent each WSI as an undirected graph. To address the problem that Mamba can only process 1D sequences, we further propose a topology-aware scanning mechanism to serialize the WSI graphs while preserving the topological relationships among the instances. Finally, in order to further perceive the topological structures among the instances and incorporate short-range feature interactions, we propose an instance aggregation block based on graph neural networks. Experiments show that MamMIL can achieve advanced performance than the state-of-the-art frameworks. The code can be accessed at https://github.com/Vison307/MamMIL.",
        "published": "2024-10-31 04:00:00",
        "id": "5e2b69cf-3bb2-4d50-9835-6547362df5f5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出用于全幻灯片图像分析的MamMIL框架，通过将选择性结构状态空间模型与多示例学习相结合，在保持线性复杂度的同时实现全局实例依赖建模，实验显示其性能优于现有框架，代码可在指定网址获取。"
        },
        "tokens": 887
    },
    {
        "title": "Towards Full Automation of Geometry Extraction for Biomechanical Analysis of Abdominal Aortic Aneurysm; Neural Network-Based versus Classical Methodologies",
        "link": "https://arxiv.org/abs/2403.07238",
        "description": "arXiv:2403.07238v2 Announce Type: replace \nAbstract: In this study, we investigated the impact of image segmentation methods on the results of stress computation in the wall of abdominal aortic aneurysms (AAAs). We compared wall stress distributions and magnitudes calculated from geometry models obtained from classical semi-automated segmentation versus automated neural network-based segmentation. 16 different AAA contrast-enhanced computed tomography (CT) images were semi-automatically segmented by an analyst, taking between 15 and 40 minutes of human effort per patient, depending on image quality. The same images were automatically segmented using PRAEVAorta commercial software by NUREA (https://www.nurea-soft.com/), developed based on artificial intelligence (AI) algorithms, and automatically post-processed with an in-house MATLAB code, requiring only 1-2 minutes of computer time per patient. Aneurysm wall stress calculations, automatically performed using the BioPARR software (https://bioparr.mech.uwa.edu.au/), revealed that, compared to the classical semi-automated segmentation, the automatic neural network-based segmentation leads to equivalent stress distributions, and slightly higher peak and 99th percentile maximum principal stress values. This difference is due to consistently larger lumen surface areas in automatically segmented models as compared to classical semi-automated segmentations, resulting in greater total pressure load on the wall. However, our statistical analysis indicated that the differences in AAA wall stress obtained using the two segmentation methods are not statistically significant and fall within the typical range of inter-analyst and intra-analyst variability, justifying the use of AI-based automatic segmentation in a fully automated AAA stress computation pipeline.",
        "published": "2024-10-31 04:00:00",
        "id": "52a630c3-f206-46b5-9324-759a9b9718fa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "研究比较了经典半自动分割与基于神经网络的自动分割在腹主动脉瘤壁应力计算中的影响，发现基于神经网络的自动分割虽使应力值略高，但差异无统计学意义，证明其可用于自动化应力计算流程。"
        },
        "tokens": 957
    },
    {
        "title": "Solving Partial Differential Equations Using Artificial Neural Networks",
        "link": "https://arxiv.org/abs/2403.09001",
        "description": "arXiv:2403.09001v2 Announce Type: replace \nAbstract: Partial differential equations have a wide range of applications in modeling multiple physical, biological, or social phenomena. Therefore, we need to approximate the solutions of these equations in computationally feasible terms. Nowadays, among the most popular numerical methods for solving partial differential equations in engineering, we encounter the finite difference and finite element methods. An alternative numerical method that has recently gained popularity for numerically solving partial differential equations is the use of artificial neural networks.\n  Artificial neural networks, or neural networks for short, are mathematical structures with universal approximation properties. In addition, thanks to the extraordinary computational development of the last decade, neural networks have become accessible and powerful numerical methods for engineers and researchers. For example, imaging and language processing are applications of neural networks today that show sublime performance inconceivable years ago.\n  This dissertation contributes to the numerical solution of partial differential equations using neural networks with the following two-fold objective: investigate the behavior of neural networks as approximators of solutions of partial differential equations and propose neural-network-based methods for frameworks that are hardly addressable via traditional numerical methods.\n  As novel neural-network-based proposals, we first present a method inspired by the finite element method when applying mesh refinements to solve parametric problems. Secondly, we propose a general residual minimization scheme based on a generalized version of the Ritz method. Finally, we develop a memory-based strategy to overcome a usual numerical integration limitation when using neural networks to solve partial differential equations.",
        "published": "2024-10-31 04:00:00",
        "id": "7ac056a0-f581-4e99-9b00-4c8decfa348b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨使用人工神经网络数值求解偏微分方程，包括研究神经网络作为偏微分方程解的逼近器的行为，并提出传统数值方法难以解决的框架下基于神经网络的方法。"
        },
        "tokens": 883
    },
    {
        "title": "SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors",
        "link": "https://arxiv.org/abs/2403.13438",
        "description": "arXiv:2403.13438v5 Announce Type: replace \nAbstract: Current state-of-the-art spatial reasoning-enhanced VLMs are trained to excel at spatial visual question answering (VQA). However, we believe that higher-level 3D-aware tasks, such as articulating dynamic scene changes and motion planning, require a fundamental and explicit 3D understanding beyond current spatial VQA datasets. In this work, we present SpatialPIN, a framework designed to enhance the spatial reasoning capabilities of VLMs through prompting and interacting with priors from multiple 3D foundation models in a zero-shot, training-free manner. Extensive experiments demonstrate that our spatial reasoning-imbued VLM performs well on various forms of spatial VQA and can extend to help in various downstream robotics tasks such as pick and stack and trajectory planning.",
        "published": "2024-10-31 04:00:00",
        "id": "c8e73549-b925-4023-a356-6709039e115f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "SpatialPIN框架以零次、无需训练的方式通过提示和与多个3D基础模型的先验交互来增强视觉 - 语言模型（VLM）的空间推理能力，实验表明其在空间视觉问答（VQA）表现良好且有助于下游机器人任务。"
        },
        "tokens": 787
    },
    {
        "title": "Enhancing Neural Network Representations with Prior Knowledge-Based Normalization",
        "link": "https://arxiv.org/abs/2403.16798",
        "description": "arXiv:2403.16798v3 Announce Type: replace \nAbstract: Deep learning models face persistent challenges in training, particularly due to internal covariate shift and label shift. While single-mode normalization methods like Batch Normalization partially address these issues, they are constrained by batch size dependencies and limiting distributional assumptions. Multi-mode normalization techniques mitigate these limitations but struggle with computational demands when handling diverse Gaussian distributions. In this paper, we introduce a new approach to multi-mode normalization that leverages prior knowledge to improve neural network representations. Our method organizes data into predefined structures, or \"contexts\", prior to training and normalizes based on these contexts, with two variants: Context Normalization (CN) and Context Normalization - Extended (CN-X). When contexts are unavailable, we introduce Adaptive Context Normalization (ACN), which dynamically builds contexts in the latent space during training. Across tasks in image classification, domain adaptation, and image generation, our methods demonstrate superior convergence and performance.",
        "published": "2024-10-31 04:00:00",
        "id": "17fb0875-1f64-4325-ac8a-f08c124ca1cb",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新的多模式归一化方法（包括Context Normalization及其扩展、Adaptive Context Normalization）利用先验知识改进神经网络表示，在图像分类、域适应和图像生成任务中表现出更好的收敛性和性能。"
        },
        "tokens": 791
    },
    {
        "title": "Is Your LiDAR Placement Optimized for 3D Scene Understanding?",
        "link": "https://arxiv.org/abs/2403.17009",
        "description": "arXiv:2403.17009v2 Announce Type: replace \nAbstract: The reliability of driving perception systems under unprecedented conditions is crucial for practical usage. Latest advancements have prompted increasing interest in multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce the Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional results in both LiDAR semantic segmentation and 3D object detection tasks, under diverse weather and sensor failure conditions.",
        "published": "2024-10-31 04:00:00",
        "id": "b1a1a3b0-2401-4bbe-9e87-f3561fcd256b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Place3D全周期管道，包含LiDAR放置优化、数据生成和下游评估，引入语义占用网格替代度量评估LiDAR放置质量、提出优化策略、收集含正常与恶劣条件的28万帧数据集，优化后的LiDAR放置在多种任务和条件下表现优于基线。"
        },
        "tokens": 885
    },
    {
        "title": "Evolution-based Feature Selection for Predicting Dissolved Oxygen Concentrations in Lakes",
        "link": "https://arxiv.org/abs/2403.18923",
        "description": "arXiv:2403.18923v2 Announce Type: replace \nAbstract: Accurate prediction of dissolved oxygen (DO) concentrations in lakes requires a comprehensive study of phenological patterns across ecosystems, highlighting the need for precise selection of interactions amongst external factors and internal physical-chemical-biological variables. This paper presents the Multi-population Cognitive Evolutionary Search (MCES), a novel evolutionary algorithm for complex feature interaction selection problems. MCES allows models within every population to evolve adaptively, selecting relevant feature interactions for different lake types and tasks. Evaluated on diverse lakes in the Midwestern USA, MCES not only consistently produces accurate predictions with few observed labels but also, through gene maps of models, reveals sophisticated phenological patterns of different lake types, embodying the innovative concept of \"AI from nature, for nature\".",
        "published": "2024-10-31 04:00:00",
        "id": "a1a6d436-67e1-40f3-a217-3ab7bf05f4e6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的进化算法MCES用于复杂特征交互选择以预测湖泊溶解氧浓度，在美国中西部湖泊评估中，该算法在少量观测标签下能准确预测并揭示湖泊类型的物候模式。"
        },
        "tokens": 758
    },
    {
        "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
        "link": "https://arxiv.org/abs/2404.00282",
        "description": "arXiv:2404.00282v3 Announce Type: replace \nAbstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, a comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems.",
        "published": "2024-10-31 04:00:00",
        "id": "f51b6ffe-0c4a-4b38-aa3c-e01d9d8bf137",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文综述了大型语言模型增强强化学习（LLM - enhanced RL）的现有文献，提出结构化分类法，总结方法、分析挑战并展望未来方向。"
        },
        "tokens": 828
    },
    {
        "title": "Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization",
        "link": "https://arxiv.org/abs/2404.00883",
        "description": "arXiv:2404.00883v2 Announce Type: replace \nAbstract: The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to consider inter-view information comprehensively. The decomposed tensors, namely the sample indicator tensor and the anchor indicator tensor, enhance the interpretability of the factorization. Extensive experiments validate the effectiveness of this method.",
        "published": "2024-10-31 04:00:00",
        "id": "b73e4439-5f2f-4087-904a-94cd6bc0983e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "针对基于锚图的聚类方法存在的问题，提出用非负张量分解锚图张量的多视图聚类方法以提高可解释性，实验验证了有效性。"
        },
        "tokens": 797
    },
    {
        "title": "A novel seamless magnetic-based actuating mechanism for end-effector-based robotic rehabilitation platforms",
        "link": "https://arxiv.org/abs/2404.01441",
        "description": "arXiv:2404.01441v2 Announce Type: replace \nAbstract: Rehabilitation robotics continues to confront substantial challenges, particularly in achieving smooth, safe, and intuitive human-robot interactions for upper limb motor training. Many current systems depend on complex mechanical designs, direct physical contact, and multiple sensors, which not only elevate costs but also reduce accessibility. Additionally, delivering seamless weight compensation and precise motion tracking remains a highly complex undertaking. To overcome these obstacles, we have developed a novel magnetic-based actuation mechanism for end-effector robotic rehabilitation. This innovative approach enables smooth, non-contact force transmission, significantly enhancing patient safety and comfort during upper limb training. To ensure consistent performance, we integrated an Extended Kalman Filter (EKF) alongside a controller for real-time position tracking, allowing the system to maintain high accuracy or recover even in the event of sensor malfunction or failure. In a user study with 12 participants, 75% rated the system highly for its smoothness, while 66.7% commended its safety and effective weight compensation. The EKF demonstrated precise tracking performance, with root mean square error (RMSE) values remaining within acceptable limits (under 2 cm). By combining magnetic actuation with advanced closed-loop control algorithms, this system marks a significant advancement in the field of upper limb rehabilitation robotics.",
        "published": "2024-10-31 04:00:00",
        "id": "81908564-8662-4107-8e49-141c51db11db",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决上肢运动训练中康复机器人面临的挑战，开发出一种新型基于磁的致动机制的末端执行器机器人康复系统，集成扩展卡尔曼滤波器实现实时位置跟踪，用户研究中多数参与者认可该系统，此系统在康复机器人领域是重要进展。"
        },
        "tokens": 879
    },
    {
        "title": "OTTER: Effortless Label Distribution Adaptation of Zero-shot Models",
        "link": "https://arxiv.org/abs/2404.08461",
        "description": "arXiv:2404.08461v2 Announce Type: replace \nAbstract: Popular zero-shot models suffer due to artifacts inherited from pretraining. One particularly detrimental issue, caused by unbalanced web-scale pretraining data, is mismatched label distribution. Existing approaches that seek to repair the label distribution are not suitable in zero-shot settings, as they have mismatching requirements, such as needing access to labeled downstream task data or knowledge of the true label balance in the pretraining distribution. We sidestep these challenges and introduce a simple and lightweight approach to adjust pretrained model predictions via optimal transport. Our technique requires only an estimate of the label distribution of a downstream task. Theoretically, we characterize the improvement produced by our procedure under certain mild conditions and provide bounds on the error caused by misspecification. Empirically, we validate our method in a wide array of zero-shot image and text classification tasks, improving accuracy by 4.8% and 15.9% on average, and beating baselines like prior matching -- often by significant margins -- in 17 out of 21 datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "eac884c8-d011-4661-a32e-58e52d5637e3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一种通过最优传输调整预训练模型预测的方法OTTER，可解决零样本模型中因预训练数据不平衡导致的标签分布不匹配问题，经理论分析和多任务实证验证有效。"
        },
        "tokens": 817
    },
    {
        "title": "FusionPortableV2: A Unified Multi-Sensor Dataset for Generalized SLAM Across Diverse Platforms and Scalable Environments",
        "link": "https://arxiv.org/abs/2404.08563",
        "description": "arXiv:2404.08563v2 Announce Type: replace \nAbstract: Simultaneous Localization and Mapping (SLAM) technology has been widely applied in various robotic scenarios, from rescue operations to autonomous driving. However, the generalization of SLAM algorithms remains a significant challenge, as current datasets often lack scalability in terms of platforms and environments. To address this limitation, we present FusionPortableV2, a multi-sensor SLAM dataset featuring sensor diversity, varied motion patterns, and a wide range of environmental scenarios. Our dataset comprises $27$ sequences, spanning over $2.5$ hours and collected from four distinct platforms: a handheld suite, a legged robots, a unmanned ground vehicle (UGV), and a vehicle. These sequences cover diverse settings, including buildings, campuses, and urban areas, with a total length of $38.7km$. Additionally, the dataset includes ground-truth (GT) trajectories and RGB point cloud maps covering approximately $0.3km^2$. To validate the utility of our dataset in advancing SLAM research, we assess several state-of-the-art (SOTA) SLAM algorithms. Furthermore, we demonstrate the dataset's broad application beyond traditional SLAM tasks by investigating its potential for monocular depth estimation. The complete dataset, including sensor data, GT, and calibration details, is accessible at https://fusionportable.github.io/dataset/fusionportable_v2.",
        "published": "2024-10-31 04:00:00",
        "id": "e9a96a0d-a32b-44c9-bd5e-61482ca761e8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了FusionPortableV2多传感器SLAM数据集，包含27个序列，涵盖多种平台和环境场景，还通过评估算法验证其对SLAM研究的作用及展示在单目深度估计方面的潜力，数据集可在指定网址获取。"
        },
        "tokens": 910
    },
    {
        "title": "A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics",
        "link": "https://arxiv.org/abs/2404.05981",
        "description": "arXiv:2404.05981v2 Announce Type: replace \nAbstract: Although accuracy and computation benchmarks are widely available to help choose among neural network models, these are usually trained on datasets with many classes, and do not give a good idea of performance for few (< 10) classes. The conventional procedure to predict performance involves repeated training and testing on the different models and dataset variations. We propose an efficient cosine similarity-based classification difficulty measure S that is calculated from the number of classes and intra- and inter-class similarity metrics of the dataset. After a single stage of training and testing per model family, relative performance for different datasets and models of the same family can be predicted by comparing difficulty measures - without further training and testing. Our proposed method is verified by extensive experiments on 8 CNN and ViT models and 7 datasets. Results show that S is highly correlated to model accuracy with correlation coefficient |r| = 0.796, outperforming the baseline Euclidean distance at |r| = 0.66. We show how a practitioner can use this measure to help select an efficient model 6 to 29x faster than through repeated training and testing. We also describe using the measure for an industrial application in which options are identified to select a model 42% smaller than the baseline YOLOv5-nano model, and if class merging from 3 to 2 classes meets requirements, 85% smaller.",
        "published": "2024-10-31 04:00:00",
        "id": "319b27d8-f159-4909-872d-dff910322447",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种基于余弦相似度的分类难度衡量方法S，通过对多个模型和数据集的实验验证其与模型准确性高度相关，可用于帮助选择高效模型。"
        },
        "tokens": 878
    },
    {
        "title": "Late Breaking Results: Fast System Technology Co-Optimization Framework for Emerging Technology Based on Graph Neural Networks",
        "link": "https://arxiv.org/abs/2404.06939",
        "description": "arXiv:2404.06939v4 Announce Type: replace \nAbstract: This paper proposes a fast system technology co-optimization (STCO) framework that optimizes power, performance, and area (PPA) for next-generation IC design, addressing the challenges and opportunities presented by novel materials and device architectures. We focus on accelerating the technology level of STCO using AI techniques, by employing graph neural network (GNN)-based approaches for both TCAD simulation and cell library characterization, which are interconnected through a unified compact model, collectively achieving over a 100X speedup over traditional methods. These advancements enable comprehensive STCO iterations with runtime speedups ranging from 1.9X to 14.1X and supports both emerging and traditional technologies.",
        "published": "2024-10-31 04:00:00",
        "id": "9e78cc1c-ce2a-48a5-a369-6863af7e45d8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文提出一种快速系统技术协同优化框架，利用基于图神经网络的方法优化下一代集成电路设计的功率、性能和面积，相比传统方法有速度提升，可支持新兴和传统技术。"
        },
        "tokens": 751
    },
    {
        "title": "Spectral Graph Pruning Against Over-Squashing and Over-Smoothing",
        "link": "https://arxiv.org/abs/2404.04612",
        "description": "arXiv:2404.04612v2 Announce Type: replace \nAbstract: Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a more effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on large heterophilic datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "b0df36cb-322d-48bb-b853-946631f18793",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Message Passing Graph Neural Networks存在过压缩和过平滑问题，受Braess现象启发，提出通过删除边来解决这两个问题的更有效频谱间隙优化框架，并在大型异质数据集上展示其有效性。"
        },
        "tokens": 783
    },
    {
        "title": "Computation and Critical Transitions of Rate-Distortion-Perception Functions With Wasserstein Barycenter",
        "link": "https://arxiv.org/abs/2404.04681",
        "description": "arXiv:2404.04681v3 Announce Type: replace \nAbstract: The information rate-distortion-perception (RDP) function characterizes the three-way trade-off between description rate, average distortion, and perceptual quality measured by discrepancy between probability distributions and has been applied to emerging areas in communications empowered by generative modeling. We study several variants of the RDP functions through the lens of optimal transport to characterize their critical transitions. By transforming the information RDP function into a Wasserstein Barycenter problem, we identify the critical transitions when one of the constraints becomes inactive. Further, the non-strictly convexity brought by the perceptual constraint can be regularized by an entropy regularization term. We prove that the entropy regularized model converges to the original problem and propose an alternating iteration method based on the Sinkhorn algorithm to numerically solve the regularized optimization problem. In many practical scenarios, the computation of the Distortion-Rate-Perception (DRP) function offers a solution to minimize distortion and perceptual discrepancy under rate constraints. However, the interchange of the rate objective and the distortion constraint significantly amplifies the complexity. The proposed method effectively addresses this complexity, providing an efficient solution for DRP functions. Using our numerical method, we propose a reverse data hiding scheme that imperceptibly embeds a secret message into an image, ensuring perceptual fidelity and achieving a significant improvement in the perceptual quality of the stego image compared to traditional methods under the same embedding rate. Our theoretical results and numerical method lay an attractive foundation for steganographic communications with perceptual quality constraints.",
        "published": "2024-10-31 04:00:00",
        "id": "e11461cc-2330-4e3e-a966-c9603fee3d78",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过最优传输研究信息率 - 失真 - 感知函数的变体以表征其临界转换，提出一种基于Sinkhorn算法的交替迭代方法解决相关优化问题，还提出一种反向数据隐藏方案"
        },
        "tokens": 909
    },
    {
        "title": "Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers",
        "link": "https://arxiv.org/abs/2404.09326",
        "description": "arXiv:2404.09326v3 Announce Type: replace \nAbstract: Few-shot knowledge distillation recently emerged as a viable approach to harness the knowledge of large-scale pre-trained models, using limited data and computational resources. In this paper, we propose a novel few-shot feature distillation approach for vision transformers. Our approach is based on two key steps. Leveraging the fact that vision transformers have a consistent depth-wise structure, we first copy the weights from intermittent layers of existing pre-trained vision transformers (teachers) into shallower architectures (students), where the intermittence factor controls the complexity of the student transformer with respect to its teacher. Next, we employ an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge into the student in a few-shot scenario, aiming to recover the information processing carried out by the skipped teacher layers. We present comprehensive experiments with supervised and self-supervised transformers as teachers, on six data sets from various domains (natural, medical and satellite images) and tasks (classification and segmentation). The empirical results confirm the superiority of our approach over state-of-the-art competitors. Moreover, the ablation results demonstrate the usefulness of each component of the proposed pipeline. We release our code at https://github.com/dianagrigore/WeCoLoRA.",
        "published": "2024-10-31 04:00:00",
        "id": "3414f0b0-24ef-4c4f-bfd9-c5cb35a78c21",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种针对视觉变换器的小样本特征蒸馏新方法，包含权重复制和增强版低秩自适应两步，实验证明该方法在多数据集上优于竞品。"
        },
        "tokens": 849
    },
    {
        "title": "Exploring the Role of Token in Transformer-based Time Series Forecasting",
        "link": "https://arxiv.org/abs/2404.10337",
        "description": "arXiv:2404.10337v3 Announce Type: replace \nAbstract: Transformer-based methods are a mainstream approach for solving time series forecasting (TSF). These methods use temporal or variable tokens from observable data to make predictions. However, most focus on optimizing the model structure, with few studies paying attention to the role of tokens for predictions. The role is crucial since a model that distinguishes useful tokens from useless ones will predict more effectively. In this paper, we explore this issue. Through theoretical analyses, we find that the gradients mainly depend on tokens that contribute to the predicted series, called positive tokens. Based on this finding, we explore what helps models select these positive tokens. Through a series of experiments, we obtain three observations: i) positional encoding (PE) helps the model identify positive tokens; ii) as the network depth increases, the PE information gradually weakens, affecting the model's ability to identify positive tokens in deeper layers; iii) both enhancing PE in the deeper layers and using semantic-based PE can improve the model's ability to identify positive tokens, thus boosting performance. Inspired by these findings, we design temporal positional encoding (T-PE) for temporal tokens and variable positional encoding (V-PE) for variable tokens. To utilize T-PE and V-PE, we propose T2B-PE, a Transformer-based dual-branch framework. Extensive experiments demonstrate that T2B-PE has superior robustness and effectiveness.",
        "published": "2024-10-31 04:00:00",
        "id": "79e433c1-d814-497d-bf8a-76997c382333",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "Transformer-based方法是解决时间序列预测（TSF）的主流方法，本文通过理论分析探索了可观测数据中的token在预测中的作用，得出一些发现并据此设计了T-PE、V-PE和T2B-PE框架，实验表明T2B-PE具有较好的鲁棒性和有效性。"
        },
        "tokens": 913
    },
    {
        "title": "High-fidelity Endoscopic Image Synthesis by Utilizing Depth-guided Neural Surfaces",
        "link": "https://arxiv.org/abs/2404.13437",
        "description": "arXiv:2404.13437v2 Announce Type: replace \nAbstract: In surgical oncology, screening colonoscopy plays a pivotal role in providing diagnostic assistance, such as biopsy, and facilitating surgical navigation, particularly in polyp detection. Computer-assisted endoscopic surgery has recently gained attention and amalgamated various 3D computer vision techniques, including camera localization, depth estimation, surface reconstruction, etc. Neural Radiance Fields (NeRFs) and Neural Implicit Surfaces (NeuS) have emerged as promising methodologies for deriving accurate 3D surface models from sets of registered images, addressing the limitations of existing colon reconstruction approaches stemming from constrained camera movement.\n  However, the inadequate tissue texture representation and confused scale problem in monocular colonoscopic image reconstruction still impede the progress of the final rendering results. In this paper, we introduce a novel method for colon section reconstruction by leveraging NeuS applied to endoscopic images, supplemented by a single frame of depth map. Notably, we pioneered the exploration of utilizing only one frame depth map in photorealistic reconstruction and neural rendering applications while this single depth map can be easily obtainable from other monocular depth estimation networks with an object scale. Through rigorous experimentation and validation on phantom imagery, our approach demonstrates exceptional accuracy in completely rendering colon sections, even capturing unseen portions of the surface. This breakthrough opens avenues for achieving stable and consistently scaled reconstructions, promising enhanced quality in cancer screening procedures and treatment interventions.",
        "published": "2024-10-31 04:00:00",
        "id": "0a03e221-8764-4f99-af83-12729c26dfea",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一种利用NeuS结合单帧深度图重建结肠切片的新方法，通过实验验证该方法在完全渲染结肠切片时具有卓越准确性，为癌症筛查和治疗干预提供更好质量保障。"
        },
        "tokens": 883
    },
    {
        "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
        "link": "https://arxiv.org/abs/2404.15155",
        "description": "arXiv:2404.15155v3 Announce Type: replace \nAbstract: Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named Medical Decision-making Agents (MDAgents) that helps address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, emulating real-world medical decision-making processes adapted to tasks of varying complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks, including a comparison of LLMs' medical complexity classification against human physicians. MDAgents achieved the best performance in seven out of ten benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant improvement of up to 4.2% (p < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of 11.8%. Our code can be found at https://github.com/mitmedialab/MDAgents.",
        "published": "2024-10-31 04:00:00",
        "id": "e086f908-d2d7-46d9-9efe-7936d2816d49",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "MDAgents是一种新的多智能体框架，能自动为大型语言模型团队分配协作结构，在医疗知识理解和多模态推理任务的十个基准测试中七个表现最佳，代码可在https://github.com/mitmedialab/MDAgents获取。"
        },
        "tokens": 892
    },
    {
        "title": "Point-JEPA: A Joint Embedding Predictive Architecture for Self-Supervised Learning on Point Cloud",
        "link": "https://arxiv.org/abs/2404.16432",
        "description": "arXiv:2404.16432v4 Announce Type: replace \nAbstract: Recent advancements in self-supervised learning in the point cloud domain have demonstrated significant potential. However, these methods often suffer from drawbacks, including lengthy pre-training time, the necessity of reconstruction in the input space, or the necessity of additional modalities. In order to address these issues, we introduce Point-JEPA, a joint embedding predictive architecture designed specifically for point cloud data. To this end, we introduce a sequencer that orders point cloud tokens to efficiently compute and utilize tokens proximity based on their indices during target and context selection. The sequencer also allows shared computations of the tokens proximity between context and target selection, further improving the efficiency. Experimentally, our method achieves competitive results with state-of-the-art methods while avoiding the reconstruction in the input space or additional modality.",
        "published": "2024-10-31 04:00:00",
        "id": "0463e355-24b2-47a9-a3d2-2ef133bac4ba",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "为解决点云自监督学习现有方法的问题，Point - JEPA架构被提出，实验中该方法取得有竞争力的成果且无需输入空间重建或额外模态。"
        },
        "tokens": 757
    },
    {
        "title": "Dynamical Mode Recognition of Coupled Flame Oscillators by Supervised and Unsupervised Learning Approaches",
        "link": "https://arxiv.org/abs/2404.17801",
        "description": "arXiv:2404.17801v2 Announce Type: replace \nAbstract: Combustion instability in gas turbines and rocket engines, as one of the most challenging problems in combustion research, arises from the complex interactions among flames, which are also influenced by chemical reactions, heat and mass transfer, and acoustics. Identifying and understanding combustion instability is essential to ensure the safe and reliable operation of many combustion systems, where exploring and classifying the dynamical behaviors of complex flame systems is a core take. To facilitate fundamental studies, the present work concerns dynamical mode recognition of coupled flame oscillators made of flickering buoyant diffusion flames, which have gained increasing attention in recent years but are not sufficiently understood. The time series data of flame oscillators are generated by fully validated reacting flow simulations. Due to limitations of expertise-based models, a data-driven approach is adopted. In this study, a nonlinear dimensional reduction model of variational autoencoder (VAE) is used to project the simulation data onto a 2-dimensional latent space. Based on the phase trajectories in latent space, both supervised and unsupervised classifiers are proposed for datasets with well known labeling and without, respectively. For labeled datasets, we establish the Wasserstein-distance-based classifier (WDC) for mode recognition; for unlabeled datasets, we develop a novel unsupervised classifier (GMM-DTWC) combining dynamic time warping (DTW) and Gaussian mixture model (GMM). Through comparing with conventional approaches for dimensionality reduction and classification, the proposed supervised and unsupervised VAE-based approaches exhibit a prominent performance for distinguishing dynamical modes, implying their potential extension to dynamical mode recognition of complex combustion problems.",
        "published": "2024-10-31 04:00:00",
        "id": "ebfe9ac1-2a62-46ec-9f53-721cd3244a1a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "采用数据驱动方法，利用变分自动编码器的非线性降维模型处理火焰振荡器模拟数据，提出监督和非监督分类器进行动力模式识别，在区分动力模式上表现突出。"
        },
        "tokens": 925
    },
    {
        "title": "Estimation of uncertainties in the density driven flow in fractured porous media using MLMC",
        "link": "https://arxiv.org/abs/2404.18003",
        "description": "arXiv:2404.18003v3 Announce Type: replace \nAbstract: We use the Multi Level Monte Carlo method to estimate uncertainties in a Henry-like salt water intrusion problem with a fracture. The flow is induced by the variation of the density of the fluid phase, which depends on the mass fraction of salt. We assume that the fracture has a known fixed location but an uncertain aperture. Other input uncertainties are the porosity and permeability fields and the recharge. In our setting, porosity and permeability vary spatially and recharge is time-dependent. For each realisation of these uncertain parameters, the evolution of the mass fraction and pressure fields is modelled by a system of non-linear and time-dependent PDEs with a jump of the solution at the fracture. The uncertainties propagate into the distribution of the salt concentration, which is an important characteristic of the quality of water resources. We show that the multilevel Monte Carlo (MLMC) method is able to reduce the overall computational cost compared to classical Monte Carlo methods. This is achieved by balancing discretisation and statistical errors. Multiple scenarios are evaluated at different spatial and temporal mesh levels. The deterministic solver ug4 is run in parallel to calculate all stochastic scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "887955d3-2fa4-4f3c-94c0-2a80d8fdb1d6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "使用多层蒙特卡洛方法估计裂隙多孔介质中密度驱动流的不确定性，将其应用于有裂隙的Henry型盐水入侵问题，通过平衡离散化和统计误差降低计算成本，并行运行ug4求解器计算所有随机场景。"
        },
        "tokens": 838
    },
    {
        "title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs",
        "link": "https://arxiv.org/abs/2405.00552",
        "description": "arXiv:2405.00552v4 Announce Type: replace \nAbstract: We present a novel approach for long-term human trajectory prediction in indoor human-centric environments, which is essential for long-horizon robot planning in these environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged (i.e., evaluated in a zero-shot fashion on the dataset) baselines for a time horizon of 60s.",
        "published": "2024-10-31 04:00:00",
        "id": "8798caa5-bb62-4bda-b4b0-fb3386f79b5c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种用于室内以人为主环境中的长期人类轨迹预测新方法，利用大型语言模型预测人与环境的交互，通过3D动态场景图提供场景信息，用概率方法得出人类位置的多模态时空分布，引入新数据集进行评估，实验显示相比基线有更好表现。"
        },
        "tokens": 918
    },
    {
        "title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning",
        "link": "https://arxiv.org/abs/2405.00705",
        "description": "arXiv:2405.00705v2 Announce Type: replace \nAbstract: The pre-trained Large Language Models (LLMs) can be adapted for many downstream tasks and tailored to align with human preferences through fine-tuning. Recent studies have discovered that LLMs can achieve desirable performance with only a small amount of high-quality data, suggesting that a large amount of the data in these extensive datasets is redundant or even harmful. Identifying high-quality data from vast datasets to curate small yet effective datasets has emerged as a critical challenge. In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning. SHED eliminates the need for human intervention or the use of commercial LLMs. Moreover, the datasets curated through SHED exhibit transferability, indicating they can be reused across different LLMs with consistently high performance. We conduct extensive experiments to evaluate the datasets curated by SHED. The results demonstrate SHED's superiority over state-of-the-art methods across various tasks and LLMs; notably, datasets comprising only 10% of the original data selected by SHED achieve performance comparable to or surpassing that of the full datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "a965a1d9-3767-4088-bb1c-c3a91904fcc4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文介绍了SHED这种基于Shapley值的自动数据集优化框架用于指令微调，无需人工干预或商业LLM，其精选的数据集可跨不同LLM复用且性能高，实验证明它在各任务和LLM上优于现有方法。"
        },
        "tokens": 843
    },
    {
        "title": "Designing Algorithmic Recommendations to Achieve Human-AI Complementarity",
        "link": "https://arxiv.org/abs/2405.01484",
        "description": "arXiv:2405.01484v2 Announce Type: replace \nAbstract: Algorithms frequently assist, rather than replace, human decision-makers. However, the design and analysis of algorithms often focus on predicting outcomes and do not explicitly model their effect on human decisions. This discrepancy between the design and role of algorithmic assistants becomes particularly concerning in light of empirical evidence that suggests that algorithmic assistants again and again fail to improve human decisions. In this article, we formalize the design of recommendation algorithms that assist human decision-makers without making restrictive ex-ante assumptions about how recommendations affect decisions. We formulate an algorithmic-design problem that leverages the potential-outcomes framework from causal inference to model the effect of recommendations on a human decision-maker's binary treatment choice. Within this model, we introduce a monotonicity assumption that leads to an intuitive classification of human responses to the algorithm. Under this assumption, we can express the human's response to algorithmic recommendations in terms of their compliance with the algorithm and the active decision they would take if the algorithm sends no recommendation. We showcase the utility of our framework using an online experiment that simulates a hiring task. We argue that our approach can make sense of the relative performance of different recommendation algorithms in the experiment and can help design solutions that realize human-AI complementarity. Finally, we leverage our approach to derive minimax optimal recommendation algorithms that can be implemented with machine learning using limited training data.",
        "published": "2024-10-31 04:00:00",
        "id": "9d686db3-0607-437e-8ef8-59e203e02c99",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出一种推荐算法设计，该设计不预先假设推荐如何影响决策以辅助人类决策者，利用因果推理中的潜在结果框架建模推荐对人类决策的影响，通过在线实验展示框架效用，还推导了可使用有限训练数据通过机器学习实现的极小极大最优推荐算法。"
        },
        "tokens": 898
    },
    {
        "title": "Advanced Detection of Source Code Clones via an Ensemble of Unsupervised Similarity Measures",
        "link": "https://arxiv.org/abs/2405.02095",
        "description": "arXiv:2405.02095v2 Announce Type: replace \nAbstract: The capability of accurately determining code similarity is crucial in many tasks related to software development. For example, it might be essential to identify code duplicates for performing software maintenance. This research introduces a novel ensemble learning approach for code similarity assessment, combining the strengths of multiple unsupervised similarity measures. The key idea is that the strengths of a diverse set of similarity measures can complement each other and mitigate individual weaknesses, leading to improved performance. Preliminary results show that while Transformers-based CodeBERT and its variant GraphCodeBERT are undoubtedly the best option in the presence of abundant training data, in the case of specific small datasets (up to 500 samples), our ensemble achieves similar results, without prejudice to the interpretability of the resulting solution, and with a much lower associated carbon footprint due to training. The source code of this novel approach can be downloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.",
        "published": "2024-10-31 04:00:00",
        "id": "732c5c98-7d1e-44ef-8e0a-4987d5a84097",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出一种新的代码相似性评估集成学习方法，结合多种无监督相似性度量的优势，初步结果显示在特定小数据集上该集成方法与基于Transformer的方法效果相似且具有低训练碳足迹，其源代码可从指定网址下载。"
        },
        "tokens": 812
    },
    {
        "title": "U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers",
        "link": "https://arxiv.org/abs/2405.02730",
        "description": "arXiv:2405.02730v3 Announce Type: replace \nAbstract: Diffusion Transformers (DiTs) introduce the transformer architecture to diffusion tasks for latent-space image generation. With an isotropic architecture that chains a series of transformer blocks, DiTs demonstrate competitive performance and good scalability; but meanwhile, the abandonment of U-Net by DiTs and their following improvements is worth rethinking. To this end, we conduct a simple toy experiment by comparing a U-Net architectured DiT with an isotropic one. It turns out that the U-Net architecture only gain a slight advantage amid the U-Net inductive bias, indicating potential redundancies within the U-Net-style DiT. Inspired by the discovery that U-Net backbone features are low-frequency-dominated, we perform token downsampling on the query-key-value tuple for self-attention that bring further improvements despite a considerable amount of reduction in computation. Based on self-attention with downsampled tokens, we propose a series of U-shaped DiTs (U-DiTs) in the paper and conduct extensive experiments to demonstrate the extraordinary performance of U-DiT models. The proposed U-DiT could outperform DiT-XL/2 with only 1/6 of its computation cost. Codes are available at https://github.com/YuchuanTian/U-DiT.",
        "published": "2024-10-31 04:00:00",
        "id": "078780d7-9931-4de7-9db7-15e5c838da6e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文《U-DiTs: Downsample Tokens in U - Shaped Diffusion Transformers》通过对比U - Net架构和各向同性架构的DiT进行实验，受U - Net骨干特征低频主导发现的启发提出U - DiTs，其性能优异且计算成本低。"
        },
        "tokens": 886
    },
    {
        "title": "On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations",
        "link": "https://arxiv.org/abs/2405.03489",
        "description": "arXiv:2405.03489v3 Announce Type: replace \nAbstract: Numerous Deep Learning (DL)-based approaches have gained attention in software Log Anomaly Detection (LAD), yet class imbalance in training data remains a challenge, with anomalies often comprising less than 1% of datasets like Thunderbird. Existing DLLAD methods may underperform in severely imbalanced datasets. Although data resampling has proven effective in other software engineering tasks, it has not been explored in LAD. This study aims to fill this gap by providing an in-depth analysis of the impact of diverse data resampling methods on existing DLLAD approaches from two distinct perspectives. Firstly, we assess the performance of these DLLAD approaches across four datasets with different levels of class imbalance, and we explore the impact of resampling ratios of normal to abnormal data on DLLAD approaches. Secondly, we evaluate the effectiveness of the data resampling methods when utilizing optimal resampling ratios of normal to abnormal data. Our findings indicate that oversampling methods generally outperform undersampling and hybrid sampling methods. Data resampling on raw data yields superior results compared to data resampling in the feature space. These improvements are attributed to the increased attention given to important tokens. By exploring the resampling ratio of normal to abnormal data, we suggest generating more data for minority classes through oversampling while removing less data from majority classes through undersampling. In conclusion, our study provides valuable insights into the intricate relationship between data resampling methods and DLLAD. By addressing the challenge of class imbalance, researchers and practitioners can enhance DLLAD performance.",
        "published": "2024-10-31 04:00:00",
        "id": "37367809-60b1-473a-af72-a6a5a3a5fc07",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究从两个视角深入分析多种数据重采样方法对深度学习日志异常检测方法的影响，发现过采样方法总体表现更好，原始数据重采样结果更优，并对正常与异常数据的重采样比例提出建议。"
        },
        "tokens": 914
    },
    {
        "title": "Aequitas Flow: Streamlining Fair ML Experimentation",
        "link": "https://arxiv.org/abs/2405.05809",
        "description": "arXiv:2405.05809v2 Announce Type: replace \nAbstract: Aequitas Flow is an open-source framework and toolkit for end-to-end Fair Machine Learning (ML) experimentation, and benchmarking in Python. This package fills integration gaps that exist in other fair ML packages. In addition to the existing audit capabilities in Aequitas, the Aequitas Flow module provides a pipeline for fairness-aware model training, hyperparameter optimization, and evaluation, enabling easy-to-use and rapid experiments and analysis of results. Aimed at ML practitioners and researchers, the framework offers implementations of methods, datasets, metrics, and standard interfaces for these components to improve extensibility. By facilitating the development of fair ML practices, Aequitas Flow hopes to enhance the incorporation of fairness concepts in AI systems making AI systems more robust and fair.",
        "published": "2024-10-31 04:00:00",
        "id": "529c6b23-35be-46b2-af2c-648c6a1da1c1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Aequitas Flow是一个用于端到端公平机器学习实验和基准测试的开源框架和工具包，填补了其他公平ML包中的集成空白，提供公平感知模型训练、超参数优化和评估管道，旨在增强AI系统中的公平性概念。"
        },
        "tokens": 765
    },
    {
        "title": "PyZoBot: A Platform for Conversational Information Extraction and Synthesis from Curated Zotero Reference Libraries through Advanced Retrieval-Augmented Generation",
        "link": "https://arxiv.org/abs/2405.07963",
        "description": "arXiv:2405.07963v2 Announce Type: replace \nAbstract: The exponential growth of scientific literature has resulted in information overload, challenging researchers to effectively synthesize relevant publications. This paper explores the integration of traditional reference management software with advanced computational techniques, including Large Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an AI-driven platform developed in Python, incorporating Zoteros reference management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge extraction and synthesis from extensive human-curated scientific literature databases. It demonstrates proficiency in handling complex natural language queries, integrating data from multiple sources, and meticulously presenting references to uphold research integrity and facilitate further exploration. By leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot offers an effective solution to manage information overload and keep pace with rapid scientific advancements. The development of such AI-enhanced tools promises significant improvements in research efficiency and effectiveness across various disciplines.",
        "published": "2024-10-31 04:00:00",
        "id": "457d5d2b-4827-43a3-885e-0a13df7db490",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文介绍了PyZoBot这一由Python开发的人工智能平台，它结合Zotero的参考管理与OpenAI的大型语言模型，通过检索增强生成技术，从人类精选的科学文献数据库中进行知识提取和合成，以应对信息过载并跟上科学进步。"
        },
        "tokens": 817
    },
    {
        "title": "Compositional imprecise probability",
        "link": "https://arxiv.org/abs/2405.09391",
        "description": "arXiv:2405.09391v2 Announce Type: replace \nAbstract: Imprecise probability is concerned with uncertainty about which probability distributions to use. It has applications in robust statistics and machine learning.\n  We look at programming language models for imprecise probability. Our desiderata are that we would like our model to support all kinds of composition, categorical and monoidal; in other words, guided by dataflow diagrams. Another equivalent perspective is that we would like a model of synthetic probability in the sense of Markov categories.\n  Imprecise probability can be modelled in various ways, with the leading monad-based approach using convex sets of probability distributions. This model is not fully compositional because the monad involved is not commutative, meaning it does not have a proper monoidal structure. In this work, we provide a new fully compositional account. The key idea is to name the non-deterministic choices. To manage the renamings and disjointness of names, we use graded monads. We show that the resulting compositional model is maximal and relate it with the earlier monadic approach, proving that we obtain tighter bounds on the uncertainty.",
        "published": "2024-10-31 04:00:00",
        "id": "b63b536a-33bd-4e42-ac20-bbc119d5d073",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究不精确概率的编程模型，提出一种新的完全组合式解释，使用分级单子管理重命名和名称不相交性以实现最大组合模型并得到更紧的不确定性界限。"
        },
        "tokens": 818
    },
    {
        "title": "A Theory of Synaptic Neural Balance: From Local to Global Order",
        "link": "https://arxiv.org/abs/2405.09688",
        "description": "arXiv:2405.09688v3 Announce Type: replace \nAbstract: We develop a general theory of synaptic neural balance and how it can emerge or be enforced in neural networks. For a given regularizer, a neuron is said to be in balance if the total cost of its input weights is equal to the total cost of its output weights. The basic example is provided by feedforward networks of ReLU units trained with $L_2$ regularizers, which exhibit balance after proper training. The theory explains this phenomenon and extends it in several directions. The first direction is the extension to bilinear and other activation functions. The second direction is the extension to more general regularizers, including all $L_p$ regularizers. The third direction is the extension to non-layered architectures, recurrent architectures, convolutional architectures, as well as architectures with mixed activation functions. Gradient descent on the error function alone does not converge in general to a balanced state, where every neuron is in balance, even when starting from a balanced state. However, gradient descent on the regularized error function ought to converge to a balanced state, and thus network balance can be used to assess learning progress. The theory is based on two local neuronal operations: scaling which is commutative, and balancing which is not commutative. Given any initial set of weights, when local balancing operations are applied to each neuron in a stochastic manner, global order always emerges through the convergence of the stochastic balancing algorithm to the same unique set of balanced weights. The reason for this is the existence of an underlying strictly convex optimization problem where the relevant variables are constrained to a linear, only architecture-dependent, manifold. Simulations show that balancing neurons prior to learning, or during learning in alternation with gradient descent steps, can improve learning speed and final performance.",
        "published": "2024-10-31 04:00:00",
        "id": "47c11bc3-5331-4a88-ad32-08bf093652e7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种突触神经平衡理论，阐述其在神经网络中的产生或强制方式、多种扩展方向、收敛性，并通过模拟表明平衡神经元可提高学习速度和最终性能。"
        },
        "tokens": 943
    },
    {
        "title": "Implicit-ARAP: Efficient Handle-Guided Deformation of High-Resolution Meshes and Neural Fields via Local Patch Meshing",
        "link": "https://arxiv.org/abs/2405.12895",
        "description": "arXiv:2405.12895v2 Announce Type: replace \nAbstract: In this work, we present the local patch mesh representation for neural signed distance fields. This technique allows to discretize local regions of the level sets of an input SDF by projecting and deforming flat patch meshes onto the level set surface, using exclusively the SDF information and its gradient. Our analysis reveals this method to be more accurate than the standard marching cubes algorithm for approximating the implicit surface. Then, we apply this representation in the setting of handle-guided deformation: we introduce two distinct pipelines, which make use of 3D neural fields to compute As-Rigid-As-Possible deformations of both high-resolution meshes and neural fields under a given set of constraints. We run a comprehensive evaluation of our method and various baselines for neural field and mesh deformation which show both pipelines achieve impressive efficiency and notable improvements in terms of quality of results and robustness. With our novel pipeline, we introduce a scalable approach to solve a well-established geometry processing problem on high-resolution meshes, and pave the way for extending other geometric tasks to the domain of implicit surfaces via local patch meshing.",
        "published": "2024-10-31 04:00:00",
        "id": "6bbaed81-b339-4002-a34c-b1d2a621dd1a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出用于神经有向距离场的局部面片网格表示，应用于手柄引导变形，评估表明相关管道在神经场和网格变形方面实现高效、结果质量和鲁棒性提升"
        },
        "tokens": 833
    },
    {
        "title": "Jumping Automata Must Pay",
        "link": "https://arxiv.org/abs/2405.11849",
        "description": "arXiv:2405.11849v2 Announce Type: replace \nAbstract: Jumping automata are finite automata that read their input in a non-sequential manner, by allowing a reading head to \"jump\" between positions on the input, consuming a permutation of the input word. We argue that allowing the head to jump should incur some cost. To this end, we propose three quantitative semantics for jumping automata, whereby the jumps of the head in an accepting run define the cost of the run. The three semantics correspond to different interpretations of jumps: the absolute distance semantics counts the distance the head jumps, the reversal semantics counts the number of times the head changes direction, and the Hamming distance measures the number of letter-swaps the run makes.\n  We study these measures, with the main focus being the boundedness problem: given a jumping automaton, decide whether its (quantitative) languages is bounded by some given number k. We establish the decidability and complexity for this problem under several variants.",
        "published": "2024-10-31 04:00:00",
        "id": "bb2fb4f3-c5a7-48a4-8ae2-59e2a687fe4b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出跳跃自动机的三种定量语义，主要研究其有界性问题的可判定性和复杂性。"
        },
        "tokens": 768
    },
    {
        "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
        "link": "https://arxiv.org/abs/2405.12399",
        "description": "arXiv:2405.12399v2 Announce Type: replace \nAbstract: World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. We further demonstrate that DIAMOND's diffusion world model can stand alone as an interactive neural game engine by training on static Counter-Strike: Global Offensive gameplay. To foster future research on diffusion for world modeling, we release our code, agents, videos and playable world models at https://diamond-wm.github.io.",
        "published": "2024-10-31 04:00:00",
        "id": "e6cbbb3c-e3bc-475c-941e-5939c29db30e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文《Diffusion for World Modeling: Visual Details Matter in Atari》介绍DIAMOND，分析使扩散适用于世界建模的关键设计选择，其在Atari 100k基准上取得新成绩且扩散世界模型可作为交互式神经游戏引擎，相关代码等已开源。"
        },
        "tokens": 871
    },
    {
        "title": "Detecting and Mitigating Bias in Algorithms Used to Disseminate Information in Social Networks",
        "link": "https://arxiv.org/abs/2405.12764",
        "description": "arXiv:2405.12764v2 Announce Type: replace \nAbstract: Social connections are conduits through which individuals communicate, information propagates, and diseases spread. Identifying individuals who are more likely to adopt ideas and spread them is essential in order to develop effective information campaigns, maximize the reach of resources, and fight epidemics. Influence maximization algorithms are used to identify sets of influencers. Based on extensive computer simulations on synthetic and ten diverse real-world social networks we show that seeding information using these methods creates information gaps. Our results show that these algorithms select influencers who do not disseminate information equitably, threatening to create an increasingly unequal society. To overcome this issue we devise a multi-objective algorithm which maximizes influence and information equity. Our results demonstrate it is possible to reduce vulnerability at a relatively low trade-off with respect to spread. This highlights that in our search for maximizing information we do not need to compromise on information equality.",
        "published": "2024-10-31 04:00:00",
        "id": "e33632a9-1066-4694-bd5a-4f112e5e4e04",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过对合成和十个不同真实世界社交网络的计算机模拟，发现影响最大化算法在传播信息时会造成信息差距，为此设计了一个多目标算法，能在较小的传播权衡下降低脆弱性，实现影响力和信息公平性的最大化。"
        },
        "tokens": 800
    },
    {
        "title": "Reward Centering",
        "link": "https://arxiv.org/abs/2405.09999",
        "description": "arXiv:2405.09999v2 Announce Type: replace \nAbstract: We show that discounted methods for solving continuing reinforcement learning problems can perform significantly better if they center their rewards by subtracting out the rewards' empirical average. The improvement is substantial at commonly used discount factors and increases further as the discount factor approaches one. In addition, we show that if a problem's rewards are shifted by a constant, then standard methods perform much worse, whereas methods with reward centering are unaffected. Estimating the average reward is straightforward in the on-policy setting; we propose a slightly more sophisticated method for the off-policy setting. Reward centering is a general idea, so we expect almost every reinforcement-learning algorithm to benefit by the addition of reward centering.",
        "published": "2024-10-31 04:00:00",
        "id": "67bc80a3-7de7-4cba-9062-d6b90375bd13",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文显示在解决连续强化学习问题时，折扣方法通过将奖励减去其经验平均值来中心化奖励会表现得更好，在策略设置中估计平均奖励较直接，在离策略设置中有更复杂方法，几乎所有强化学习算法都可受益于奖励中心化。"
        },
        "tokens": 745
    },
    {
        "title": "On the stability of gradient descent with second order dynamics for time-varying cost functions",
        "link": "https://arxiv.org/abs/2405.13765",
        "description": "arXiv:2405.13765v2 Announce Type: replace \nAbstract: Gradient based optimization algorithms deployed in Machine Learning (ML) applications are often analyzed and compared by their convergence rates or regret bounds. While these rates and bounds convey valuable information they don't always directly translate to stability guarantees. Stability and similar concepts, like robustness, will become ever more important as we move towards deploying models in real-time and safety critical systems. In this work we build upon the results in Gaudio et al. 2021 and Moreu & Annaswamy 2022 for gradient descent with second order dynamics when applied to explicitly time varying cost functions and provide more general stability guarantees. These more general results can aid in the design and certification of these optimization schemes so as to help ensure safe and reliable deployment for real-time learning applications. We also hope that the techniques provided here will stimulate and cross-fertilize the analysis that occurs on the same algorithms from the online learning and stochastic optimization communities.",
        "published": "2024-10-31 04:00:00",
        "id": "8bc679c8-a952-4fef-b6d9-305ce38125cc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究基于Gaudio等人2021年和Moreu & Annaswamy 2022年成果，为二阶动态梯度下降应用于时变成本函数提供更通用的稳定性保证，有助于实时学习应用的安全可靠部署并希望激发相关算法分析。"
        },
        "tokens": 810
    },
    {
        "title": "LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate",
        "link": "https://arxiv.org/abs/2405.13985",
        "description": "arXiv:2405.13985v2 Announce Type: replace \nAbstract: High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning -- ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.\n  We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) -- on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.",
        "published": "2024-10-31 04:00:00",
        "id": "e604b87d-071f-40cc-9f54-215e433ff8b1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对视觉Transformer（ViT）无法有效利用更大图像（若不微调）的问题，提出名为LookHere的新方法，在无外推时可提升性能，有外推时性能优于现有方法，还发布了高分辨率测试集ImageNet - HR。"
        },
        "tokens": 910
    },
    {
        "title": "Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning",
        "link": "https://arxiv.org/abs/2405.14039",
        "description": "arXiv:2405.14039v2 Announce Type: replace \nAbstract: Real-world data deviating from the independent and identically distributed (i.i.d.) assumption of in-distribution training data poses security threats to deep networks, thus advancing out-of-distribution (OOD) detection algorithms. Detection methods in generative language models (GLMs) mainly focus on uncertainty estimation and embedding distance measurement, with the latter proven to be most effective in traditional linguistic tasks like summarization and translation. However, another complex generative scenario mathematical reasoning poses significant challenges to embedding-based methods due to its high-density feature of output spaces, but this feature causes larger discrepancies in the embedding shift trajectory between different samples in latent spaces. Hence, we propose a trajectory-based method TV score, which uses trajectory volatility for OOD detection in mathematical reasoning. Experiments show that our method outperforms all traditional algorithms on GLMs under mathematical reasoning scenarios and can be extended to more applications with high-density features in output spaces, such as multiple-choice questions.",
        "published": "2024-10-31 04:00:00",
        "id": "43b5cc09-0182-4655-b468-3b11c0a613d2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对现实世界数据偏离分布训练数据的独立同分布假设对深度网络造成安全威胁的问题，提出一种基于轨迹的TV分数方法，用于数学推理中的离群值检测，该方法在数学推理场景下的生成式语言模型中优于传统算法且可扩展到更多输出空间有高密度特征的应用。"
        },
        "tokens": 823
    },
    {
        "title": "SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network",
        "link": "https://arxiv.org/abs/2405.14398",
        "description": "arXiv:2405.14398v3 Announce Type: replace \nAbstract: Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture-recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness. To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\%$). Moreover, the actual deployment on the CPU demonstrated a system latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at https://github.com/guoweiyu/SpGesture/.",
        "published": "2024-10-31 04:00:00",
        "id": "e710bcd3-dd0b-4686-a87a-4c182d64cd7c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于脉冲神经网络的SpGesture框架用于基于表面肌电信号（sEMG）的手势识别，具有鲁棒性、高精度等优点，还收集新数据集验证，代码已开源。"
        },
        "tokens": 918
    },
    {
        "title": "YOLOv10: Real-Time End-to-End Object Detection",
        "link": "https://arxiv.org/abs/2405.14458",
        "description": "arXiv:2405.14458v2 Announce Type: replace \nAbstract: Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance.",
        "published": "2024-10-31 04:00:00",
        "id": "2c9464fd-24e4-49de-9185-60d7abe09ec5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文《YOLOv10: Real - Time End - to - End Object Detection》提出了无NMS训练的一致双分配法和整体效率 - 精度驱动的模型设计策略来优化YOLO，新的YOLOv10系列在各模型规模上实现了性能和效率的先进水平。"
        },
        "tokens": 1013
    },
    {
        "title": "DisC-GS: Discontinuity-aware Gaussian Splatting",
        "link": "https://arxiv.org/abs/2405.15196",
        "description": "arXiv:2405.15196v2 Announce Type: replace \nAbstract: Recently, Gaussian Splatting, a method that represents a 3D scene as a collection of Gaussian distributions, has gained significant attention in addressing the task of novel view synthesis. In this paper, we highlight a fundamental limitation of Gaussian Splatting: its inability to accurately render discontinuities and boundaries in images due to the continuous nature of Gaussian distributions. To address this issue, we propose a novel framework enabling Gaussian Splatting to perform discontinuity-aware image rendering. Additionally, we introduce a B\\'ezier-boundary gradient approximation strategy within our framework to keep the \"differentiability\" of the proposed discontinuity-aware rendering process. Extensive experiments demonstrate the efficacy of our framework.",
        "published": "2024-10-31 04:00:00",
        "id": "c5c16292-060c-46d1-abbe-8ec4cc73cb1d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "针对高斯散点表示3D场景的高斯喷溅法不能准确渲染图像中的不连续和边界这一局限，论文提出新框架实现不连续感知图像渲染，并引入贝塞尔边界梯度近似策略，实验证明其有效性。"
        },
        "tokens": 749
    },
    {
        "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search",
        "link": "https://arxiv.org/abs/2405.15383",
        "description": "arXiv:2405.15383v2 Announce Type: replace \nAbstract: In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has potential to be more precise, reliable, interpretable, and extremely efficient. However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",
        "published": "2024-10-31 04:00:00",
        "id": "497ba546-dc8e-42c9-846b-3a7999fa4c95",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出GIF - MCTS代码生成策略，引入CWMB基准，GIF - MCTS在CWMB和其他两个基准上超越所有基线，其合成的代码世界模型可用于规划以提高模型 - 基于强化学习代理的样本效率和推理速度。"
        },
        "tokens": 854
    },
    {
        "title": "Kronecker-Factored Approximate Curvature for Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2405.15603",
        "description": "arXiv:2405.15603v3 Announce Type: replace \nAbstract: Physics-informed neural networks (PINNs) are infamous for being hard to train. Recently, second-order methods based on natural gradient and Gauss-Newton methods have shown promising performance, improving the accuracy achieved by first-order methods by several orders of magnitude. While promising, the proposed methods only scale to networks with a few thousand parameters due to the high computational cost to evaluate, store, and invert the curvature matrix. We propose Kronecker-factored approximate curvature (KFAC) for PINN losses that greatly reduces the computational cost and allows scaling to much larger networks. Our approach goes beyond the established KFAC for traditional deep learning problems as it captures contributions from a PDE's differential operator that are crucial for optimization. To establish KFAC for such losses, we use Taylor-mode automatic differentiation to describe the differential operator's computation graph as a forward network with shared weights. This allows us to apply KFAC thanks to a recently-developed general formulation for networks with weight sharing. Empirically, we find that our KFAC-based optimizers are competitive with expensive second-order methods on small problems, scale more favorably to higher-dimensional neural networks and PDEs, and consistently outperform first-order methods and LBFGS.",
        "published": "2024-10-31 04:00:00",
        "id": "a4fa8cb5-dfeb-4781-a3d9-d37079af3d3e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种用于物理信息神经网络（PINNs）损失的Kronecker分解近似曲率（KFAC）方法，可降低计算成本并适用于更大网络，经验上在一些方面优于一阶方法和LBFGS。"
        },
        "tokens": 853
    },
    {
        "title": "The Road Less Scheduled",
        "link": "https://arxiv.org/abs/2405.15682",
        "description": "arXiv:2405.15682v4 Announce Type: replace \nAbstract: Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.",
        "published": "2024-10-31 04:00:00",
        "id": "57db9208-4ca3-4717-be3a-1040fca9f615",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": true,
                "relRank": 5
            },
            "keyFacts": "提出一种无调度方法，不依赖学习率调度且无需指定优化停止步骤，相比现有方法有卓越性能，无额外超参数，是新理论的成果且有开源实现，Schedule - Free AdamW是某挑战赛获胜作品的核心算法。"
        },
        "tokens": 779
    },
    {
        "title": "Improved Particle Approximation Error for Mean Field Neural Networks",
        "link": "https://arxiv.org/abs/2405.15767",
        "description": "arXiv:2405.15767v3 Announce Type: replace \nAbstract: Mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional defined over the space of probability distributions. MFLD has gained attention due to its connection with noisy gradient descent for mean-field two-layer neural networks. Unlike standard Langevin dynamics, the nonlinearity of the objective functional induces particle interactions, necessitating multiple particles to approximate the dynamics in a finite-particle setting. Recent works (Chen et al., 2022; Suzuki et al., 2023b) have demonstrated the uniform-in-time propagation of chaos for MFLD, showing that the gap between the particle system and its mean-field limit uniformly shrinks over time as the number of particles increases. In this work, we improve the dependence on logarithmic Sobolev inequality (LSI) constants in their particle approximation errors, which can exponentially deteriorate with the regularization coefficient. Specifically, we establish an LSI-constant-free particle approximation error concerning the objective gap by leveraging the problem structure in risk minimization. As the application, we demonstrate improved convergence of MFLD, sampling guarantee for the mean-field stationary distribution, and uniform-in-time Wasserstein propagation of chaos in terms of particle complexity.",
        "published": "2024-10-31 04:00:00",
        "id": "e440d890-9186-4bbc-a9eb-f0b454f0264a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究改进了平均场神经网络中粒子近似误差对对数Sobolev不等式常数的依赖关系，建立无LSI常数的粒子近似误差，还展示了MFLD收敛性、采样保证和混沌传播的改进。"
        },
        "tokens": 851
    },
    {
        "title": "Accelerating Transformers with Spectrum-Preserving Token Merging",
        "link": "https://arxiv.org/abs/2405.16148",
        "description": "arXiv:2405.16148v2 Announce Type: replace \nAbstract: Increasing the throughput of the Transformer architecture, a foundational component used in numerous state-of-the-art models for vision and language tasks (e.g., GPT, LLaVa), is an important problem in machine learning. One recent and effective strategy is to merge token representations within Transformer models, aiming to reduce computational and memory requirements while maintaining accuracy. Prior works have proposed algorithms based on Bipartite Soft Matching (BSM), which divides tokens into distinct sets and merges the top k similar tokens. However, these methods have significant drawbacks, such as sensitivity to token-splitting strategies and damage to informative tokens in later layers. This paper presents a novel paradigm called PiToMe, which prioritizes the preservation of informative tokens using an additional metric termed the energy score. This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved. Experimental findings demonstrate that PiToMe saved from 40-60\\% FLOPs of the base models while exhibiting superior off-the-shelf performance on image classification (0.5\\% average performance drop of ViT-MAE-H compared to 2.6\\% as baselines), image-text retrieval (0.3\\% average performance drop of CLIP on Flickr30k compared to 4.5\\% as others), and analogously in visual questions answering with LLaVa-7B. Furthermore, PiToMe is theoretically shown to preserve intrinsic spectral properties of the original token space under mild conditions",
        "published": "2024-10-31 04:00:00",
        "id": "dc1dc213-58a5-47ad-a473-44bde07b27ee",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出名为PiToMe的新范式，用能量分数优先保留信息性token以加速Transformer架构，在多个任务中减少大量FLOPs且性能优于同类方法，理论上在温和条件下可保留原始token空间的固有光谱特性。"
        },
        "tokens": 931
    },
    {
        "title": "Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception",
        "link": "https://arxiv.org/abs/2405.16493",
        "description": "arXiv:2405.16493v2 Announce Type: replace \nAbstract: Biological motion perception (BMP) refers to humans' ability to perceive and recognize the actions of living beings solely from their motion patterns, sometimes as minimal as those depicted on point-light displays. While humans excel at these tasks without any prior training, current AI models struggle with poor generalization performance. To close this research gap, we propose the Motion Perceiver (MP). MP solely relies on patch-level optical flows from video clips as inputs. During training, it learns prototypical flow snapshots through a competitive binding mechanism and integrates invariant motion representations to predict action labels for the given video. During inference, we evaluate the generalization ability of all AI models and humans on 62,656 video stimuli spanning 24 BMP conditions using point-light displays in neuroscience. Remarkably, MP outperforms all existing AI models with a maximum improvement of 29% in top-1 action recognition accuracy on these conditions. Moreover, we benchmark all AI models in point-light displays of two standard video datasets in computer vision. MP also demonstrates superior performance in these cases. More interestingly, via psychophysics experiments, we found that MP recognizes biological movements in a way that aligns with human behaviors. Our data and code are available at https://github.com/ZhangLab-DeepNeuroCogLab/MotionPerceiver.",
        "published": "2024-10-31 04:00:00",
        "id": "44ff8959-203b-42c0-ba58-efa5e9a614bd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出Motion Perceiver（MP）模型，其仅依靠视频片段的补丁级光流输入，通过竞争绑定机制学习原型流快照并集成不变运动表示来预测动作标签，在生物运动感知任务上的泛化能力优于现有AI模型，且其识别生物运动的方式与人类行为相符。"
        },
        "tokens": 898
    },
    {
        "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
        "link": "https://arxiv.org/abs/2405.17503",
        "description": "arXiv:2405.17503v3 Announce Type: replace \nAbstract: Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.",
        "published": "2024-10-31 04:00:00",
        "id": "05f63787-5dd4-4f2e-8dfc-45e2294627b1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨用大型语言模型（LLMs）迭代改进和修复源代码时存在探索 - 利用权衡问题，提出基于汤普森采样解决该问题的LLM程序合成算法，该算法在多个领域更高效。"
        },
        "tokens": 800
    },
    {
        "title": "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes",
        "link": "https://arxiv.org/abs/2405.17580",
        "description": "arXiv:2405.17580v2 Announce Type: replace \nAbstract: The training dynamics of linear networks are well studied in two distinct setups: the lazy regime and balanced/active regime, depending on the initialization and width of the network. We provide a surprisingly simple unifying formula for the evolution of the learned matrix that contains as special cases both lazy and balanced regimes but also a mixed regime in between the two. In the mixed regime, a part of the network is lazy while the other is balanced. More precisely the network is lazy along singular values that are below a certain threshold and balanced along those that are above the same threshold. At initialization, all singular values are lazy, allowing for the network to align itself with the task, so that later in time, when some of the singular value cross the threshold and become active they will converge rapidly (convergence in the balanced regime is notoriously difficult in the absence of alignment). The mixed regime is the `best of both worlds': it converges from any random initialization (in contrast to balanced dynamics which require special initialization), and has a low rank bias (absent in the lazy dynamics). This allows us to prove an almost complete phase diagram of training behavior as a function of the variance at initialization and the width, for a MSE training task.",
        "published": "2024-10-31 04:00:00",
        "id": "a72ae38d-ad7e-40bc-a0c2-0a1dd83c1637",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一个统一公式描述线性网络学习矩阵的演化，存在混合机制，融合了懒惰和平衡机制的优点，可证明均方误差训练任务的训练行为相图。"
        },
        "tokens": 847
    },
    {
        "title": "Bias Detection Via Signaling",
        "link": "https://arxiv.org/abs/2405.17694",
        "description": "arXiv:2405.17694v2 Announce Type: replace \nAbstract: We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by information design. Specifically, we measure an agent's bias by designing a signaling scheme and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.",
        "published": "2024-10-31 04:00:00",
        "id": "d8baeee4-3844-405c-b628-73f7af84a939",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章介绍并研究检测主体信念更新是否存在偏差的问题，通过设计信号方案并观察主体行动来测量偏差，给出单信号足够的场景特征和计算最优信号方案的算法。"
        },
        "tokens": 763
    },
    {
        "title": "Enabling Generative Design Tools with LLM Agents for Mechanical Computation Devices: A Case Study",
        "link": "https://arxiv.org/abs/2405.17837",
        "description": "arXiv:2405.17837v3 Announce Type: replace \nAbstract: In the field of Human-Computer Interaction (HCI), interactive devices with embedded mechanical computation are gaining attention. The rise of these cutting-edge devices has created a need for specialized design tools that democratize the prototyping process. While current tools streamline prototyping through parametric design and simulation, they often come with a steep learning curve and may not fully support creative ideation. In this study, we use fluidic computation interfaces as a case study to explore how design tools for such devices can be augmented by Large Language Model agents (LLMs). Integrated with LLMs, the Generative Design Tool (GDT) better understands the capabilities and limitations of new technologies, proposes diverse and practical applications, and suggests designs that are technically and contextually appropriate. Additionally, it generates design parameters for visualizing results and producing fabrication-ready support files. This paper details the GDT's framework, implementation, and performance while addressing its potential and challenges.",
        "published": "2024-10-31 04:00:00",
        "id": "a8395262-af08-43ce-875b-bba052e35359",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在人机交互领域，以流体计算接口为例研究将大型语言模型代理用于增强机械计算设备的生成设计工具，包括其框架、实现、性能、潜力与挑战等。"
        },
        "tokens": 793
    },
    {
        "title": "$C^2M^3$: Cycle-Consistent Multi-Model Merging",
        "link": "https://arxiv.org/abs/2405.17897",
        "description": "arXiv:2405.17897v2 Announce Type: replace \nAbstract: In this paper, we present a novel data-free method for merging neural networks in weight space. Differently from most existing works, our method optimizes for the permutations of network neurons globally across all layers. This allows us to enforce cycle consistency of the permutations when merging $N \\geq 3$ models, allowing circular compositions of permutations to be computed without accumulating error along the path. We qualitatively and quantitatively motivate the need for such a constraint, showing its benefits when merging sets of models in scenarios spanning varying architectures and datasets. We finally show that, when coupled with activation renormalization, our approach yields the best results in the task.",
        "published": "2024-10-31 04:00:00",
        "id": "dd089fd9-0dd1-4334-ab8a-6375f88a9c49",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种新的数据无关的神经网络权重空间合并方法，优化网络神经元的全层排列，通过循环一致性约束在多种场景下合并模型，结合激活重归一化可在任务中取得最佳结果。"
        },
        "tokens": 741
    },
    {
        "title": "Adam with model exponential moving average is effective for nonconvex optimization",
        "link": "https://arxiv.org/abs/2405.18199",
        "description": "arXiv:2405.18199v2 Announce Type: replace \nAbstract: In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements -- momentum and discounting factors -- as well as model EMA, motivating their wide applications in practice.",
        "published": "2024-10-31 04:00:00",
        "id": "77ac0148-57fe-4e2b-b217-ca0c5d826db7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "理论分析表明，带有模型指数移动平均（EMA）的Adam的裁剪版本在多种非凸优化设置中能达到最优收敛率，且Adam的坐标适应性具有优势。"
        },
        "tokens": 740
    },
    {
        "title": "Continuous Product Graph Neural Networks",
        "link": "https://arxiv.org/abs/2405.18877",
        "description": "arXiv:2405.18877v2 Announce Type: replace \nAbstract: Processing multidomain data defined on multiple graphs holds significant potential in various practical applications in computer science. However, current methods are mostly limited to discrete graph filtering operations. Tensorial partial differential equations on graphs (TPDEGs) provide a principled framework for modeling structured data across multiple interacting graphs, addressing the limitations of the existing discrete methodologies. In this paper, we introduce Continuous Product Graph Neural Networks (CITRUS) that emerge as a natural solution to the TPDEG. CITRUS leverages the separability of continuous heat kernels from Cartesian graph products to efficiently implement graph spectral decomposition. We conduct thorough theoretical analyses of the stability and over-smoothing properties of CITRUS in response to domain-specific graph perturbations and graph spectra effects on the performance. We evaluate CITRUS on well-known traffic and weather spatiotemporal forecasting datasets, demonstrating superior performance over existing approaches. The implementation codes are available at https://github.com/ArefEinizade2/CITRUS.",
        "published": "2024-10-31 04:00:00",
        "id": "44fb7b63-3b6c-442f-84c2-87b823cb2ff4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了连续乘积图神经网络（CITRUS）这一解决图上张量偏微分方程（TPDEG）的自然方案，分析其稳定性和过平滑特性，在交通和天气时空预测数据集上评估，代码已开源。"
        },
        "tokens": 809
    },
    {
        "title": "Neural Isometries: Taming Transformations for Equivariant ML",
        "link": "https://arxiv.org/abs/2405.19296",
        "description": "arXiv:2405.19296v2 Announce Type: replace \nAbstract: Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.",
        "published": "2024-10-31 04:00:00",
        "id": "26d03850-870d-41bc-adaf-92bb29533856",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "2024年的论文介绍Neural Isometries自动编码器框架，用于处理现实世界几何和3D视觉任务中的对称问题，可作为自监督表示学习的有效骨干，其等距映射能直接从场景相邻视图编码间的映射系数回归相机姿态。"
        },
        "tokens": 842
    },
    {
        "title": "MemControl: Mitigating Memorization in Diffusion Models via Automated Parameter Selection",
        "link": "https://arxiv.org/abs/2405.19458",
        "description": "arXiv:2405.19458v2 Announce Type: replace \nAbstract: Diffusion models excel in generating images that closely resemble their training data but are also susceptible to data memorization, raising privacy, ethical, and legal concerns, particularly in sensitive domains such as medical imaging. We hypothesize that this memorization stems from the overparameterization of deep models and propose that regularizing model capacity during fine-tuning can mitigate this issue. Firstly, we empirically show that regulating the model capacity via Parameter-efficient fine-tuning (PEFT) mitigates memorization to some extent, however, it further requires the identification of the exact parameter subsets to be fine-tuned for high-quality generation. To identify these subsets, we introduce a bi-level optimization framework, MemControl, that automates parameter selection using memorization and generation quality metrics as rewards during fine-tuning. The parameter subsets discovered through MemControl achieve a superior tradeoff between generation quality and memorization. For the task of medical image generation, our approach outperforms existing state-of-the-art memorization mitigation strategies by fine-tuning as few as 0.019% of model parameters. Moreover, we demonstrate that the discovered parameter subsets are transferable to non-medical domains. Our framework is scalable to large datasets, agnostic to reward functions, and can be integrated with existing approaches for further memorization mitigation. To the best of our knowledge, this is the first study to empirically evaluate memorization in medical images and propose a targeted yet universal mitigation strategy. The code is available at https://github.com/Raman1121/Diffusion_Memorization_HPO",
        "published": "2024-10-31 04:00:00",
        "id": "b7b43795-b844-4759-96d8-0b18d9cd677e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究假设扩散模型中的记忆源于深度模型的过度参数化，提出MemControl双级优化框架，通过自动选择参数减轻记忆，在医学图像生成任务中表现出色且参数子集可转移到非医学领域。"
        },
        "tokens": 931
    },
    {
        "title": "Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases",
        "link": "https://arxiv.org/abs/2405.19581",
        "description": "arXiv:2405.19581v2 Announce Type: replace \nAbstract: Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.",
        "published": "2024-10-31 04:00:00",
        "id": "b128fa01-edd5-40f0-8304-fb9e50f344d3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种新的探测与恢复框架用于二进制分析，该框架利用源代 码基础模型中的预训练知识合成代码片段辅助黑盒语言模型提升恢复精度，在二进制摘要和函数名恢复上有显著提升。"
        },
        "tokens": 932
    },
    {
        "title": "SWIFT: A Monotonic, Flux-Form Semi-Lagrangian Tracer Transport Scheme for Flow with Large Courant Numbers",
        "link": "https://arxiv.org/abs/2405.20006",
        "description": "arXiv:2405.20006v3 Announce Type: replace \nAbstract: Local conservation of mass and entropy are becoming increasingly desirable properties for modern numerical weather and climate models. This work presents a Flux-Form Semi-Lagrangian (FFSL) transport scheme, called SWIFT, that facilitates this conservation for tracer variables, whilst maintaining other vital properties such as preservation of a constant, monotonicity and positivity. Importantly, these properties all hold for large Courant numbers and multi-dimensional flow, making the scheme appropriate for use within a dynamical core which takes large time steps.\n  The SWIFT scheme presented here can be seen as an evolution of the FFSL methods of Leonard et al and Lin and Rood. Two-dimensional and three-dimensional schemes consist of a splitting into a sequence of one-dimensional calculations. The new SWIFT splitting presented here allows monotonic and positivity properties from the one-dimensional calculations to be inherited by the multi-dimensional scheme. These one-dimensional calculations involve separating the mass flux into terms that correspond to integer and fractional parts of the Courant number. Key to achieving conservation is coupling the transport of tracers to the transport of the fluid density, through re-use of the discrete mass flux that was calculated from the fluid density in the transport of the tracers. This work also describes how these properties can still be attained when the tracer is vertically-staggered from the density in a Charney-Phillips grid.",
        "published": "2024-10-31 04:00:00",
        "id": "bf5282b3-56e2-4adc-8f64-03a9554fe0ca",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "论文提出名为SWIFT的通量形式半拉格朗日（FFSL）输运方案，能在大库朗数和多维流情况下保持质量和熵的局部守恒、单调性、正定性等性质，还描述了在查尼 - 菲利普斯网格中示踪剂与密度垂直交错时仍能实现这些性质的方法。"
        },
        "tokens": 918
    },
    {
        "title": "Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations",
        "link": "https://arxiv.org/abs/2405.20082",
        "description": "arXiv:2405.20082v3 Announce Type: replace \nAbstract: Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-and-play neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68\\%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries.",
        "published": "2024-10-31 04:00:00",
        "id": "3545065d-9b26-4b02-b920-86134f20abb2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为Segment, Shuffle, and Stitch (S3)的神经网络层，通过特定操作改善时间序列模型中的表征学习，经多数据集和基线实验证明其可提升时间序列分类、预测和异常检测任务性能并使学习更稳定，代码已开源。"
        },
        "tokens": 936
    },
    {
        "title": "OpenDAS: Open-Vocabulary Domain Adaptation for 2D and 3D Segmentation",
        "link": "https://arxiv.org/abs/2405.20141",
        "description": "arXiv:2405.20141v4 Announce Type: replace \nAbstract: Recently, Vision-Language Models (VLMs) have advanced segmentation techniques by shifting from the traditional segmentation of a closed-set of predefined object classes to open-vocabulary segmentation (OVS), allowing users to segment novel classes and concepts unseen during training of the segmentation model. However, this flexibility comes with a trade-off: fully-supervised closed-set methods still outperform OVS methods on base classes, that is on classes on which they have been explicitly trained. This is due to the lack of pixel-aligned training masks for VLMs (which are trained on image-caption pairs), and the absence of domain-specific knowledge, such as autonomous driving. Therefore, we propose the task of open-vocabulary domain adaptation to infuse domain-specific knowledge into VLMs while preserving their open-vocabulary nature. By doing so, we achieve improved performance in base and novel classes. Existing VLM adaptation methods improve performance on base (training) queries, but fail to fully preserve the open-set capabilities of VLMs on novel queries. To address this shortcoming, we combine parameter-efficient prompt tuning with a triplet-loss-based training strategy that uses auxiliary negative queries. Notably, our approach is the only parameter-efficient method that consistently surpasses the original VLM on novel classes. Our adapted VLMs can seamlessly be integrated into existing OVS pipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D segmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for open-vocabulary 3D instance segmentation without other changes. The project page is available at https://open-das.github.io/.",
        "published": "2024-10-31 04:00:00",
        "id": "25c3ae6f-dae3-41dd-ae01-c567357a8820",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出Open - Vocabulary Domain Adaptation任务，结合参数高效的提示调整与基于三元组损失的训练策略，可提升基础和新类别中的性能并保留开放词汇性质，能集成到现有OVS管道以提升分割效果。"
        },
        "tokens": 963
    },
    {
        "title": "Slight Corruption in Pre-training Data Makes Better Diffusion Models",
        "link": "https://arxiv.org/abs/2405.20494",
        "description": "arXiv:2405.20494v2 Announce Type: replace \nAbstract: Diffusion models (DMs) have shown remarkable capabilities in generating realistic high-quality images, audios, and videos. They benefit significantly from extensive pre-training on large-scale datasets, including web-crawled data with paired data and conditions, such as image-text and image-class pairs. Despite rigorous filtering, these pre-training datasets often inevitably contain corrupted pairs where conditions do not accurately describe the data. This paper presents the first comprehensive study on the impact of such corruption in pre-training data of DMs. We synthetically corrupt ImageNet-1K and CC3M to pre-train and evaluate over 50 conditional DMs. Our empirical findings reveal that various types of slight corruption in pre-training can significantly enhance the quality, diversity, and fidelity of the generated images across different DMs, both during pre-training and downstream adaptation stages. Theoretically, we consider a Gaussian mixture model and prove that slight corruption in the condition leads to higher entropy and a reduced 2-Wasserstein distance to the ground truth of the data distribution generated by the corruptly trained DMs. Inspired by our analysis, we propose a simple method to improve the training of DMs on practical datasets by adding condition embedding perturbations (CEP). CEP significantly improves the performance of various DMs in both pre-training and downstream tasks. We hope that our study provides new insights into understanding the data and pre-training processes of DMs and all models are released at https://huggingface.co/DiffusionNoise.",
        "published": "2024-10-31 04:00:00",
        "id": "d937a596-3dd7-4a5c-9390-a07d6bc71379",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现扩散模型预训练数据中的轻微损坏能提升生成图像的质量、多样性和保真度，理论上证明了条件中的轻微损坏有益，还提出添加条件嵌入扰动来改进扩散模型训练。"
        },
        "tokens": 905
    },
    {
        "title": "Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure",
        "link": "https://arxiv.org/abs/2405.20671",
        "description": "arXiv:2405.20671v2 Announce Type: replace \nAbstract: Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more \"relevant\" tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, our models trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as Nx2 multiplication and a two-dimensional task.",
        "published": "2024-10-31 04:00:00",
        "id": "1ea4eff2-afd2-4894-9f8d-19d0546986dd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出位置耦合方法，将任务结构直接嵌入到（仅解码器的）Transformer的位置编码中，以改善其在算术任务中的长度泛化能力，在整数加法等任务上进行了实证和理论验证且可应用于其他算法任务。"
        },
        "tokens": 847
    },
    {
        "title": "Improving Generalization and Convergence by Enhancing Implicit Regularization",
        "link": "https://arxiv.org/abs/2405.20763",
        "description": "arXiv:2405.20763v3 Announce Type: replace \nAbstract: In this work, we propose an Implicit Regularization Enhancement (IRE) framework to accelerate the discovery of flat solutions in deep learning, thereby improving generalization and convergence. Specifically, IRE decouples the dynamics of flat and sharp directions, which boosts the sharpness reduction along flat directions while maintaining the training stability in sharp directions. We show that IRE can be practically incorporated with {\\em generic base optimizers} without introducing significant computational overload. Experiments show that IRE consistently improves the generalization performance for image classification tasks across a variety of benchmark datasets (CIFAR-10/100, ImageNet) and models (ResNets and ViTs). Surprisingly, IRE also achieves a $2\\times$ {\\em speed-up} compared to AdamW in the pre-training of Llama models (of sizes ranging from 60M to 229M) on datasets including Wikitext-103, Minipile, and Openwebtext. Moreover, we provide theoretical guarantees, showing that IRE can substantially accelerate the convergence towards flat minima in Sharpness-aware Minimization (SAM).",
        "published": "2024-10-31 04:00:00",
        "id": "4e7300f9-2d2b-493e-8772-d282d0334375",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出隐式正则化增强（IRE）框架以加速深度学习中平坦解的发现，提升泛化和收敛性，可与通用基础优化器结合，在多个图像分类任务、Llama模型预训练中改善性能且有加速效果，还有理论保证在锐度感知最小化（SAM）中加速收敛。"
        },
        "tokens": 865
    },
    {
        "title": "einspace: Searching for Neural Architectures from Fundamental Operations",
        "link": "https://arxiv.org/abs/2405.20838",
        "description": "arXiv:2405.20838v2 Announce Type: replace \nAbstract: Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren't diverse enough to include such transformations a priori. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce einspace, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.",
        "published": "2024-10-31 04:00:00",
        "id": "df9b02c7-8bbb-4bd5-8c09-adf222ea407b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了基于参数化概率无上下文语法的einspace搜索空间，用于神经架构搜索，实验表明该搜索空间可找到新架构并改进现有架构。"
        },
        "tokens": 839
    },
    {
        "title": "Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters",
        "link": "https://arxiv.org/abs/2406.01249",
        "description": "arXiv:2406.01249v2 Announce Type: replace \nAbstract: Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. In this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. Therefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. Notably, such symmetries operate in the signal space rather than directly in the spatial space. We remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. The proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. We demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.",
        "published": "2024-10-31 04:00:00",
        "id": "f0ffeccd-2519-4ed8-9971-d397ef486452",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "本文聚焦于将图像卷积网络中的平移等变性扩展到一般图，提出对图功能平移完全等变的非线性光谱滤波器（NLSFs），其有通用逼近特性且基于可在图间转移的新光谱域形式，在节点和图分类基准测试中性能优于现有光谱图神经网络。"
        },
        "tokens": 851
    },
    {
        "title": "Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses",
        "link": "https://arxiv.org/abs/2406.01288",
        "description": "arXiv:2406.01288v2 Announce Type: replace \nAbstract: Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For examples, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.",
        "published": "2024-10-31 04:00:00",
        "id": "97bfdd4a-2b89-4bdf-921d-8619322e8f8b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出改进的少样本越狱技术，可有效突破对齐语言模型及其防御，在多种模型上实现高成功率，代码已开源。"
        },
        "tokens": 847
    },
    {
        "title": "REvolve: Reward Evolution with Large Language Models using Human Feedback",
        "link": "https://arxiv.org/abs/2406.01309",
        "description": "arXiv:2406.01309v2 Announce Type: replace \nAbstract: Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have been used for reward generation from natural language task descriptions, leveraging their extensive instruction tuning and commonsense understanding of human behavior. In this work, we hypothesize that LLMs, guided by human feedback, can be used to formulate reward functions that reflect human implicit knowledge. We study this in three challenging settings -- autonomous driving, humanoid locomotion, and dexterous manipulation -- wherein notions of ``good\" behavior are tacit and hard to quantify. To this end, we introduce REvolve, a truly evolutionary framework that uses LLMs for reward design in RL. REvolve generates and refines reward functions by utilizing human feedback to guide the evolution process, effectively translating implicit human knowledge into explicit reward functions for training (deep) RL agents. Experimentally, we demonstrate that agents trained on REvolve-designed rewards outperform other state-of-the-art baselines.",
        "published": "2024-10-31 04:00:00",
        "id": "39e2c288-d86d-402e-ab2c-be0b4dcf54b8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "REvolve是一个利用人类反馈引导进化过程生成和优化奖励函数的框架，通过大语言模型将人类隐性知识转化为显式奖励函数以训练强化学习代理，经实验在三个挑战性场景中基于该框架设计奖励训练的代理性能优于其他基准。"
        },
        "tokens": 849
    },
    {
        "title": "TE-NeXt: A LiDAR-Based 3D Sparse Convolutional Network for Traversability Estimation",
        "link": "https://arxiv.org/abs/2406.01395",
        "description": "arXiv:2406.01395v2 Announce Type: replace \nAbstract: This paper presents TE-NeXt, a novel and efficient architecture for Traversability Estimation (TE) from sparse LiDAR point clouds based on a residual convolution block. TE-NeXt block fuses notions of current trends such as attention mechanisms and 3D sparse convolutions. TE-NeXt aims to demonstrate high capacity for generalisation in a variety of urban and natural environments, using well-known and accessible datasets such as SemanticKITTI, Rellis-3D and SemanticUSL. Thus, the designed architecture ouperforms state-of-the-art methods in the problem of semantic segmentation, demonstrating better results in unstructured environments and maintaining high reliability and robustness in urbans environments, which leads to better abstraction. Implementation is available in a open repository to the scientific community with the aim of ensuring the reproducibility of results.",
        "published": "2024-10-31 04:00:00",
        "id": "9b5d8d01-af93-434f-ad21-42fcd39a591c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出基于残差卷积块的TE-NeXt架构，融合注意力机制和3D稀疏卷积用于从稀疏LiDAR点云进行可通行性估计，在语义分割问题上优于现有方法且代码开源。"
        },
        "tokens": 793
    },
    {
        "title": "Charting the Landscape of Nefarious Uses of Generative Artificial Intelligence for Online Election Interference",
        "link": "https://arxiv.org/abs/2406.01862",
        "description": "arXiv:2406.01862v4 Announce Type: replace \nAbstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) pose significant risks, particularly in the realm of online election interference. This paper explores the nefarious applications of GenAI, highlighting their potential to disrupt democratic processes through deepfakes, botnets, targeted misinformation campaigns, and synthetic identities. By examining recent case studies and public incidents, we illustrate how malicious actors exploit these technologies to try influencing voter behavior, spread disinformation, and undermine public trust in electoral systems. The paper also discusses the societal implications of these threats, emphasizing the urgent need for robust mitigation strategies and international cooperation to safeguard democratic integrity.",
        "published": "2024-10-31 04:00:00",
        "id": "cc8e1f3b-9f72-4aa1-9b53-cf1e396ac991",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨生成式人工智能在网络选举干扰中的恶意应用，通过案例说明恶意行为者如何利用该技术影响选民行为、传播虚假信息，强调需要应对策略与国际合作来保护民主完整性。"
        },
        "tokens": 737
    },
    {
        "title": "Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections",
        "link": "https://arxiv.org/abs/2406.03052",
        "description": "arXiv:2406.03052v2 Announce Type: replace \nAbstract: Despite the remarkable capabilities demonstrated by Graph Neural Networks (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks. However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality. To this end, we introduce a Node Injection-based Fairness Attack (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting. In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes' feature matrix to further ensure the effectiveness of fairness attacks. Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes. We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms. Our code and data are released at: https://github.com/CGCL-codes/NIFA.",
        "published": "2024-10-31 04:00:00",
        "id": "56eec4e7-d921-4c74-a7af-e58bb2740fe5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍了一种基于节点注入的公平性攻击（NIFA），在更现实的设置下探索图神经网络（GNN）公平性的漏洞，实验表明NIFA能显著破坏主流GNN的公平性，代码和数据已公开。"
        },
        "tokens": 842
    },
    {
        "title": "CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models",
        "link": "https://arxiv.org/abs/2406.06007",
        "description": "arXiv:2406.06007v2 Announce Type: replace \nAbstract: Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://cares-ai.github.io/.",
        "published": "2024-10-31 04:00:00",
        "id": "5380b365-accf-4987-872b-16998abd8080",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍CARES以全面评估医学大视觉语言模型在医疗领域的可信度，包含五个维度，其基准含约41K问答对，分析发现模型存在可信度问题，已公开基准和代码。"
        },
        "tokens": 820
    },
    {
        "title": "Certified Robustness to Data Poisoning in Gradient-Based Training",
        "link": "https://arxiv.org/abs/2406.05670",
        "description": "arXiv:2406.05670v2 Announce Type: replace \nAbstract: Modern machine learning pipelines leverage large amounts of public data, making it infeasible to guarantee data quality and leaving models open to poisoning and backdoor attacks. Provably bounding model behavior under such attacks remains an open problem. In this work, we address this challenge by developing the first framework providing provable guarantees on the behavior of models trained with potentially manipulated data without modifying the model or learning algorithm. In particular, our framework certifies robustness against untargeted and targeted poisoning, as well as backdoor attacks, for bounded and unbounded manipulations of the training inputs and labels. Our method leverages convex relaxations to over-approximate the set of all possible parameter updates for a given poisoning threat model, allowing us to bound the set of all reachable parameters for any gradient-based learning algorithm. Given this set of parameters, we provide bounds on worst-case behavior, including model performance and backdoor success rate. We demonstrate our approach on multiple real-world datasets from applications including energy consumption, medical imaging, and autonomous driving.",
        "published": "2024-10-31 04:00:00",
        "id": "4288e5d9-8842-41de-ba77-2a714fd2da5a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出首个框架，能在不修改模型或学习算法的情况下，为用可能被操纵数据训练的模型的行为提供可证明的保证，可抵御多种攻击，在多个现实世界数据集上进行了演示。"
        },
        "tokens": 816
    },
    {
        "title": "TLCM: Training-efficient Latent Consistency Model for Image Generation with 2-8 Steps",
        "link": "https://arxiv.org/abs/2406.05768",
        "description": "arXiv:2406.05768v4 Announce Type: replace \nAbstract: Distilling latent diffusion models (LDMs) into ones that are fast to sample from is attracting growing research interest. However, the majority of existing methods face two critical challenges: (1) They hinge on long training using a huge volume of real data. (2) They routinely lead to quality degradation for generation, especially in text-image alignment. This paper proposes a novel training-efficient Latent Consistency Model (TLCM) to overcome these challenges. Our method first accelerates LDMs via data-free multistep latent consistency distillation (MLCD), and then data-free latent consistency distillation is proposed to efficiently guarantee the inter-segment consistency in MLCD. Furthermore, we introduce bags of techniques, e.g., distribution matching, adversarial learning, and preference learning, to enhance TLCM's performance at few-step inference without any real data. TLCM demonstrates a high level of flexibility by enabling adjustment of sampling steps within the range of 2 to 8 while still producing competitive outputs compared to full-step approaches. Notably, TLCM enjoys the data-free merit by employing synthetic data from the teacher for distillation. With just 70 training hours on an A100 GPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of 33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark, surpassing various accelerated models and even outperforming the teacher model in human preference metrics. We also demonstrate the versatility of TLCMs in applications including image style transfer, controllable generation, and Chinese-to-image generation.",
        "published": "2024-10-31 04:00:00",
        "id": "dddb6bce-35d3-4654-bf9a-4fdd45b3a03a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种训练高效的潜在一致性模型TLCM用于图像生成，可2 - 8步生成，克服现有方法挑战且无需真实数据，3步的TLCM在基准测试上表现优异，还展示了在多种应用中的通用性。"
        },
        "tokens": 953
    },
    {
        "title": "VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction",
        "link": "https://arxiv.org/abs/2406.05774",
        "description": "arXiv:2406.05774v2 Announce Type: replace \nAbstract: Although 3D Gaussian Splatting has been widely studied because of its realistic and efficient novel-view synthesis, it is still challenging to extract a high-quality surface from the point-based representation. Previous works improve the surface by incorporating geometric priors from the off-the-shelf normal estimator. However, there are two main limitations: 1) Supervising normals rendered from 3D Gaussians effectively updates the rotation parameter but is less effective for other geometric parameters; 2) The inconsistency of predicted normal maps across multiple views may lead to severe reconstruction artifacts. In this paper, we propose a Depth-Normal regularizer that directly couples normal with other geometric parameters, leading to full updates of the geometric parameters from normal regularization. We further propose a confidence term to mitigate inconsistencies of normal predictions across multiple views. Moreover, we also introduce a densification and splitting strategy to regularize the size and distribution of 3D Gaussians for more accurate surface modeling. Compared with Gaussian-based baselines, experiments show that our approach obtains better reconstruction quality and maintains competitive appearance quality at faster training speed and 100+ FPS rendering.",
        "published": "2024-10-31 04:00:00",
        "id": "776ce07f-2271-4deb-b122-732ee2f3ec64",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出VCR - GauS，通过深度 - 法线正则化器解决3D高斯表面重建中的问题，提升重建质量并保持外观质量，训练速度快且渲染帧率高。"
        },
        "tokens": 835
    },
    {
        "title": "Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning",
        "link": "https://arxiv.org/abs/2406.03519",
        "description": "arXiv:2406.03519v3 Announce Type: replace \nAbstract: High utility and rigorous data privacy are of the main goals of a federated learning (FL) system, which learns a model from the data distributed among some clients. The latter has been tried to achieve by using differential privacy in FL (DPFL). There is often heterogeneity in clients privacy requirements, and existing DPFL works either assume uniform privacy requirements for clients or are not applicable when server is not fully trusted (our setting). Furthermore, there is often heterogeneity in batch and/or dataset size of clients, which as shown, results in extra variation in the DP noise level across clients model updates. With these sources of heterogeneity, straightforward aggregation strategies, e.g., assigning clients aggregation weights proportional to their privacy parameters will lead to lower utility. We propose Robust-HDP, which efficiently estimates the true noise level in clients model updates and reduces the noise-level in the aggregated model updates considerably. Robust-HDP improves utility and convergence speed, while being safe to the clients that may maliciously send falsified privacy parameter to server. Extensive experimental results on multiple datasets and our theoretical analysis confirm the effectiveness of Robust-HDP. Our code can be found here.",
        "published": "2024-10-31 04:00:00",
        "id": "9eb75520-2bf3-41de-977d-6ee6c2f3090f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对联邦学习中的异质性和隐私要求，提出Robust - HDP算法，能有效估计客户端模型更新中的真实噪声水平并提高效用和收敛速度，实验和理论分析证实其有效性。"
        },
        "tokens": 836
    },
    {
        "title": "Time-MMD: A New Multi-Domain Multimodal Dataset for Time Series Analysis",
        "link": "https://arxiv.org/abs/2406.08627",
        "description": "arXiv:2406.08627v2 Announce Type: replace \nAbstract: Time series data are ubiquitous across a wide range of real-world domains. While real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first multi-domain, multimodal time series dataset covering 9 primary data domains. Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the first multimodal time-series forecasting (TSF) library, seamlessly pipelining multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality, evidenced by over 15% mean squared error reduction in general, and up to 40% in domains with rich textual data. More importantly, our datasets and library revolutionize broader applications, impacts, research topics to advance TSA. The dataset and library are available at https://github.com/AdityaLab/Time-MMD and https://github.com/AdityaLab/MM-TSFlib.",
        "published": "2024-10-31 04:00:00",
        "id": "b4514d7d-e3e4-4ece-bc2e-a155be20e6f8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "介绍了Time - MMD首个多域多模态时间序列数据集涵盖9个主要数据域，还开发了MM - TSFlib多模态时间序列预测库，通过MM - TSFlib在Time - MMD上的实验表明多模态对时间序列分析性能有显著提升。"
        },
        "tokens": 914
    },
    {
        "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
        "link": "https://arxiv.org/abs/2406.07476",
        "description": "arXiv:2406.07476v3 Announce Type: replace \nAbstract: In this paper, we present the VideoLLaMA 2, a set of Video Large Language Models (Video-LLMs) designed to enhance spatial-temporal modeling and audio understanding in video and audio-oriented tasks. Building upon its predecessor, VideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC) connector, which effectively captures the intricate spatial and temporal dynamics of video data. Additionally, we integrate an Audio Branch into the model through joint training, thereby enriching the multimodal understanding capabilities of the model by seamlessly incorporating audio cues. Comprehensive evaluations on multiple-choice video question answering (MC-VQA), open-ended video question answering (OE-VQA), and video captioning (VC) tasks demonstrate that VideoLLaMA 2 consistently achieves competitive results among open-source models and even gets close to some proprietary models on several benchmarks. Furthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and audio-video question-answering (AQA & OE-AVQA) benchmarks over existing models. These advancements underline VideoLLaMA 2's superior performance in multimodal comprehension, setting a new standard for intelligent video analysis systems. All models are public to facilitate further research.",
        "published": "2024-10-31 04:00:00",
        "id": "233368c9-a131-4b20-95b2-bb1e18f3d07a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "VideoLLaMA 2旨在提升视频和音频任务中的时空建模与音频理解能力，通过特定的时空卷积连接器与音频分支集成提升多模态理解能力，在多项任务评估中取得较好成果且模型已公开。"
        },
        "tokens": 866
    },
    {
        "title": "Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models",
        "link": "https://arxiv.org/abs/2406.08384",
        "description": "arXiv:2406.08384v2 Announce Type: replace \nAbstract: Recent advancements in deep generative models present new opportunities for music production but also pose challenges, such as high computational demands and limited audio quality. Moreover, current systems frequently rely solely on text input and typically focus on producing complete musical pieces, which is incompatible with existing workflows in music production. To address these issues, we introduce \"Diff-A-Riff,\" a Latent Diffusion Model designed to generate high-quality instrumental accompaniments adaptable to any musical context. This model offers control through either audio references, text prompts, or both, and produces 48kHz pseudo-stereo audio while significantly reducing inference time and memory usage. We demonstrate the model's capabilities through objective metrics and subjective listening tests, with extensive examples available on the accompanying website: sonycslparis.github.io/diffariff-companion/",
        "published": "2024-10-31 04:00:00",
        "id": "099d316e-833a-44d8-bb9a-e3656931389c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "Diff - A - Riff是一个潜在扩散模型，可通过音频参考、文本提示或两者结合来生成高质量乐器伴奏，降低推理时间和内存使用，其能力已通过客观指标和主观测试得到证明。"
        },
        "tokens": 780
    },
    {
        "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
        "link": "https://arxiv.org/abs/2406.08466",
        "description": "arXiv:2406.08466v2 Announce Type: replace \nAbstract: Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.\n  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a>1$, we show that the reducible part of the test error is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.",
        "published": "2024-10-31 04:00:00",
        "id": "89483306-848c-416b-9eb8-485376524619",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究无限维线性回归中的缩放定律理论，假设最优参数满足高斯先验且数据协方差矩阵有幂律谱，通过单步随机梯度下降训练模型，得出测试误差可约部分的表达式，理论与经验神经缩放定律一致并经数值模拟验证。"
        },
        "tokens": 871
    },
    {
        "title": "Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text",
        "link": "https://arxiv.org/abs/2406.06056",
        "description": "arXiv:2406.06056v2 Announce Type: replace \nAbstract: Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text. Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets. However, existing SBDH datasets exhibit substantial limitations in their availability and coverage. In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories. We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities. Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 63.75% macro-F improvements. Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints while being substantially cheaper than expert-annotated real-world data. Human evaluation reveals a 71.06% Human-LLM alignment and uncovers areas for future refinements.",
        "published": "2024-10-31 04:00:00",
        "id": "e4e4446e-e823-48c2-bdb8-653436b0c832",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Synth - SBDH这一具有详细SBDH注释的合成数据集，在三个任务上展示其效用，模型经其训练后表现优于未用其训练的模型，且成本低于专家注释的真实世界数据，人类评估揭示了人类 - LLM一致性并指出改进方向。"
        },
        "tokens": 865
    },
    {
        "title": "Adaptive Randomized Smoothing: Certified Adversarial Robustness for Multi-Step Defences",
        "link": "https://arxiv.org/abs/2406.10427",
        "description": "arXiv:2406.10427v2 Announce Type: replace \nAbstract: We propose Adaptive Randomized Smoothing (ARS) to certify the predictions of our test-time adaptive models against adversarial examples. ARS extends the analysis of randomized smoothing using $f$-Differential Privacy to certify the adaptive composition of multiple steps. For the first time, our theory covers the sound adaptive composition of general and high-dimensional functions of noisy inputs. We instantiate ARS on deep image classification to certify predictions against adversarial examples of bounded $L_{\\infty}$ norm. In the $L_{\\infty}$ threat model, ARS enables flexible adaptation through high-dimensional input-dependent masking. We design adaptivity benchmarks, based on CIFAR-10 and CelebA, and show that ARS improves standard test accuracy by $1$ to $15\\%$ points. On ImageNet, ARS improves certified test accuracy by up to $1.6\\%$ points over standard RS without adaptivity. Our code is available at https://github.com/ubc-systopia/adaptive-randomized-smoothing .",
        "published": "2024-10-31 04:00:00",
        "id": "fb9eb68d-a66a-470e-83eb-64d5c09fd93d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出自适应随机平滑（ARS）用于认证对抗样本的预测，扩展随机平滑分析以认证多步自适应组合，在深度图像分类上实例化ARS，设计适应基准并显示ARS可提高标准测试准确性。"
        },
        "tokens": 828
    },
    {
        "title": "Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure",
        "link": "https://arxiv.org/abs/2406.09023",
        "description": "arXiv:2406.09023v3 Announce Type: replace \nAbstract: Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning. While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach. The success of deep learning motivates the use of learning-based approaches to estimate SPD matrices with neural networks in a data-driven fashion. However, designing effective neural architectures for SPD learning is challenging, particularly when the task requires additional structural constraints, such as element-wise sparsity. Current approaches either do not ensure that the output meets all desired properties or lack expressivity. In this paper, we introduce SpodNet, a novel and generic learning module that guarantees SPD outputs and supports additional structural constraints. Notably, it solves the challenging task of learning jointly SPD and sparse matrices. Our experiments illustrate the versatility and relevance of SpodNet layers for such applications.",
        "published": "2024-10-31 04:00:00",
        "id": "e5105a2d-9d6f-4cce-b287-0207ebf45701",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "本文介绍了一种名为SpodNet的新型通用学习模块，它能保证对称正定（SPD）输出并支持额外结构约束，解决了联合学习SPD和稀疏矩阵的难题，展示了其在相关应用中的通用性和相关性。"
        },
        "tokens": 798
    },
    {
        "title": "Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation",
        "link": "https://arxiv.org/abs/2406.09068",
        "description": "arXiv:2406.09068v3 Announce Type: replace \nAbstract: Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.",
        "published": "2024-10-31 04:00:00",
        "id": "51f2eecd-f119-4b9b-b244-2e888de94e6c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该论文指出离线多智能体强化学习（MARL）研究受基线和评估协议不一致困扰，研究表明简单基线能在多数任务达最先进水平，还引入了标准化评估方法并提供基线实现及统计结果。"
        },
        "tokens": 894
    },
    {
        "title": "Scale Equivariant Graph Metanetworks",
        "link": "https://arxiv.org/abs/2406.10685",
        "description": "arXiv:2406.10685v2 Announce Type: replace \nAbstract: This paper pertains to an emerging machine learning paradigm: learning higher-order functions, i.e. functions whose inputs are functions themselves, $\\textit{particularly when these inputs are Neural Networks (NNs)}$. With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. $\\textit{However, are these the sole symmetries present in NN parameterizations}$? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as $\\textit{scaling symmetries}$, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose $\\textit{Scale Equivariant Graph MetaNetworks - ScaleGMNs}$, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.",
        "published": "2024-10-31 04:00:00",
        "id": "3ce8b50c-a4d9-48bf-a5c2-1cd76a579e8b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出Scale Equivariant Graph MetaNetworks（ScaleGMNs）框架，通过纳入缩放对称性来调整图元网络范式，在特定条件下可模拟输入前馈神经网络的正向和反向传递，实验表明该方法提升了多个数据集和激活函数的性能，代码已开源。"
        },
        "tokens": 969
    },
    {
        "title": "DenoiseRep: Denoising Model for Representation Learning",
        "link": "https://arxiv.org/abs/2406.08773",
        "description": "arXiv:2406.08773v2 Announce Type: replace \nAbstract: The denoising model has been proven a powerful generative model but has little exploration of discriminative tasks. Representation learning is important in discriminative tasks, which is defined as \"learning representations (or features) of the data that make it easier to extract useful information when building classifiers or other predictors\". In this paper, we propose a novel Denoising Model for Representation Learning (DenoiseRep) to improve feature discrimination with joint feature extraction and denoising. DenoiseRep views each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step. This unifies the frameworks of feature extraction and denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step. After that, DenoiseRep fuses the parameters of feature extraction and denoising layers, and theoretically demonstrates its equivalence before and after the fusion, thus making feature denoising computation-free. DenoiseRep is a label-free algorithm that incrementally improves features but also complementary to the label if available. Experimental results on various discriminative vision tasks, including re-identification (Market-1501, DukeMTMC-reID, MSMT17, CUHK-03, vehicleID), image classification (ImageNet, UB200, Oxford-Pet, Flowers), object detection (COCO), image segmentation (ADE20K) show stability and impressive improvements. We also validate its effectiveness on the CNN (ResNet) and Transformer (ViT, Swin, Vmamda) architectures.",
        "published": "2024-10-31 04:00:00",
        "id": "1cd681d7-c8d3-46a1-87fc-1edb4c4ab818",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新的用于表征学习的去噪模型DenoiseRep，通过联合特征提取和去噪提高特征判别能力，在多个视觉判别任务上显示出稳定性和显著改进，适用于多种架构。"
        },
        "tokens": 943
    },
    {
        "title": "Exploring Parent-Child Perceptions on Safety in Generative AI: Concerns, Mitigation Strategies, and Design Implications",
        "link": "https://arxiv.org/abs/2406.10461",
        "description": "arXiv:2406.10461v2 Announce Type: replace \nAbstract: The widespread use of Generative Artificial Intelligence (GAI) among teenagers has led to significant misuse and safety concerns. To identify risks and understand parental controls challenges, we conducted a content analysis on Reddit and interviewed 20 participants (seven teenagers and 13 parents). Our study reveals a significant gap in parental awareness of the extensive ways children use GAI, such as interacting with character-based chatbots for emotional support or engaging in virtual relationships. Parents and children report differing perceptions of risks associated with GAI. Parents primarily express concerns about data collection, misinformation, and exposure to inappropriate content. In contrast, teenagers are more concerned about becoming addicted to virtual relationships with GAI, the potential misuse of GAI to spread harmful content in social groups, and the invasion of privacy due to unauthorized use of their personal data in GAI applications. The absence of parental control features on GAI platforms forces parents to rely on system-built controls, manually check histories, share accounts, and engage in active mediation. Despite these efforts, parents struggle to grasp the full spectrum of GAI-related risks and to perform effective real-time monitoring, mediation, and education. We provide design recommendations to improve parent-child communication and enhance the safety of GAI use.",
        "published": "2024-10-31 04:00:00",
        "id": "83f773ee-c808-4e62-9986-d27722d5b958",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过对Reddit内容分析和对20名参与者的访谈，研究发现家长与青少年对生成式人工智能安全的认知存在差异，家长管控困难，还提出了改善亲子沟通和提升使用安全性的设计建议。"
        },
        "tokens": 870
    },
    {
        "title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training",
        "link": "https://arxiv.org/abs/2406.10670",
        "description": "arXiv:2406.10670v3 Announce Type: replace \nAbstract: Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n  Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4",
        "published": "2024-10-31 04:00:00",
        "id": "e7509456-3ce5-46e9-8aac-7c11e7995801",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种名为CoLoR - Filter的数据选择方法用于语言模型预训练，通过经验贝叶斯方法得出选择标准，经两个语言建模任务实证评估，该方法在数据选择上有良好扩展性。"
        },
        "tokens": 921
    },
    {
        "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
        "link": "https://arxiv.org/abs/2406.11813",
        "description": "arXiv:2406.11813v2 Announce Type: replace \nAbstract: Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.",
        "published": "2024-10-31 04:00:00",
        "id": "809af415-f4d1-4e19-b2f9-f91fe0a8f1aa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究大型语言模型预训练期间如何获取事实性知识，发现预训练数据量增加对获取和保持事实知识能力无显著提升、训练步骤与遗忘和泛化呈幂律关系、较大批量训练可增强抗遗忘能力等，并基于此对大型语言模型的一些行为做出解释。"
        },
        "tokens": 866
    },
    {
        "title": "BSRBF-KAN: A combination of B-splines and Radial Basis Functions in Kolmogorov-Arnold Networks",
        "link": "https://arxiv.org/abs/2406.11173",
        "description": "arXiv:2406.11173v5 Announce Type: replace \nAbstract: In this paper, we introduce BSRBF-KAN, a Kolmogorov Arnold Network (KAN) that combines B-splines and radial basis functions (RBFs) to fit input vectors during data training. We perform experiments with BSRBF-KAN, multi-layer perception (MLP), and other popular KANs, including EfficientKAN, FastKAN, FasterKAN, and GottliebKAN over the MNIST and Fashion-MNIST datasets. BSRBF-KAN shows stability in 5 training runs with a competitive average accuracy of 97.55% on MNIST and 89.33% on Fashion-MNIST and obtains convergence better than other networks. We expect BSRBF-KAN to open many combinations of mathematical functions to design KANs. Our repo is publicly available at: https://github.com/hoangthangta/BSRBF_KAN.",
        "published": "2024-10-31 04:00:00",
        "id": "91ad1e57-e5d0-4875-88d9-d0dddb6f47c5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "介绍BSRBF - KAN（一种结合B - 样条和径向基函数的Kolmogorov Arnold网络），在MNIST和Fashion - MNIST数据集上与其他网络进行实验，显示出稳定性且收敛性更好，代码库公开。"
        },
        "tokens": 822
    },
    {
        "title": "Unleashing the Potential of Open-set Noisy Samples Against Label Noise for Medical Image Classification",
        "link": "https://arxiv.org/abs/2406.12293",
        "description": "arXiv:2406.12293v2 Announce Type: replace \nAbstract: Addressing mixed closed-set and open-set label noise in medical image classification remains a largely unexplored challenge. Unlike natural image classification, which often separates and processes closed-set and open-set noisy samples from clean ones, medical image classification contends with high inter-class similarity, complicating the identification of open-set noisy samples. Additionally, existing methods often fail to fully utilize open-set noisy samples for label noise mitigation, leading to their exclusion or the application of uniform soft labels. To address these challenges, we propose the Extended Noise-robust Contrastive and Open-set Feature Augmentation framework for medical image classification tasks. This framework incorporates the Extended Noise-robust Supervised Contrastive Loss, which helps differentiate features among both in-distribution and out-of-distribution classes. This loss treats open-set noisy samples as an extended class, improving label noise mitigation by weighting contrastive pairs according to label reliability. Additionally, we develop the Open-set Feature Augmentation module that enriches open-set samples at the feature level and then assigns them dynamic class labels, thereby leveraging the model's capacity and reducing overfitting to noisy data. We evaluated the proposed framework on both a synthetic noisy dataset and a real-world noisy dataset. The results indicate the superiority of our framework over four existing methods and the effectiveness of leveraging open-set noisy samples to combat label noise.",
        "published": "2024-10-31 04:00:00",
        "id": "4abf7c5b-05e9-439e-bd18-43f3f5209b1b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出针对医学图像分类的扩展噪声鲁棒对比和开放集特征增强框架，包含扩展噪声鲁棒监督对比损失、开放集特征增强模块，经评估在处理混合闭集和开集标签噪声方面优于现有方法并能有效利用开集噪声样本对抗标签噪声。"
        },
        "tokens": 898
    },
    {
        "title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models",
        "link": "https://arxiv.org/abs/2406.11675",
        "description": "arXiv:2406.11675v4 Announce Type: replace \nAbstract: Large Language Models (LLMs) often suffer from overconfidence during inference, particularly when adapted to downstream domain-specific tasks with limited data. Previous work addresses this issue by employing approximate Bayesian estimation after the LLMs are trained, enabling them to quantify uncertainty. However, such post-training approaches' performance is severely limited by the parameters learned during training. In this paper, we go beyond post-training Bayesianization and propose Bayesian Low-Rank Adaptation by Backpropagation (BLoB), an algorithm that continuously and jointly adjusts both the mean and covariance of LLM parameters throughout the whole fine-tuning process. Our empirical results verify the effectiveness of BLoB in terms of generalization and uncertainty estimation, when evaluated on both in-distribution and out-of-distribution data.",
        "published": "2024-10-31 04:00:00",
        "id": "a1388e44-2261-4c1e-806d-ba4aad4f2b67",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出BLoB算法，在大型语言模型微调过程中持续联合调整参数均值和协方差，经验证其在泛化和不确定性估计方面有效。"
        },
        "tokens": 756
    },
    {
        "title": "On the Worst Prompt Performance of Large Language Models",
        "link": "https://arxiv.org/abs/2406.10248",
        "description": "arXiv:2406.10248v4 Announce Type: replace \nAbstract: The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts. Data and code are available at https://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "890cad28-7a81-4168-88a2-df6b4d1eda8e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍RobustAlpacaEval新基准，用它对ChatGPT和六个开源大语言模型实验，发现模型性能因提示不同差异大，难以确定最差提示，现有提升方法效果有限，强调需创建更具弹性的大语言模型。"
        },
        "tokens": 935
    },
    {
        "title": "The EarlyBird Gets the WORM: Heuristically Accelerating EarlyBird Convergence",
        "link": "https://arxiv.org/abs/2406.11872",
        "description": "arXiv:2406.11872v2 Announce Type: replace \nAbstract: The Lottery Ticket hypothesis proposes that ideal, sparse subnetworks, called lottery tickets, exist in untrained dense neural networks. The Early Bird hypothesis proposes an efficient algorithm to find these winning lottery tickets in convolutional neural networks, using the novel concept of distance between subnetworks to detect convergence in the subnetworks of a model. However, this approach overlooks unchanging groups of unimportant neurons near the search's end. We proposes WORM, a method that exploits these static groups by truncating their gradients, forcing the model to rely on other neurons. Experiments show WORM achieves faster ticket identification during training on convolutional neural networks, despite the additional computational overhead, when compared to EarlyBird search. Additionally, WORM-pruned models lose less accuracy during pruning and recover accuracy faster, improving the robustness of a given model. Furthermore, WORM is also able to generalize the Early Bird hypothesis reasonably well to larger models, such as transformers, displaying its flexibility to adapt to more complex architectures.",
        "published": "2024-10-31 04:00:00",
        "id": "1530c830-56bc-4cee-8080-1def30f86f1d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出WORM方法，可利用未被Early Bird假设算法重视的静态神经元组，实验表明在卷积神经网络训练时WORM比EarlyBird搜索更快识别有效子网络，且WORM修剪模型精度损失小、恢复快并能较好推广到更大模型。"
        },
        "tokens": 819
    },
    {
        "title": "QTIP: Quantization with Trellises and Incoherence Processing",
        "link": "https://arxiv.org/abs/2406.11235",
        "description": "arXiv:2406.11235v3 Announce Type: replace \nAbstract: Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes. Since LLM inference is usually memory-bound, PTQ methods can improve inference throughput. Recent state-of-the-art PTQ approaches use vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping. However, VQ requires a codebook with size exponential in the dimension. This limits current VQ-based PTQ works to low VQ dimensions ($\\le 8$) that in turn limit quantization quality. Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient \"bitshift\" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed.",
        "published": "2024-10-31 04:00:00",
        "id": "24ac7c6c-09d5-4a52-842a-2eb965a81074",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "QTIP使用格状编码量化（TCQ）实现超高维量化，其一系列格状代码在量化质量和推理速度上都达到了最先进的结果。"
        },
        "tokens": 811
    },
    {
        "title": "Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models",
        "link": "https://arxiv.org/abs/2406.12311",
        "description": "arXiv:2406.12311v2 Announce Type: replace \nAbstract: Binarization, which converts weight parameters to binary values, has emerged as an effective strategy to reduce the size of large language models (LLMs). However, typical binarization techniques significantly diminish linguistic effectiveness of LLMs. To address this issue, we introduce a novel binarization technique called Mixture of Scales (BinaryMoS). Unlike conventional methods, BinaryMoS employs multiple scaling experts for binary weights, dynamically merging these experts for each token to adaptively generate scaling factors. This token-adaptive approach boosts the representational power of binarized LLMs by enabling contextual adjustments to the values of binary weights. Moreover, because this adaptive process only involves the scaling factors rather than the entire weight matrix, BinaryMoS maintains compression efficiency similar to traditional static binarization methods. Our experimental results reveal that BinaryMoS surpasses conventional binarization techniques in various natural language processing tasks and even outperforms 2-bit quantization methods, all while maintaining similar model size to static binarization techniques.",
        "published": "2024-10-31 04:00:00",
        "id": "e7a380e4-ebc7-4b43-a9e6-4a73ad6b9eb9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种新的二值化技术Mixture of Scales (BinaryMoS)，通过采用多尺度专家和令牌自适应方法提升二值化大型语言模型的表征能力，在自然语言处理任务中表现优于传统二值化技术和2位量化方法且保持相似模型大小。"
        },
        "tokens": 834
    },
    {
        "title": "From Instance Training to Instruction Learning: Task Adapters Generation from Instructions",
        "link": "https://arxiv.org/abs/2406.12382",
        "description": "arXiv:2406.12382v2 Announce Type: replace \nAbstract: Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements.",
        "published": "2024-10-31 04:00:00",
        "id": "52fcf4fb-5d6d-4f1e-85d4-8b299bb7ccec",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍Task Adapters Generation from Instructions (TAGI)，它基于任务指令以参数生成方式构建特定任务模型，通过两阶段训练具备跨任务泛化能力，在相关数据集上评估表现良好。"
        },
        "tokens": 879
    },
    {
        "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors",
        "link": "https://arxiv.org/abs/2406.12459",
        "description": "arXiv:2406.12459v2 Announce Type: replace \nAbstract: Despite recent advancements in high-fidelity human reconstruction techniques, the requirements for densely captured images or time-consuming per-instance optimization significantly hinder their applications in broader scenarios. To tackle these issues, we present HumanSplat which predicts the 3D Gaussian Splatting properties of any human from a single input image in a generalizable manner. In particular, HumanSplat comprises a 2D multi-view diffusion model and a latent reconstruction transformer with human structure priors that adeptly integrate geometric priors and semantic features within a unified framework. A hierarchical loss that incorporates human semantic information is further designed to achieve high-fidelity texture modeling and better constrain the estimated multiple views. Comprehensive experiments on standard benchmarks and in-the-wild images demonstrate that HumanSplat surpasses existing state-of-the-art methods in achieving photorealistic novel-view synthesis.",
        "published": "2024-10-31 04:00:00",
        "id": "9bb22724-316d-4309-ae4d-e6e0d8660670",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "HumanSplat可从单张输入图像中以通用方式预测人物的3D高斯溅射特性，包含2D多视图扩散模型和带有人类结构先验的潜在重建变换器，设计了分层损失以实现高保真纹理建模，在标准基准和野生图像实验中表现优于现有方法。"
        },
        "tokens": 798
    },
    {
        "title": "Stealth edits to large language models",
        "link": "https://arxiv.org/abs/2406.12670",
        "description": "arXiv:2406.12670v2 Announce Type: replace \nAbstract: We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious stealth attacks. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as stealth editing methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new jet-pack network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.",
        "published": "2024-10-31 04:00:00",
        "id": "25a79de0-853d-4b2f-9558-72865b8e8930",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文揭示大型语言模型编辑技术的理论基础并提出新方法，新方法无需重新训练，还揭示模型易受恶意隐形攻击的漏洞并介绍了新的网络块，有实验结果支持且有代码示例。"
        },
        "tokens": 887
    },
    {
        "title": "Depth Anywhere: Enhancing 360 Monocular Depth Estimation via Perspective Distillation and Unlabeled Data Augmentation",
        "link": "https://arxiv.org/abs/2406.12849",
        "description": "arXiv:2406.12849v2 Announce Type: replace \nAbstract: Accurately estimating depth in 360-degree imagery is crucial for virtual reality, autonomous navigation, and immersive media applications. Existing depth estimation methods designed for perspective-view imagery fail when applied to 360-degree images due to different camera projections and distortions, whereas 360-degree methods perform inferior due to the lack of labeled data pairs. We propose a new depth estimation framework that utilizes unlabeled 360-degree data effectively. Our approach uses state-of-the-art perspective depth estimation models as teacher models to generate pseudo labels through a six-face cube projection technique, enabling efficient labeling of depth in 360-degree images. This method leverages the increasing availability of large datasets. Our approach includes two main stages: offline mask generation for invalid regions and an online semi-supervised joint training regime. We tested our approach on benchmark datasets such as Matterport3D and Stanford2D3D, showing significant improvements in depth estimation accuracy, particularly in zero-shot scenarios. Our proposed training pipeline can enhance any 360 monocular depth estimator and demonstrates effective knowledge transfer across different camera projections and data types. See our project page for results: https://albert100121.github.io/Depth-Anywhere/",
        "published": "2024-10-31 04:00:00",
        "id": "6fe83bd6-8f86-4188-af43-f8c6bcebcc1d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的深度估计框架，利用未标记的360度数据，通过六面立方体投影技术生成伪标签，经两个主要阶段测试在基准数据集上显示出深度估计精度显著提高。"
        },
        "tokens": 868
    },
    {
        "title": "LIVE: Learnable In-Context Vector for Visual Question Answering",
        "link": "https://arxiv.org/abs/2406.13185",
        "description": "arXiv:2406.13185v2 Announce Type: replace \nAbstract: As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities. However, applying ICL usually faces two major challenges: 1) using more ICDs will largely increase the inference time and 2) the performance is sensitive to the selection of ICDs. These challenges are further exacerbated in LMMs due to the integration of multiple data types and the combinational complexity of multimodal ICDs. Recently, to address these challenges, some NLP studies introduce non-learnable In-Context Vectors (ICVs) which extract useful task information from ICDs into a single vector and then insert it into the LLM to help solve the corresponding task. However, although useful in simple NLP tasks, these non-learnable methods fail to handle complex multimodal tasks like Visual Question Answering (VQA). In this study, we propose \\underline{\\textbf{L}}earnable \\underline{\\textbf{I}}n-Context \\underline{\\textbf{Ve}}ctor (LIVE) to distill essential task information from demonstrations, improving ICL performance in LMMs. Experiments show that LIVE can significantly reduce computational costs while enhancing accuracy in VQA tasks compared to traditional ICL and other non-learnable ICV methods. The code is available at \\url{https://github.com/ForJadeForest/LIVE-Learnable-In-Context-Vector}.",
        "published": "2024-10-31 04:00:00",
        "id": "c67da508-e827-4155-a61f-d3bc65a9a431",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受语言模型中上下文学习（ICL）技术启发，研究者将其应用于大型多模态模型（LMMs）时面临挑战，本研究提出可学习的上下文向量（LIVE）来提升LMMs中的ICL性能，实验表明LIVE可在视觉问答（VQA）任务中降低计算成本并提高准确性，代码已开源。"
        },
        "tokens": 988
    },
    {
        "title": "MC-MKE: A Fine-Grained Multimodal Knowledge Editing Benchmark Emphasizing Modality Consistency",
        "link": "https://arxiv.org/abs/2406.13219",
        "description": "arXiv:2406.13219v2 Announce Type: replace \nAbstract: Multimodal large language models (MLLMs) are prone to non-factual or outdated knowledge issues, which can manifest as misreading and misrecognition errors due to the complexity of multimodal knowledge. Previous benchmarks have not systematically analyzed the performance of editing methods in correcting these two error types. To better represent and correct these errors, we decompose multimodal knowledge into its visual and textual components. Different error types correspond to different editing formats, which edit distinct parts of the multimodal knowledge. We present MC-MKE, a fine-grained Multimodal Knowledge Editing benchmark emphasizing Modality Consistency. Our benchmark facilitates independent correction of misreading and misrecognition errors by editing the corresponding knowledge component. We evaluate four multimodal knowledge editing methods on MC-MKE, revealing their limitations, particularly in terms of modality consistency. Our work highlights the challenges posed by multimodal knowledge editing and motivates further research in developing effective techniques for this task.",
        "published": "2024-10-31 04:00:00",
        "id": "895b11aa-8fcc-4987-bd28-657c3fd17332",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为更好地表示和纠正多模态知识中的误读和误认错误，提出MC-MKE基准，评估四种多模态知识编辑方法，揭示其局限性，强调多模态知识编辑面临的挑战。"
        },
        "tokens": 805
    },
    {
        "title": "Data Contamination Can Cross Language Barriers",
        "link": "https://arxiv.org/abs/2406.13236",
        "description": "arXiv:2406.13236v2 Announce Type: replace \nAbstract: The opacity in developing large language models (LLMs) is raising growing concerns about the potential contamination of public benchmarks in the pre-training data. Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination. In this paper, we first present a cross-lingual form of contamination that inflates LLMs' performance while evading current detection methods, deliberately injected by overfitting LLMs on the translated versions of benchmark test sets. Then, we propose generalization-based approaches to unmask such deeply concealed contamination. Specifically, we examine the LLM's performance change after modifying the original benchmark by replacing the false answer choices with correct ones from other questions. Contaminated models can hardly generalize to such easier situations, where the false choices can be \\emph{not even wrong}, as all choices are correct in their memorization. Experimental results demonstrate that cross-lingual contamination can easily fool existing detection methods, but not ours. In addition, we discuss the potential utilization of cross-lingual contamination in interpreting LLMs' working mechanisms and in post-training LLMs for enhanced multilingual capabilities. The code and dataset we use can be obtained from \\url{https://github.com/ShangDataLab/Deep-Contam}.",
        "published": "2024-10-31 04:00:00",
        "id": "b081c173-4670-49fd-af06-bd4d7d0a472b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对大型语言模型（LLMs）预训练数据可能污染公共基准的问题，本文提出一种跨语言污染形式可逃避现有检测方法，同时给出基于泛化的检测方法及相关实验结果，并讨论跨语言污染在解释LLMs工作机制和增强多语言能力方面的潜在用途。"
        },
        "tokens": 884
    },
    {
        "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
        "link": "https://arxiv.org/abs/2406.13249",
        "description": "arXiv:2406.13249v2 Announce Type: replace \nAbstract: Retrieval augmented generation (RAG) has been applied in many scenarios to augment large language models (LLMs) with external documents provided by retrievers. However, a semantic gap exists between LLMs and retrievers due to differences in their training objectives and architectures. This misalignment forces LLMs to passively accept the documents provided by the retrievers, leading to incomprehension in the generation process, where the LLMs are burdened with the task of distinguishing these documents using their inherent knowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill this gap by incorporating Retrieval information into Retrieval Augmented Generation. Specifically, R$^2$AG utilizes the nuanced features from the retrievers and employs a R$^2$-Former to capture retrieval information. Then, a retrieval-aware prompting strategy is designed to integrate retrieval information into LLMs' generation. Notably, R$^2$AG suits low-source scenarios where LLMs and retrievers are frozen. Extensive experiments across five datasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our analysis reveals that retrieval information serves as an anchor to aid LLMs in the generation process, thereby filling the semantic gap.",
        "published": "2024-10-31 04:00:00",
        "id": "5cc749e0-0c49-436b-a8cf-166f85b0e218",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "R^2AG是一种新的增强型RAG框架，通过将检索信息融入检索增强生成来填补大型语言模型和检索器之间的语义差距，利用检索器的细微特征并采用R^2 - Former捕获检索信息，设计检索感知提示策略将检索信息整合到LLMs的生成中，在五个数据集的大量实验验证了其有效性、稳健性和效率。"
        },
        "tokens": 905
    },
    {
        "title": "Fusion of Movement and Naive Predictions for Point Forecasting in Univariate Random Walks",
        "link": "https://arxiv.org/abs/2406.14469",
        "description": "arXiv:2406.14469v5 Announce Type: replace \nAbstract: Point forecasting in univariate random walks is an important yet challenging research topic. Many attempts at this task often fail to surpass the na\\\"ive baseline because of the randomness of the data and the improper utilization of exogenous variables as features. In view of the limitations of existing random walk forecasting methods, this study introduces a variant definition of random walks, proposing that point forecasting can be improved beyond the na\\\"ive baseline through the fusion of movement and na\\\"ive predictions (FMNP). FMNP naturally bridges movement prediction and point forecasting. It employs an exogenous variable to provide a consistent movement prediction for the target variable and uses a linear regression to combine movement and na\\\"ive predictions. In forecasting five financial time series in the U.S. market with the FTSE opening price as the exogenous variable, FMNP consistently outperforms na\\\"ive baselines and is superior to baseline models such as ARIMA, MA, MLP, DNN, LSTM, and CNN-LSTM. FMNP is particularly advantageous when accurate point predictions are challenging but accurate movement predictions are attainable, translating movement predictions into point forecasts in random walk contexts.",
        "published": "2024-10-31 04:00:00",
        "id": "5732d1a1-1040-438f-8b25-e9dc02198405",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "针对单变量随机游走中的点预测，研究提出融合移动和朴素预测可超越朴素基线，在以富时开盘价为外生变量预测美国市场五个金融时间序列时，该方法表现优于朴素基线和其他基准模型。"
        },
        "tokens": 853
    },
    {
        "title": "Optimal deep learning of holomorphic operators between Banach spaces",
        "link": "https://arxiv.org/abs/2406.13928",
        "description": "arXiv:2406.13928v2 Announce Type: replace \nAbstract: Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators - an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures - specifically, those with constant width exceeding the depth - under standard $\\ell^2$-loss minimization. We first identify a family of DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are `problem agnostic', with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.",
        "published": "2024-10-31 04:00:00",
        "id": "fb8b1719-634f-4e60-92f3-536f7fc68885",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究解决Banach空间间算子学习问题，聚焦全连接架构下学习全纯算子，确定了使深度学习过程达到最优泛化界的深度神经网络族，证明深度学习对此问题是最优的，并给出数值结果展示在一些偏微分方程问题上的实际性能。"
        },
        "tokens": 901
    },
    {
        "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
        "link": "https://arxiv.org/abs/2406.13975",
        "description": "arXiv:2406.13975v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, evaluating these reasoning abilities has become increasingly challenging. Existing outcome-based benchmarks are beginning to saturate, becoming less effective in tracking meaningful progress. To address this, we present a process-based benchmark MR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Our meta-reasoning paradigm is especially suited for system-2 slow thinking, mirroring the human cognitive process of carefully examining assumptions, conditions, calculations, and logic to identify mistakes. MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, with models like the o1 series from OpenAI demonstrating strong performance by effectively scrutinizing the solution space, many other state-of-the-art models fall significantly behind on MR-Ben, exposing potential shortcomings in their training strategies and inference methodologies.",
        "published": "2024-10-31 04:00:00",
        "id": "3ace3c39-8aec-48f8-8e16-f5bcfc2d19e5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决评估大型语言模型推理能力的挑战，提出基于过程的基准MR - Ben，需元推理技能，包含5975个由人类专家精心挑选的多学科问题，通过设计的评估指标发现当前LLMs存在局限与弱点。"
        },
        "tokens": 877
    },
    {
        "title": "Control when confidence is costly",
        "link": "https://arxiv.org/abs/2406.14427",
        "description": "arXiv:2406.14427v2 Announce Type: replace \nAbstract: We develop a version of stochastic control that accounts for computational costs of inference. Past studies identified efficient coding without control, or efficient control that neglects the cost of synthesizing information. Here we combine these concepts into a framework where agents rationally approximate inference for efficient control. Specifically, we study Linear Quadratic Gaussian (LQG) control with an added internal cost on the relative precision of the posterior probability over the world state. This creates a trade-off: an agent can obtain more utility overall by sacrificing some task performance, if doing so saves enough bits during inference. We discover that the rational strategy that solves the joint inference and control problem goes through phase transitions depending on the task demands, switching from a costly but optimal inference to a family of suboptimal inferences related by rotation transformations, each misestimate the stability of the world. In all cases, the agent moves more to think less. This work provides a foundation for a new type of rational computations that could be used by both brains and machines for efficient but computationally constrained control.",
        "published": "2024-10-31 04:00:00",
        "id": "c321c461-80c7-4730-a24a-e37ca7936d78",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出考虑推理计算成本的随机控制版本，研究线性二次高斯控制中世界状态后验概率相对精度的内部成本，发现解决联合推理和控制问题的理性策略存在相变，代理以牺牲部分任务性能来减少推理中的比特消耗。"
        },
        "tokens": 815
    },
    {
        "title": "Certification for Differentially Private Prediction in Gradient-Based Training",
        "link": "https://arxiv.org/abs/2406.13433",
        "description": "arXiv:2406.13433v2 Announce Type: replace \nAbstract: Differential privacy upper-bounds the information leakage of machine learning models, yet providing meaningful privacy guarantees has proven to be challenging in practice. The private prediction setting where model outputs are privatized is being investigated as an alternate way to provide formal guarantees at prediction time. Most current private prediction algorithms, however, rely on global sensitivity for noise calibration, which often results in large amounts of noise being added to the predictions. Data-specific noise calibration, such as smooth sensitivity, could significantly reduce the amount of noise added, but were so far infeasible to compute exactly for modern machine learning models. In this work we provide a novel and practical approach based on convex relaxation and bound propagation to compute a provable upper-bound for the local and smooth sensitivity of a prediction. This bound allows us to reduce the magnitude of noise added or improve privacy accounting in the private prediction setting. We validate our framework on datasets from financial services, medical image classification, and natural language processing and across models and find our approach to reduce the noise added by up to order of magnitude.",
        "published": "2024-10-31 04:00:00",
        "id": "ec28ea18-137c-463c-9832-c1fc1210457c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种基于凸松弛和边界传播计算预测的局部和平滑敏感度可证明上界的新方法，可减少隐私预测设置中的噪声或改进隐私计算，已在多个数据集和模型上得到验证。"
        },
        "tokens": 814
    },
    {
        "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding",
        "link": "https://arxiv.org/abs/2406.14515",
        "description": "arXiv:2406.14515v3 Announce Type: replace \nAbstract: The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
        "published": "2024-10-31 04:00:00",
        "id": "187dc01e-359c-44b4-a012-54e83d9a6737",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决传统VideoQA基准测试的局限性，推出MMBench - Video定量基准来评估大型视觉 - 语言模型的视频理解能力，采用YouTube长视频、自由形式问题并按能力分类法人工注释，用GPT - 4评估，评估代码将集成到VLMEvalKit。"
        },
        "tokens": 887
    },
    {
        "title": "CooHOI: Learning Cooperative Human-Object Interaction with Manipulated Object Dynamics",
        "link": "https://arxiv.org/abs/2406.14558",
        "description": "arXiv:2406.14558v3 Announce Type: replace \nAbstract: Enabling humanoid robots to clean rooms has long been a pursued dream within humanoid research communities. However, many tasks require multi-humanoid collaboration, such as carrying large and heavy furniture together. Given the scarcity of motion capture data on multi-humanoid collaboration and the efficiency challenges associated with multi-agent learning, these tasks cannot be straightforwardly addressed using training paradigms designed for single-agent scenarios. In this paper, we introduce Cooperative Human-Object Interaction (CooHOI), a framework designed to tackle the challenge of multi-humanoid object transportation problem through a two-phase learning paradigm: individual skill learning and subsequent policy transfer. First, a single humanoid character learns to interact with objects through imitation learning from human motion priors. Then, the humanoid learns to collaborate with others by considering the shared dynamics of the manipulated object using centralized training and decentralized execution (CTDE) multi-agent RL algorithms. When one agent interacts with the object, resulting in specific object dynamics changes, the other agents learn to respond appropriately, thereby achieving implicit communication and coordination between teammates. Unlike previous approaches that relied on tracking-based methods for multi-humanoid HOI, CooHOI is inherently efficient, does not depend on motion capture data of multi-humanoid interactions, and can be seamlessly extended to include more participants and a wide range of object types.",
        "published": "2024-10-31 04:00:00",
        "id": "677b1d14-42ba-4c30-8130-68bb535b95b0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出CooHOI框架，通过两阶段学习范式解决多人形机器人物体运输问题，先单个人形角色通过模仿学习获得与物体交互技能，再用多智能体强化学习算法实现协作，该框架高效且不依赖多人形交互的动作捕捉数据并可扩展。"
        },
        "tokens": 896
    },
    {
        "title": "Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study",
        "link": "https://arxiv.org/abs/2406.14629",
        "description": "arXiv:2406.14629v2 Announce Type: replace \nAbstract: Teaching to improve student models (e.g., knowledge distillation) is an extensively studied methodology in LLMs. However, for humans, teaching improves not only students but also teachers, by fostering more rigorous and clear reasoning as well as knowledge building. We ask: Can LLMs also learn by teaching (LbT) for better reasoning? If the answer is yes, we can potentially unlock the possibility of continuously advancing the models without solely relying on human-produced data or stronger models. In this paper, we provide a preliminary exploration on this question. We show that LbT ideas can be incorporated into existing LLM training/prompting pipelines and bring improvements. Specifically, we design three methods, each mimicking one of the three levels of LbT: observing students' feedback, learning from the feedback, and learning iteratively, with the goals of improving answer accuracy without training or improving models' inherent capability with fine-tuning. We reveal some findings: (1) Teaching materials that make it easier for students to learn have clearer and more accurate logic when using in-context learning as the student's \"learning\" method; (2) Weak-to-strong generalization: LbT might help improve strong models by teaching weak models; (3) Diversity in students might help: teaching multiple students could be better than teaching one student or the teacher itself. We hope that our exploration can inspire future research on LbT and more broadly adopting the advanced techniques in education to improve LLMs. The code and website are at https://github.com/imagination-research/lbt and https://sites.google.com/view/llm-learning-by-teaching.",
        "published": "2024-10-31 04:00:00",
        "id": "4be1703b-8859-43f2-88c3-fdcc8fbed076",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该研究初步探索大型语言模型（LLMs）能否通过教学（LbT）实现更好的推理，设计了三种模仿LbT的方法并得出相关发现。"
        },
        "tokens": 933
    },
    {
        "title": "Exploring Design Choices for Building Language-Specific LLMs",
        "link": "https://arxiv.org/abs/2406.14670",
        "description": "arXiv:2406.14670v2 Announce Type: replace \nAbstract: Despite rapid progress in large language models (LLMs), their performance on a vast majority of languages remains unsatisfactory. In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs. We conduct systematic experiments on how design choices (base model selection, vocabulary extension, and continued pretraining) impact the adapted LLM, both in terms of efficiency (how many tokens are needed to encode the same amount of information) and end task performance. We find that (1) the initial performance of LLM does not always correlate with the final performance after the adaptation. Adapting an English-centric models can yield better results than adapting multilingual models despite their worse initial performance on low-resource languages. (2) Efficiency can easily improved with simple vocabulary extension and continued pretraining in most LLMs we study, and (3) The optimal adaptation method (choice of the base model, new vocabulary size, training data, initialization strategy) is highly language-dependent, and the simplest embedding initialization works well across various experimental settings. Together, our work lays foundations on efficiently building language-specific LLMs by adapting existing LLMs.",
        "published": "2024-10-31 04:00:00",
        "id": "f4b26444-ebad-468e-a12e-90ecb70aa6ac",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究通过调整单语和多语大型语言模型（LLMs）构建特定语言的LLMs，进行系统实验探究设计选择对调整后模型在效率和任务性能方面的影响，发现初始性能与调整后最终性能不总相关、效率可通过词汇扩展和继续预训练提高、最优调整方法与语言高度相关等。"
        },
        "tokens": 862
    },
    {
        "title": "Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph",
        "link": "https://arxiv.org/abs/2406.15627",
        "description": "arXiv:2406.15627v2 Announce Type: replace \nAbstract: Uncertainty quantification (UQ) is a critical component of machine learning (ML) applications. The rapid proliferation of large language models (LLMs) has stimulated researchers to seek efficient and effective approaches to UQ for text generation. As with other ML models, LLMs are prone to making incorrect predictions, in the form of ``hallucinations'' whereby claims are fabricated or low-quality outputs are generated for a given input. UQ is a key element in dealing with these challenges. However, research to date on UQ methods for LLMs has been fragmented, in terms of the literature on UQ techniques and evaluation methods. In this work, we tackle this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines, and provides an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across nine tasks, and identify the most promising approaches.\n  Code: https://github.com/IINemo/lm-polygraph",
        "published": "2024-10-31 04:00:00",
        "id": "4ff63377-675f-4746-9dfb-24b822053afc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一个新基准，包含用于文本生成任务的UQ基线，提供可控、一致的评估环境，还支持对置信度归一化方法的评估，通过基准对九项任务进行大规模实证研究并确定最有前景的方法。"
        },
        "tokens": 861
    },
    {
        "title": "Bandits with Preference Feedback: A Stackelberg Game Perspective",
        "link": "https://arxiv.org/abs/2406.16745",
        "description": "arXiv:2406.16745v2 Announce Type: replace \nAbstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.",
        "published": "2024-10-31 04:00:00",
        "id": "9f98a977-2d51-457f-beb4-da6643ebae4c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在带偏好反馈的老虎机问题中，针对无限域和非线性（核化）奖励场景提出MAXMINLCB算法，该算法将探索与利用的权衡模拟为零和Stackelberg游戏，表现优于现有算法并满足最优遗憾保证，此算法基于新的基于偏好的置信序列。"
        },
        "tokens": 824
    },
    {
        "title": "Soft Language Prompts for Language Transfer",
        "link": "https://arxiv.org/abs/2407.02317",
        "description": "arXiv:2407.02317v2 Announce Type: replace \nAbstract: Cross-lingual knowledge transfer, especially between high- and low-resource languages, remains challenging in natural language processing (NLP). This study offers insights for improving cross-lingual NLP applications through the combination of parameter-efficient fine-tuning methods. We systematically explore strategies for enhancing cross-lingual transfer through the incorporation of language-specific and task-specific adapters and soft prompts. We present a detailed investigation of various combinations of these methods, exploring their efficiency across 16 languages, focusing on 10 mid- and low-resource languages. We further present to our knowledge the first use of soft prompts for language transfer, a technique we call soft language prompts. Our findings demonstrate that in contrast to claims of previous work, a combination of language and task adapters does not always work best; instead, combining a soft language prompt with a task adapter outperforms most configurations in many cases.",
        "published": "2024-10-31 04:00:00",
        "id": "e61c72aa-7d1e-422b-8819-545bd3611167",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨通过参数高效微调方法组合改进跨语言自然语言处理应用，研究特定及任务适配器与软提示的组合策略，提出软语言提示技术并发现软语言提示与任务适配器组合在很多情况下性能更优。"
        },
        "tokens": 788
    },
    {
        "title": "Gaussian process-based online health monitoring and fault analysis of lithium-ion battery systems from field data",
        "link": "https://arxiv.org/abs/2406.19015",
        "description": "arXiv:2406.19015v3 Announce Type: replace \nAbstract: Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.",
        "published": "2024-10-31 04:00:00",
        "id": "ede681d3-99d3-4828-8303-d474e26c1d72",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "应用高斯过程电阻模型对磷酸铁锂电池现场数据进行分析，开发概率故障检测规则，以理解电池在现场的退化和故障情况，并展示基于数据的高效在线监测潜力。"
        },
        "tokens": 820
    },
    {
        "title": "Towards Universal Mesh Movement Networks",
        "link": "https://arxiv.org/abs/2407.00382",
        "description": "arXiv:2407.00382v3 Announce Type: replace \nAbstract: Solving complex Partial Differential Equations (PDEs) accurately and efficiently is an essential and challenging problem in all scientific and engineering disciplines. Mesh movement methods provide the capability to improve the accuracy of the numerical solution without increasing the overall mesh degree of freedom count. Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries. However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements. In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries. UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh. We evaluate our method on advection and Navier-Stokes based examples, as well as a real-world tsunami simulation case. Our method outperforms existing learning-based mesh movement methods in terms of the benchmarks described above. In comparison to the conventional sophisticated Monge-Amp\\`ere PDE-solver based method, our approach not only significantly accelerates mesh movement, but also proves effective in scenarios where the conventional method fails. Our project page is at https://erizmr.github.io/UM2N/.",
        "published": "2024-10-31 04:00:00",
        "id": "a8bcd4b1-d770-4589-b2b4-b997f12a7900",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出通用网格移动网络（UM2N），可零采样方式移动不同尺寸分布和结构的网格，在多个示例中表现优于现有基于学习的网格移动方法，相比传统方法加速显著且在传统方法失败的场景中有效。"
        },
        "tokens": 910
    },
    {
        "title": "Towards optimal hierarchical training of neural networks",
        "link": "https://arxiv.org/abs/2407.02242",
        "description": "arXiv:2407.02242v2 Announce Type: replace \nAbstract: We propose a hierarchical training algorithm for standard feed-forward neural networks that adaptively extends the network architecture as soon as the optimization reaches a stationary point. By solving small (low-dimensional) optimization problems, the extended network provably escapes any local minimum or stationary point. Under some assumptions on the approximability of the data with stable neural networks, we show that the algorithm achieves an optimal convergence rate s in the sense that loss is bounded by the number of parameters to the -s. As a byproduct, we obtain computable indicators which judge the optimality of the training state of a given network and derive a new notion of generalization error.",
        "published": "2024-10-31 04:00:00",
        "id": "75464096-fd1d-4644-a451-3543838ebad4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种针对标准前馈神经网络的分层训练算法，在一些假设下能达到最优收敛率，还得到可计算指标判断网络训练状态最优性并推导出新的泛化误差概念。"
        },
        "tokens": 727
    },
    {
        "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
        "link": "https://arxiv.org/abs/2406.17808",
        "description": "arXiv:2406.17808v2 Announce Type: replace \nAbstract: The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.",
        "published": "2024-10-31 04:00:00",
        "id": "acc0b00d-2e1f-4c37-a136-309714654f37",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种利用级联子缓存缓冲区选择性保留最相关标记的新机制，能在不增加缓存大小的情况下维持更长的上下文历史，在多个关键基准测试中表现优于线性缓存基线，还可减少预填充阶段延迟。"
        },
        "tokens": 894
    },
    {
        "title": "Generative Large Language Models in Automated Fact-Checking: A Survey",
        "link": "https://arxiv.org/abs/2407.02351",
        "description": "arXiv:2407.02351v2 Announce Type: replace \nAbstract: The dissemination of false information on online platforms presents a serious societal challenge. While manual fact-checking remains crucial, Large Language Models (LLMs) offer promising opportunities to support fact-checkers with their vast knowledge and advanced reasoning capabilities. This survey explores the application of generative LLMs in fact-checking, highlighting various approaches and techniques for prompting or fine-tuning these models. By providing an overview of existing methods and their limitations, the survey aims to enhance the understanding of how LLMs can be used in fact-checking and to facilitate further progress in their integration into the fact-checking process.",
        "published": "2024-10-31 04:00:00",
        "id": "85b5231c-50d0-4c8c-8ccc-f1b8d1bc5e77",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "探讨生成式大型语言模型（LLMs）在事实核查中的应用、方法和技术及其局限性，旨在增进对LLMs用于事实核查的理解并推动其融入事实核查流程。"
        },
        "tokens": 722
    },
    {
        "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
        "link": "https://arxiv.org/abs/2407.02490",
        "description": "arXiv:2407.02490v2 Announce Type: replace \nAbstract: The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.",
        "published": "2024-10-31 04:00:00",
        "id": "c4993a28-961b-4aa5-999b-ed00bda631c3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "MInference 1.0是一种稀疏计算方法，旨在加速长序列处理的预填充，通过识别长上下文注意矩阵中的三种独特模式进行高效稀疏计算，可直接应用于现有LLM，在多种下游任务和模型上能有效降低预填充推理延迟达10倍且保持准确性。"
        },
        "tokens": 1023
    },
    {
        "title": "Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI Responses",
        "link": "https://arxiv.org/abs/2407.02551",
        "description": "arXiv:2407.02551v2 Announce Type: replace \nAbstract: Vulnerability of Frontier language models to misuse and jailbreaks has prompted the development of safety measures like filters and alignment training in an effort to ensure safety through robustness to adversarially crafted prompts. We assert that robustness is fundamentally insufficient for ensuring safety goals, and current defenses and evaluation methods fail to account for risks of dual-intent queries and their composition for malicious goals. To quantify these risks, we introduce a new safety evaluation framework based on impermissible information leakage of model outputs and demonstrate how our proposed question-decomposition attack can extract dangerous knowledge from a censored LLM more effectively than traditional jailbreaking. Underlying our proposed evaluation method is a novel information-theoretic threat model of inferential adversaries, distinguished from security adversaries, such as jailbreaks, in that success is measured by inferring impermissible knowledge from victim outputs as opposed to forcing explicitly impermissible outputs from the victim. Through our information-theoretic framework, we show that to ensure safety against inferential adversaries, defense mechanisms must ensure information censorship, bounding the leakage of impermissible information. However, we prove that such defenses inevitably incur a safety-utility trade-off.",
        "published": "2024-10-31 04:00:00",
        "id": "a3a5f84a-99e6-475a-b04f-a3d7d7d2b525",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究指出前沿语言模型安全措施中的稳健性不足以确保安全目标，提出基于模型输出的不允许信息泄漏的新安全评估框架，证明防御机制必然会在安全 - 效用之间权衡。"
        },
        "tokens": 842
    },
    {
        "title": "DiffPhyCon: A Generative Approach to Control Complex Physical Systems",
        "link": "https://arxiv.org/abs/2407.06494",
        "description": "arXiv:2407.06494v4 Announce Type: replace \nAbstract: Controlling the evolution of complex physical systems is a fundamental task across science and engineering. Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon.",
        "published": "2024-10-31 04:00:00",
        "id": "04d76a83-2949-4f36-a649-3b25e4feb8f3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "DiffPhyCon是一种新的物理系统控制方法，它同时最小化生成能量函数和预定义控制目标，在三个任务中表现优于其他方法，相关数据集、代码和项目网站已发布。"
        },
        "tokens": 904
    },
    {
        "title": "It's Our Loss: No Privacy Amplification for Hidden State DP-SGD With Non-Convex Loss",
        "link": "https://arxiv.org/abs/2407.06496",
        "description": "arXiv:2407.06496v3 Announce Type: replace \nAbstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular iterative algorithm used to train machine learning models while formally guaranteeing the privacy of users. However, the privacy analysis of DP-SGD makes the unrealistic assumption that all intermediate iterates (aka internal state) of the algorithm are released since, in practice, only the final trained model, i.e., the final iterate of the algorithm is released. In this hidden state setting, prior work has provided tighter analyses, albeit only when the loss function is constrained, e.g., strongly convex and smooth or linear. On the other hand, the privacy leakage observed empirically from hidden state DP-SGD, even when using non-convex loss functions, suggests that there is in fact a gap between the theoretical privacy analysis and the privacy guarantees achieved in practice. Therefore, it remains an open question whether hidden state privacy amplification for DP-SGD is possible for all (possibly non-convex) loss functions in general.\n  In this work, we design a counter-example and show, both theoretically and empirically, that a hidden state privacy amplification result for DP-SGD for all loss functions in general is not possible. By carefully constructing a loss function for DP-SGD, we show that for specific loss functions, the final iterate of DP-SGD alone leaks as much information as the sequence of all iterates combined. Furthermore, we empirically verify this result by evaluating the privacy leakage from the final iterate of DP-SGD with our loss function and show that this exactly matches the theoretical upper bound guaranteed by DP. Therefore, we show that the current privacy analysis for DP-SGD is tight for general loss functions and conclude that no privacy amplification is possible for DP-SGD in general for all (possibly non-convex) loss functions.",
        "published": "2024-10-31 04:00:00",
        "id": "ffc47796-bf36-44e5-9f5c-f86377745433",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过理论和实证构建反例表明，一般而言，对于所有（可能为非凸）损失函数，DP - SGD的隐藏状态隐私放大结果不可能存在，即当前DP - SGD的隐私分析对于一般损失函数是紧的，无法进行隐私放大。"
        },
        "tokens": 988
    },
    {
        "title": "Aligning Diffusion Behaviors with Q-functions for Efficient Continuous Control",
        "link": "https://arxiv.org/abs/2407.09024",
        "description": "arXiv:2407.09024v2 Announce Type: replace \nAbstract: Drawing upon recent advances in language model alignment, we formulate offline Reinforcement Learning as a two-stage optimization problem: First pretraining expressive generative policies on reward-free behavior datasets, then fine-tuning these policies to align with task-specific annotations like Q-values. This strategy allows us to leverage abundant and diverse behavior data to enhance generalization and enable rapid adaptation to downstream tasks using minimal annotations. In particular, we introduce Efficient Diffusion Alignment (EDA) for solving continuous control problems. EDA utilizes diffusion models for behavior modeling. However, unlike previous approaches, we represent diffusion policies as the derivative of a scalar neural network with respect to action inputs. This representation is critical because it enables direct density calculation for diffusion models, making them compatible with existing LLM alignment theories. During policy fine-tuning, we extend preference-based alignment methods like Direct Preference Optimization (DPO) to align diffusion behaviors with continuous Q-functions. Our evaluation on the D4RL benchmark shows that EDA exceeds all baseline methods in overall performance. Notably, EDA maintains about 95\\% of performance and still outperforms several baselines given only 1\\% of Q-labelled data during fine-tuning.",
        "published": "2024-10-31 04:00:00",
        "id": "46a6241d-051b-41f3-a9c8-aeba82b0a0c5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出将离线强化学习作为两阶段优化问题，引入高效扩散对齐（EDA）解决连续控制问题，在D4RL基准测试中性能超基线方法。"
        },
        "tokens": 837
    },
    {
        "title": "PARE-Net: Position-Aware Rotation-Equivariant Networks for Robust Point Cloud Registration",
        "link": "https://arxiv.org/abs/2407.10142",
        "description": "arXiv:2407.10142v2 Announce Type: replace \nAbstract: Learning rotation-invariant distinctive features is a fundamental requirement for point cloud registration. Existing methods often use rotation-sensitive networks to extract features, while employing rotation augmentation to learn an approximate invariant mapping rudely. This makes networks fragile to rotations, overweight, and hinders the distinctiveness of features. To tackle these problems, we propose a novel position-aware rotation-equivariant network, for efficient, light-weighted, and robust registration. The network can provide a strong model inductive bias to learn rotation-equivariant/invariant features, thus addressing the aforementioned limitations. To further improve the distinctiveness of descriptors, we propose a position-aware convolution, which can better learn spatial information of local structures. Moreover, we also propose a feature-based hypothesis proposer. It leverages rotation-equivariant features that encode fine-grained structure orientations to generate reliable model hypotheses. Each correspondence can generate a hypothesis, thus it is more efficient than classic estimators that require multiple reliable correspondences. Accordingly, a contrastive rotation loss is presented to enhance the robustness of rotation-equivariant features against data degradation. Extensive experiments on indoor and outdoor datasets demonstrate that our method significantly outperforms the SOTA methods in terms of registration recall while being lightweight and keeping a fast speed. Moreover, experiments on rotated datasets demonstrate its robustness against rotation variations. Code is available at https://github.com/yaorz97/PARENet.",
        "published": "2024-10-31 04:00:00",
        "id": "8a8f42de-589e-436c-8085-4bb5d134fc3b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出PARE - Net用于点云注册，它是一种位置感知旋转等变网络，可提供模型归纳偏差学习旋转等变/不变特征，还提出位置感知卷积和基于特征的假设提议器，对比旋转损失以增强特征鲁棒性，实验表明其性能优于SOTA方法。"
        },
        "tokens": 912
    },
    {
        "title": "Enabling MCTS Explainability for Sequential Planning Through Computation Tree Logic",
        "link": "https://arxiv.org/abs/2407.10820",
        "description": "arXiv:2407.10820v3 Announce Type: replace \nAbstract: Monte Carlo tree search (MCTS) is one of the most capable online search algorithms for sequential planning tasks, with significant applications in areas such as resource allocation and transit planning. Despite its strong performance in real-world deployment, the inherent complexity of MCTS makes it challenging to understand for users without technical background. This paper considers the use of MCTS in transportation routing services, where the algorithm is integrated to develop optimized route plans. These plans are required to meet a range of constraints and requirements simultaneously, further complicating the task of explaining the algorithm's operation in real-world contexts. To address this critical research gap, we introduce a novel computation tree logic-based explainer for MCTS. Our framework begins by taking user-defined requirements and translating them into rigorous logic specifications through the use of language templates. Then, our explainer incorporates a logic verification and quantitative evaluation module that validates the states and actions traversed by the MCTS algorithm. The outcomes of this analysis are then rendered into human-readable descriptive text using a second set of language templates. The user satisfaction of our approach was assessed through a survey with 82 participants. The results indicated that our explanatory approach significantly outperforms other baselines in user preference.",
        "published": "2024-10-31 04:00:00",
        "id": "850f620b-7544-4bbe-880a-731fae79824b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对蒙特卡洛树搜索（MCTS）在运输路线服务中因复杂难被无技术背景用户理解的问题，本文提出基于计算树逻辑的解释器，经用户调查评估，该解释方法在用户偏好方面显著优于其他基准。"
        },
        "tokens": 867
    },
    {
        "title": "Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion",
        "link": "https://arxiv.org/abs/2407.10973",
        "description": "arXiv:2407.10973v2 Announce Type: replace \nAbstract: Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description? In this paper, we present Make-An-Agent, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. Trained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by Make-An-Agent onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/",
        "published": "2024-10-31 04:00:00",
        "id": "b7d17575-ae87-47de-8d52-dc5e55d26eb4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Make - An - Agent是一种新的策略参数生成器，利用条件扩散模型将行为转化为策略，经策略网络检查点及其对应轨迹训练，具有通用性、可扩展性和强泛化能力，在多任务中展现效能和效率，且能将生成策略应用于现实世界机器人运动任务。"
        },
        "tokens": 836
    },
    {
        "title": "Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws",
        "link": "https://arxiv.org/abs/2408.02946",
        "description": "arXiv:2408.02946v4 Announce Type: replace \nAbstract: LLMs produce harmful and undesirable behavior when trained on poisoned datasets that contain a small fraction of corrupted or harmful data. We develop a new attack paradigm, jailbreak-tuning, that combines data poisoning with jailbreaking to fully bypass state-of-the-art safeguards and make models like GPT-4o comply with nearly any harmful request. Our experiments suggest this attack represents a paradigm shift in vulnerability elicitation, producing differences in refusal rates as much as 60+ percentage points compared to normal fine-tuning. Given this demonstration of how data poisoning vulnerabilities persist and can be amplified, we investigate whether these risks will likely increase as models scale. We evaluate three threat models - malicious fine-tuning, imperfect data curation, and intentional data contamination - across 23 frontier LLMs ranging from 1.5 to 72 billion parameters. Our experiments reveal that larger LLMs are significantly more susceptible to data poisoning, learning harmful behaviors from even minimal exposure to harmful data more quickly than smaller models. These findings underscore the need for leading AI companies to thoroughly red team fine-tuning APIs before public release and to develop more robust safeguards against data poisoning, particularly as models continue to scale in size and capability.",
        "published": "2024-10-31 04:00:00",
        "id": "0d55ff02-e9a2-4178-8f62-7a62408d81cf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现LLMs在包含少量有害数据的中毒数据集上训练会产生有害行为，开发了新攻击范式jailbreak - tuning，实验表明大型LLMs更易受数据中毒影响，强调AI公司需防范数据中毒风险。"
        },
        "tokens": 857
    },
    {
        "title": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
        "link": "https://arxiv.org/abs/2407.14494",
        "description": "arXiv:2407.14494v2 Announce Type: replace \nAbstract: Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",
        "published": "2024-10-31 04:00:00",
        "id": "400bb7c0-32ac-4c36-adca-d323d96e5931",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "InterpBench是一组半合成且逼真的变换器，可用于评估机械可解释性技术，文中介绍了Strict IIT（SIIT）训练神经网络的方法，并使用该基准评估现有电路发现技术。"
        },
        "tokens": 796
    },
    {
        "title": "u-$\\mu$P: The Unit-Scaled Maximal Update Parametrization",
        "link": "https://arxiv.org/abs/2407.17465",
        "description": "arXiv:2407.17465v2 Announce Type: replace \nAbstract: The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a loss that is equal to or lower than comparable $\\mu$P models and working out-of-the-box in FP8.",
        "published": "2024-10-31 04:00:00",
        "id": "f1146e04-2ad1-463e-87fe-72216b7c2cc8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章介绍了一种新方案u-μP，它结合最大更新参数化（μP）和单位缩放，使模型的默认值接近最优，有助于更有效的扫描策略，u-μP模型损失等于或低于可比的μP模型且能在FP8中直接使用。"
        },
        "tokens": 836
    },
    {
        "title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models",
        "link": "https://arxiv.org/abs/2408.00113",
        "description": "arXiv:2408.00113v2 Announce Type: replace \nAbstract: What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features -- for example, \"there is a knight on F3\" -- which we leverage into $\\textit{supervised}$ metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, $\\textit{p-annealing}$, which improves performance on prior unsupervised metrics as well as our new metrics.",
        "published": "2024-10-31 04:00:00",
        "id": "64a173ff-3dde-41fa-856c-8bae36321f17",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决评估稀疏自动编码器（SAE）质量难的问题，提出通过棋类游戏模型衡量可解释字典学习进展，还引入新的SAE训练技术p - annealing。"
        },
        "tokens": 784
    },
    {
        "title": "Empowering Persian LLMs for Instruction Following: A Novel Dataset and Training Approach",
        "link": "https://arxiv.org/abs/2407.11186",
        "description": "arXiv:2407.11186v3 Announce Type: replace \nAbstract: Instruction-tuned large language models have demonstrated remarkable capabilities in following human instructions across various domains. However, their proficiency remains notably deficient in many low-resource languages. To address this challenge, we begin by introducing FarsInstruct a comprehensive instruction dataset designed to enhance the instruction following ability of large language models specifically for the Persian language a significant yet underrepresented language globally. FarsInstruct encompasses a wide range of task types and datasets, each containing a mix of straightforward to complex manual written instructions, as well as translations from the Public Pool of Prompts, ensuring a rich linguistic and cultural representation. Furthermore, we introduce Co-CoLA, a framework designed to enhance the multi-task adaptability of LoRA-tuned models. Through extensive experimental analyses, our study showcases the effectiveness of the FarsInstruct dataset coupled with training by the Co-CoLA framework, in improving the performance of large language models within the Persian context. As of the current writing, FarsInstruct comprises 197 templates across 21 distinct datasets, and we intend to update it consistently, thus augmenting its applicability.",
        "published": "2024-10-31 04:00:00",
        "id": "1882f9eb-ac18-4d58-ade5-e628e0a0c3c1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了为提升波斯语大型语言模型指令遵循能力构建的FarsInstruct数据集和Co - CoLA框架，实验分析表明二者结合可提高波斯语语境下大型语言模型的性能。"
        },
        "tokens": 826
    },
    {
        "title": "Certifiably Robust Policies for Uncertain Parametric Environments",
        "link": "https://arxiv.org/abs/2408.03093",
        "description": "arXiv:2408.03093v3 Announce Type: replace \nAbstract: We present a data-driven approach for producing policies that are provably robust across unknown stochastic environments. Existing approaches can learn models of a single environment as an interval Markov decision processes (IMDP) and produce a robust policy with a probably approximately correct (PAC) guarantee on its performance. However these are unable to reason about the impact of environmental parameters underlying the uncertainty. We propose a framework based on parametric Markov decision processes (MDPs) with unknown distributions over parameters. We learn and analyse IMDPs for a set of unknown sample environments induced by parameters. The key challenge is then to produce meaningful performance guarantees that combine the two layers of uncertainty: (1) multiple environments induced by parameters with an unknown distribution; (2) unknown induced environments which are approximated by IMDPs. We present a novel approach based on scenario optimisation that yields a single PAC guarantee quantifying the risk level for which a specified performance level can be assured in unseen environments, plus a means to trade-off risk and performance. We implement and evaluate our framework using multiple robust policy generation methods on a range of benchmarks. We show that our approach produces tight bounds on a policy's performance with high confidence.",
        "published": "2024-10-31 04:00:00",
        "id": "d7960977-a6ac-401d-89ff-13c9ef1c5a2b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种数据驱动方法来生成在未知随机环境中可证明稳健的策略，基于参数马尔可夫决策过程框架，采用情景优化方法得出单个PAC保证，结合多种策略生成方法在基准上评估，可高置信度得出策略性能的紧密界限。"
        },
        "tokens": 853
    },
    {
        "title": "Electricity Market-Clearing With Extreme Events",
        "link": "https://arxiv.org/abs/2408.03409",
        "description": "arXiv:2408.03409v2 Announce Type: replace \nAbstract: Extreme events jeopardize power network operations, causing beyond-design failures and massive supply interruptions. Existing market designs fail to internalize and systematically assess the risk of extreme and rare events. Efficiently maintaining the reliability of renewable-dominant power systems during extreme weather events requires co-optimizing system resources, while differentiating between large/rare and small/frequent deviations from forecast conditions. To address this gap in both research and practice, we propose managing the uncertainties associated with extreme weather events through an additional reserve service, termed extreme reserve. The procurement of extreme reserve is co-optimized with energy and regular reserve using a large deviation theory chance-constrained (LDT-CC) model, where LDT offers a mathematical framework to quantify the increased uncertainty during extreme events. To mitigate the high additional costs associated with reserve scheduling under the LDT-CC model, we also propose an LDT model based on weighted chance constraints (LDT-WCC). This model prepares the power system for extreme events at a lower cost, making it a less conservative alternative to the LDT-CC model. The proposed market design leads to a competitive equilibrium while ensuring cost recovery. Numerical experiments on an illustrative system and a modified 8-zone ISO New England system highlight the advantages of the proposed market design.",
        "published": "2024-10-31 04:00:00",
        "id": "f93c9758-975e-4552-8818-0e8f2a83be0c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出通过极端储备服务管理极端天气事件相关的不确定性，用LDT - CC模型及成本更低的LDT - WCC模型共同优化能源、常规储备和极端储备的采购，数值实验突显了所提市场设计的优势。"
        },
        "tokens": 871
    },
    {
        "title": "Evaluating LLMs on Entity Disambiguation in Tables",
        "link": "https://arxiv.org/abs/2408.06423",
        "description": "arXiv:2408.06423v2 Announce Type: replace \nAbstract: Tables are crucial containers of information, but understanding their meaning may be challenging. Over the years, there has been a surge in interest in data-driven approaches based on deep learning that have increasingly been combined with heuristic-based ones. In the last period, the advent of \\acf{llms} has led to a new category of approaches for table annotation. However, these approaches have not been consistently evaluated on a common ground, making evaluation and comparison difficult. This work proposes an extensive evaluation of four STI SOTA approaches: Alligator (formerly s-elbat), Dagobah, TURL, and TableLlama; the first two belong to the family of heuristic-based algorithms, while the others are respectively encoder-only and decoder-only Large Language Models (LLMs). We also include in the evaluation both GPT-4o and GPT-4o-mini, since they excel in various public benchmarks. The primary objective is to measure the ability of these approaches to solve the entity disambiguation task with respect to both the performance achieved on a common-ground evaluation setting and the computational and cost requirements involved, with the ultimate aim of charting new research paths in the field.",
        "published": "2024-10-31 04:00:00",
        "id": "26888668-a5cd-4660-aafe-4faeb06e8a0f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出对四种STI SOTA方法（Alligator、Dagobah、TURL、TableLlama以及GPT - 4o和GPT - 4o - mini）进行广泛评估，旨在衡量这些方法在实体消歧任务中的能力，包括性能、计算和成本要求等，为该领域研究开辟新路径。"
        },
        "tokens": 865
    },
    {
        "title": "LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description",
        "link": "https://arxiv.org/abs/2408.04957",
        "description": "arXiv:2408.04957v4 Announce Type: replace \nAbstract: Visual Spatial Description (VSD) aims to generate texts that describe the spatial relationships between objects within images. Traditional visual spatial relationship classification (VSRC) methods typically output the spatial relationship between two objects in an image, often neglecting world knowledge and lacking general language capabilities. In this paper, we propose a Large Language-and-Vision Assistant for Visual Spatial Description, named LLaVA-VSD, which is designed for the classification, description, and open-ended description of visual spatial relationships. Specifically, the model first constructs a VSD instruction-following dataset using given figure-caption pairs for the three tasks. It then employs LoRA to fine-tune a Large Language and Vision Assistant for VSD, which has 13 billion parameters and supports high-resolution images. Finally, a large language model (Qwen-2) is used to refine the generated sentences, enhancing their diversity and accuracy. LLaVA-VSD demonstrates excellent multimodal conversational capabilities and can follow open-ended instructions to assist with inquiries about object relationships in images.",
        "published": "2024-10-31 04:00:00",
        "id": "27123a1b-15d7-4bd6-b395-19cb2d8907a6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为LLaVA-VSD的大型语言 - 视觉助手用于视觉空间描述，包括构建数据集、微调模型、用大语言模型优化句子等，展现多模态对话能力。"
        },
        "tokens": 812
    },
    {
        "title": "Learning a robust shape parameter for RBF approximation",
        "link": "https://arxiv.org/abs/2408.05081",
        "description": "arXiv:2408.05081v2 Announce Type: replace \nAbstract: Radial basis functions (RBFs) play an important role in function interpolation, in particular in an arbitrary set of interpolation nodes. The accuracy of the interpolation depends on a parameter called the shape parameter. There are many approaches in literature on how to appropriately choose it as to increase the accuracy of interpolation while avoiding instability issues. However, finding the optimal shape parameter value in general remains a challenge. In this work, we present a novel approach to determine the shape parameter in RBFs. First, we construct an optimisation problem to obtain a shape parameter that leads to an interpolation matrix with bounded condition number, then, we introduce a data-driven method that controls the condition of the interpolation matrix to avoid numerically unstable interpolations, while keeping a very good accuracy. In addition, a fall-back procedure is proposed to enforce a strict upper bound on the condition number, as well as a learning strategy to improve the performance of the data-driven method by learning from previously run simulations. We present numerical test cases to assess the performance of the proposed methods in interpolation tasks and in a RBF based finite difference (RBF-FD) method, in one and two-space dimensions.",
        "published": "2024-10-31 04:00:00",
        "id": "4d3673cd-1003-41b4-9b58-c97be15bc441",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种确定径向基函数（RBFs）中形状参数的新方法，构建优化问题获取有界条件数的形状参数，引入数据驱动方法控制插值矩阵条件以平衡稳定性和精度，还提出回退程序和学习策略，并通过数值测试评估其在插值任务和RBF - FD方法中的性能。"
        },
        "tokens": 860
    },
    {
        "title": "Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications",
        "link": "https://arxiv.org/abs/2408.05148",
        "description": "arXiv:2408.05148v3 Announce Type: replace \nAbstract: Run to run variability in parallel programs caused by floating-point non-associativity has been known to significantly affect reproducibility in iterative algorithms, due to accumulating errors. Non-reproducibility can critically affect the efficiency and effectiveness of correctness testing for stochastic programs. Recently, the sensitivity of deep learning training and inference pipelines to floating-point non-associativity has been found to sometimes be extreme. It can prevent certification for commercial applications, accurate assessment of robustness and sensitivity, and bug detection. New approaches in scientific computing applications have coupled deep learning models with high-performance computing, leading to an aggravation of debugging and testing challenges. Here we perform an investigation of the statistical properties of floating-point non-associativity within modern parallel programming models, and analyze performance and productivity impacts of replacing atomic operations with deterministic alternatives on GPUs. We examine the recently-added deterministic options in PyTorch within the context of GPU deployment for deep learning, uncovering and quantifying the impacts of input parameters triggering run to run variability and reporting on the reliability and completeness of the documentation. Finally, we evaluate the strategy of exploiting automatic determinism that could be provided by deterministic hardware, using the Groq accelerator for inference portions of the deep learning pipeline. We demonstrate the benefits that a hardware-based strategy can provide within reproducibility and correctness efforts.",
        "published": "2024-10-31 04:00:00",
        "id": "32a0cef4-aded-4834-bd7a-f2f84a505495",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究浮点非结合性对高性能计算和深度学习应用可重复性的影响，分析用确定性操作替代原子操作对GPU的影响，探究PyTorch确定性选项并量化影响，评估利用确定性硬件提供自动确定性策略的好处。"
        },
        "tokens": 875
    },
    {
        "title": "2D-OOB: Attributing Data Contribution Through Joint Valuation Framework",
        "link": "https://arxiv.org/abs/2408.03572",
        "description": "arXiv:2408.03572v2 Announce Type: replace \nAbstract: Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model. However, it is crucial to recognize that the quality of cells within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar score assigned by existing data valuation methods blurs the distinction between noisy and clean cells of a data point, making it challenging to interpret the data values. In this paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art performance across multiple use cases while being exponentially faster. Specifically, 2D-OOB shows promising results in detecting and rectifying fine-grained outliers at the cell level, and localizing backdoor triggers in data poisoning attacks.",
        "published": "2024-10-31 04:00:00",
        "id": "62e05dec-b38e-4708-a6a7-c23b18e69163",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出2D - OOB框架，用于联合确定有用（或有害）样本及驱动它们的特定单元，实验表明该框架在多个用例中性能达顶尖水平且速度更快，在检测和纠正单元级细粒度异常值、定位数据中毒攻击中的后门触发器方面有前景。"
        },
        "tokens": 824
    },
    {
        "title": "Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion Models",
        "link": "https://arxiv.org/abs/2408.06646",
        "description": "arXiv:2408.06646v2 Announce Type: replace \nAbstract: Stable Diffusion Models (SDMs) have shown remarkable proficiency in image synthesis. However, their broad application is impeded by their large model sizes and intensive computational requirements, which typically require expensive cloud servers for deployment. On the flip side, while there are many compact models tailored for edge devices that can reduce these demands, they often compromise on semantic integrity and visual quality when compared to full-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative, training-free SDMs inference framework designed for edge-cloud collaborative inference. Hybrid SD distributes the early steps of the diffusion process to the large models deployed on cloud servers, enhancing semantic planning. Furthermore, small efficient models deployed on edge devices can be integrated for refining visual details in the later stages. Acknowledging the diversity of edge devices with differing computational and storage capacities, we employ structural pruning to the SDMs U-Net and train a lightweight VAE. Empirical evaluations demonstrate that our compressed models achieve state-of-the-art parameter efficiency (225.8M) on edge devices with competitive image quality. Additionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud collaborative inference.",
        "published": "2024-10-31 04:00:00",
        "id": "eb066d1e-80a3-4ab8-9ee0-12a432ded35f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "Hybrid SD是一种用于边缘 - 云协同推理的无训练稳定扩散模型推理框架，可将扩散过程的早期步骤分配到云服务器上的大型模型，边缘设备上的小型模型用于后期细化视觉细节，经结构剪枝和训练轻量级VAE，压缩模型在边缘设备上实现了高效参数和较好图像质量，还能降低云成本。"
        },
        "tokens": 882
    },
    {
        "title": "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models",
        "link": "https://arxiv.org/abs/2408.09053",
        "description": "arXiv:2408.09053v2 Announce Type: replace \nAbstract: Parameter-efficient fine-tuning (PEFT) methods are increasingly used with pre-trained language models (PLMs) for continual learning (CL). These methods typically involve training a PEFT module for each new task and employing similarity-based selection to route modules during inference. However, they face two major limitations: 1) interference during module training with already learned modules and 2) suboptimal routing when composing modules. In this paper, we present L2R, a method that isolates the training of new PEFT modules to ensure their task specialization. L2R then learns to compose the learned modules by training a network of routers that leverages a small memory containing examples of previously seen tasks. We evaluate our method in two CL setups using various benchmarks. Our results demonstrate that L2R provides an effective composition of PEFT modules, leading to improved generalization and performance compared to other methods.",
        "published": "2024-10-31 04:00:00",
        "id": "cf573f3b-b8b7-411b-8d80-f3f7eae17257",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出L2R方法，该方法通过隔离新的参数高效微调（PEFT）模块训练确保任务专一性，并训练路由器网络来组合已学习模块，在使用不同基准的两种持续学习（CL）设置下评估，结果显示其能有效组合PEFT模块，相比其他方法提高了泛化能力和性能。"
        },
        "tokens": 813
    },
    {
        "title": "Reward Difference Optimization For Sample Reweighting In Offline RLHF",
        "link": "https://arxiv.org/abs/2408.09385",
        "description": "arXiv:2408.09385v2 Announce Type: replace \nAbstract: With the rapid advances in Large Language Models (LLMs), aligning LLMs with human preferences become increasingly important. Although Reinforcement Learning with Human Feedback (RLHF) proves effective, it is complicated and highly resource-intensive. As such, offline RLHF has been introduced as an alternative solution, which directly optimizes LLMs with ranking losses on a fixed preference dataset. Current offline RLHF only captures the \"ordinal relationship\" between responses, overlooking the crucial aspect of how much one is preferred over the others. To address this issue, we propose a simple yet effective solution called Reward Difference Optimization, shorted as RDO. Specifically, we introduce reward difference coefficients to reweigh sample pairs in offline RLHF. We then develop a difference model which captures rich interactions between a pair of responses for predicting these difference coefficients. Experiments with 7B LLMs on the HH and TL;DR datasets substantiate the effectiveness of our method in both automatic metrics and human evaluation, thereby highlighting its potential for aligning LLMs with human intent and values",
        "published": "2024-10-31 04:00:00",
        "id": "62677604-7632-4e30-98fd-f9548d81fa68",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对离线强化学习人类反馈（RLHF）中的样本重加权问题，提出奖励差异优化（RDO），通过引入奖励差异系数重新权衡样本对，开发差异模型预测系数，经实验验证其在对齐大型语言模型与人类意图和价值观方面的有效性。"
        },
        "tokens": 830
    },
    {
        "title": "A Benchmark for AI-based Weather Data Assimilation",
        "link": "https://arxiv.org/abs/2408.11438",
        "description": "arXiv:2408.11438v2 Announce Type: replace \nAbstract: Recent advancements in Artificial Intelligence (AI) have led to the development of several Large Weather Models (LWMs) that rival State-Of-The-Art (SOTA) Numerical Weather Prediction (NWP) systems. Until now, these models have still relied on traditional NWP-generated analysis fields as input and are far from autonomous. Currently, scientists are increasingly focusing on developing data-driven data assimilation (DA) models for LWMs. To expedite advancements in this field and facilitate the operationalization of data-driven end-to-end weather forecasting systems, we propose DABench, a benchmark constructed by simulated observations, real-world observations, and ERA5 reanalysis. DABench contributes four standard features: (1) sparse and noisy observations provided for both simulated and real-world experiments; (2) a Skillful pre-trained Transformer-based weather prediction model, Sformer, designed to generate background fields while rigorously assessing the impact of assimilation outcomes on predictions; (3) standardized evaluation metrics for the model comparison; (4) a strong DA baseline, 4DVarFormerV2. Our experimental results demonstrate that the end-to-end weather forecasting system, integrating 4DVarFormerV2 and Sformer, can assimilate real-world observations, thereby facilitating a stable DA cycle lasting one year and achieving a skillful forecasting lead time of up to 7 days. The proposed DABench will significantly advance research in AI-based DA, AI-based weather forecasting, and related domains.",
        "published": "2024-10-31 04:00:00",
        "id": "98b51a42-c865-408b-a615-ba06a627aff5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为加速AI数据同化在天气预报领域的发展，提出由模拟观测、真实观测和再分析构建的基准DABench，其有四个标准特征，实验表明整合特定模型的端到端天气预报系统可同化真实观测并实现7天有效预报。"
        },
        "tokens": 911
    },
    {
        "title": "Mutagenesis screen to map the functions of parameters of Large Language Models",
        "link": "https://arxiv.org/abs/2408.11494",
        "description": "arXiv:2408.11494v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have significantly advanced artificial intelligence, excelling in numerous tasks. Although the functionality of a model is inherently tied to its parameters, a systematic method for exploring the connections between the parameters and the functionality are lacking. Models sharing similar structure and parameter counts exhibit significant performance disparities across various tasks, prompting investigations into the varying patterns that govern their performance. We adopted a mutagenesis screen approach inspired by the methods used in biological studies, to investigate Llama2-7b and Zephyr. This technique involved mutating elements within the models' matrices to their maximum or minimum values to examine the relationship between model parameters and their functionalities. Our research uncovered multiple levels of fine structures within both models. Many matrices showed a mixture of maximum and minimum mutations following mutagenesis, but others were predominantly sensitive to one type. Notably, mutations that produced phenotypes, especially those with severe outcomes, tended to cluster along axes. Additionally, the location of maximum and minimum mutations often displayed a complementary pattern on matrix in both models, with the Gate matrix showing a unique two-dimensional asymmetry after rearrangement. In Zephyr, certain mutations consistently resulted in poetic or conversational rather than descriptive outputs. These \"writer\" mutations grouped according to the high-frequency initial word of the output, with a marked tendency to share the row coordinate even when they are in different matrices. Our findings affirm that the mutagenesis screen is an effective tool for deciphering the complexities of large language models and identifying unexpected ways to expand their potential, providing deeper insights into the foundational aspects of AI systems.",
        "published": "2024-10-31 04:00:00",
        "id": "73f373f8-8296-442e-822e-8090f53bf666",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过采用受生物学研究启发的诱变筛选法研究Llama2 - 7b和Zephyr模型，发现模型内存在多级精细结构，突变结果呈现多种规律，证明该方法有助于解读大语言模型的复杂性并挖掘潜力。"
        },
        "tokens": 930
    },
    {
        "title": "On the Credibility of Backdoor Attacks Against Object Detectors in the Physical World",
        "link": "https://arxiv.org/abs/2408.12122",
        "description": "arXiv:2408.12122v2 Announce Type: replace \nAbstract: Object detectors are vulnerable to backdoor attacks. In contrast to classifiers, detectors possess unique characteristics, architecturally and in task execution; often operating in challenging conditions, for instance, detecting traffic signs in autonomous cars. But, our knowledge dominates attacks against classifiers and tests in the \"digital domain\".\n  To address this critical gap, we conducted an extensive empirical study targeting multiple detector architectures and two challenging detection tasks in real-world settings: traffic signs and vehicles. Using the diverse, methodically collected videos captured from driving cars and flying drones, incorporating physical object trigger deployments in authentic scenes, we investigated the viability of physical object-triggered backdoor attacks in application settings.\n  Our findings revealed 8 key insights. Importantly, the prevalent \"digital\" data poisoning method for injecting backdoors into models does not lead to effective attacks against detectors in the real world, although proven effective in classification tasks. We construct a new, cost-efficient attack method, dubbed MORPHING, incorporating the unique nature of detection tasks; ours is remarkably successful in injecting physical object-triggered backdoors, even capable of poisoning triggers with clean label annotations or invisible triggers without diminishing the success of physical object triggered backdoors. We discovered that the defenses curated are ill-equipped to safeguard detectors against such attacks. To underscore the severity of the threat and foster further research, we, for the first time, release an extensive video test set of real-world backdoor attacks. Our study not only establishes the credibility and seriousness of this threat but also serves as a clarion call to the research community to advance backdoor defenses in the context of object detection.",
        "published": "2024-10-31 04:00:00",
        "id": "33370ee5-2862-485b-a4c5-16c47bf6f03a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过在真实世界场景中对多个探测器架构和两个检测任务进行实证研究，发现向探测器注入后门的传统数据中毒方法在现实世界中对探测器无效，构建新攻击方法MORPHING有效，现有防御措施难以抵御此类攻击，并首次发布相关视频测试集。"
        },
        "tokens": 955
    },
    {
        "title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities",
        "link": "https://arxiv.org/abs/2408.13296",
        "description": "arXiv:2408.13296v3 Announce Type: replace \nAbstract: This report examines the fine-tuning of Large Language Models (LLMs), integrating theoretical insights with practical applications. It outlines the historical evolution of LLMs from traditional Natural Language Processing (NLP) models to their pivotal role in AI. A comparison of fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, highlights their applicability to different tasks. The report introduces a structured seven-stage pipeline for fine-tuning LLMs, spanning data preparation, model initialization, hyperparameter tuning, and model deployment. Emphasis is placed on managing imbalanced datasets and optimization techniques. Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half Fine-Tuning are explored for balancing computational efficiency with performance. Advanced techniques such as memory fine-tuning, Mixture of Experts (MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized networks and multi-agent collaboration. The report also examines novel approaches like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), which align LLMs with human preferences, alongside pruning and routing optimizations to improve efficiency. Further sections cover validation frameworks, post-deployment monitoring, and inference optimization, with attention to deploying LLMs on distributed and cloud-based platforms. Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and challenges related to scalability, privacy, and accountability are also addressed. This report offers actionable insights for researchers and practitioners navigating LLM fine-tuning in an evolving landscape.",
        "published": "2024-10-31 04:00:00",
        "id": "9b6f1064-a558-4355-ae3f-01a9b1acf1de",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "报告对大型语言模型（LLMs）的微调进行研究，涵盖多种微调方法、七阶段管道、参数高效方法、高级技术、新方法、验证框架等多方面内容。"
        },
        "tokens": 926
    },
    {
        "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
        "link": "https://arxiv.org/abs/2408.14398",
        "description": "arXiv:2408.14398v3 Announce Type: replace \nAbstract: Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. In this paper, we set out to investigate calibrating the pruning of multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. Our results offer practical suggestions, for example, calibrating in the target language can efficiently retain the language modeling capability but does not necessarily benefit downstream tasks. Through further analysis of latent subspaces, pruning masks, and individual neurons within pruned models, we find that while pruning generally preserves strong language-specific features, it may fail to retain language-specific neuron activation patterns and subtle, language-agnostic features associated with knowledge and reasoning that are needed for complex tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "47de32c5-4144-487f-a4e9-20506340d3b4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "研究对多语言大型语言模型的剪枝进行特定语言校准，比较不同校准语言在多语言模型剪枝中的效果，结果提供了实用建议并通过分析发现了剪枝中的相关特征保留情况。"
        },
        "tokens": 827
    },
    {
        "title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery",
        "link": "https://arxiv.org/abs/2408.15099",
        "description": "arXiv:2408.15099v3 Announce Type: replace \nAbstract: What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula promise to enable agents to be robust to in- and out-of-distribution tasks. This work investigates how existing UED methods select training environments, focusing on task prioritisation metrics. Surprisingly, despite methods aiming to maximise regret in theory, the practical approximations do not correlate with regret but with success rate. As a result, a significant portion of an agent's experience comes from environments it has already mastered, offering little to no contribution toward enhancing its abilities. Put differently, current methods fail to predict intuitive measures of ``learnability.'' Specifically, they are unable to consistently identify those scenarios that the agent can sometimes solve, but not always. Based on our analysis, we develop a method that directly trains on scenarios with high learnability. This simple and intuitive approach outperforms existing UED methods in several binary-outcome environments, including the standard domain of Minigrid and a novel setting closely inspired by a real-world robotics problem. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR). We open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability.",
        "published": "2024-10-31 04:00:00",
        "id": "526a915c-6438-424e-9af5-76ae53a7dcf8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现现有无监督环境设计（UED）方法在选择训练环境时存在问题，开发出直接针对高可学习性场景训练的方法，在几个二元结果环境中表现优于现有UED方法，并引入新的对抗性评估程序。"
        },
        "tokens": 909
    },
    {
        "title": "Real-Time Recurrent Learning using Trace Units in Reinforcement Learning",
        "link": "https://arxiv.org/abs/2409.01449",
        "description": "arXiv:2409.01449v2 Announce Type: replace \nAbstract: Recurrent Neural Networks (RNNs) are used to learn representations in partially observable environments. For agents that learn online and continually interact with the environment, it is desirable to train RNNs with real-time recurrent learning (RTRL); unfortunately, RTRL is prohibitively expensive for standard RNNs. A promising direction is to use linear recurrent architectures (LRUs), where dense recurrent weights are replaced with a complex-valued diagonal, making RTRL efficient. In this work, we build on these insights to provide a lightweight but effective approach for training RNNs in online RL. We introduce Recurrent Trace Units (RTUs), a small modification on LRUs that we nonetheless find to have significant performance benefits over LRUs when trained with RTRL. We find RTUs significantly outperform other recurrent architectures across several partially observable environments while using significantly less computation.",
        "published": "2024-10-31 04:00:00",
        "id": "d795714d-d7f7-4b89-b313-44f107cb745c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文提出循环追踪单元（RTUs），是对线性递归架构（LRUs）的小改进，用实时递归学习（RTRL）训练时性能优于LRUs，在部分可观测环境中表现更优且计算量更小。"
        },
        "tokens": 792
    },
    {
        "title": "Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)",
        "link": "https://arxiv.org/abs/2408.15874",
        "description": "arXiv:2408.15874v3 Announce Type: replace \nAbstract: Outlier detection algorithms typically assign an outlier score to each observation in a dataset, indicating the degree to which an observation is an outlier. However, these scores are often not comparable across algorithms and can be difficult for humans to interpret. Statistical scaling addresses this problem by transforming outlier scores into outlier probabilities without using ground-truth labels, thereby improving interpretability and comparability across algorithms. However, the quality of this transformation can be different for outliers and inliers. Missing outliers in scenarios where they are of particular interest - such as healthcare, finance, or engineering - can be costly or dangerous. Thus, ensuring good probabilities for outliers is essential. This paper argues that statistical scaling, as commonly used in the literature, does not produce equally good probabilities for outliers as for inliers. Therefore, we propose robust statistical scaling, which uses robust estimators to improve the probabilities for outliers. We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers.",
        "published": "2024-10-31 04:00:00",
        "id": "4527e614-98ae-408b-8f83-21d203341077",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出稳健统计缩放方法，通过使用稳健估计器改进异常值概率，解决统计缩放中异常值概率不如正常值概率好的问题，并在真实数据集和异常检测算法中评估该方法。"
        },
        "tokens": 829
    },
    {
        "title": "The Sample-Communication Complexity Trade-off in Federated Q-Learning",
        "link": "https://arxiv.org/abs/2408.16981",
        "description": "arXiv:2408.16981v2 Announce Type: replace \nAbstract: We consider the problem of federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite-horizon Markov decision process with finite state and action spaces. We investigate the trade-off between sample and communication complexities for the widely used class of intermittent communication algorithms. We first establish the converse result, where it is shown that a federated Q-learning algorithm that offers any speedup with respect to the number of agents in the per-agent sample complexity needs to incur a communication cost of at least an order of $\\frac{1}{1-\\gamma}$ up to logarithmic factors, where $\\gamma$ is the discount factor. We also propose a new algorithm, called Fed-DVR-Q, which is the first federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in federated Q-learning.",
        "published": "2024-10-31 04:00:00",
        "id": "15048556-84e9-4bb9-b1d7-998c7b803c4a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究联邦Q - 学习中的样本 - 通信复杂性权衡，先给出逆结果，后提出Fed - DVR - Q算法实现最优样本和通信复杂性。"
        },
        "tokens": 792
    },
    {
        "title": "Unlocking the Wisdom of Large Language Models: An Introduction to The Path to Artificial General Intelligence",
        "link": "https://arxiv.org/abs/2409.01007",
        "description": "arXiv:2409.01007v2 Announce Type: replace \nAbstract: This booklet, \"Unlocking the Wisdom of LLM Collaborative Intelligence,\" introduces the comprehensive work \"The Path to Artificial General Intelligence.\" Through ten aphorisms, it distills the core principles of LLM Collaborative Intelligence (LCI) as a promising framework toward achieving AGI. The booklet also offers titles, abstracts, and introductions from the main chapters, along with the first two chapters in full. The second edition, released this week, includes significant enhancements to Chapters 6 to 9 and a revised preface addressing Yann LeCun's skepticism about AGI. LeCun argues that LLMs lack memory, planning, and grounding, but we propose that LCI's collaborative architecture, involving multimodal LLMs with executive, legislative, and judicial roles, overcomes these limitations. Chapters on SocraSynth, EVINCE, consciousness modeling, and behavior modeling demonstrate that collaborative LLMs with checks and balances can achieve intelligence beyond any single model's capability. By combining complementary strengths, such as world modeling and advanced sensory capabilities, LCI enables models to work together and perceive reality beyond human limitations. As with human institutions, progress depends on cooperation, not isolation. Collaborative LLMs may unlock new levels of intelligence, paving the way toward AGI.",
        "published": "2024-10-31 04:00:00",
        "id": "c4d4c5a5-1aa0-4cfd-837f-5e43b49a7d39",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "The booklet 'Unlocking the Wisdom of LLM Collaborative Intelligence' introduces 'The Path to Artificial General Intelligence' through ten aphorisms, distills LLM Collaborative Intelligence principles, and addresses Yann LeCun's skepticism by proposing that LCI's collaborative architecture overcomes limitations of LLMs."
        },
        "tokens": 886
    },
    {
        "title": "Unifying Model Execution and Deductive Verification with Interaction Trees in Isabelle/HOL",
        "link": "https://arxiv.org/abs/2408.15817",
        "description": "arXiv:2408.15817v2 Announce Type: replace \nAbstract: Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation. On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering. To ensure coherent results between execution and proof, we need unifying semantics and automation. In this paper, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework. ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable. We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and \\Circus languages, and abstract system models in the style of the Z and B methods. We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model. Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models.",
        "published": "2024-10-31 04:00:00",
        "id": "33f9748d-f39e-44db-878e-d2dc43b144c0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文通过在Isabelle/HOL中构建交互树（ITrees）形成执行和验证框架，用于多种程序和模型，展示ITrees对不同语义的解释能力并演示其可执行性以支持模型动画。"
        },
        "tokens": 836
    },
    {
        "title": "Improving Apple Object Detection with Occlusion-Enhanced Distillation",
        "link": "https://arxiv.org/abs/2409.01573",
        "description": "arXiv:2409.01573v2 Announce Type: replace \nAbstract: Apples growing in natural environments often face severe visual obstructions from leaves and branches. This significantly increases the risk of false detections in object detection tasks, thereby escalating the challenge. Addressing this issue, we introduce a technique called \"Occlusion-Enhanced Distillation\" (OED). This approach utilizes occlusion information to regularize the learning of semantically aligned features on occluded datasets and employs Exponential Moving Average (EMA) to enhance training stability. Specifically, we first design an occlusion-enhanced dataset that integrates Grounding DINO and SAM methods to extract occluding elements such as leaves and branches from each sample, creating occlusion examples that reflect the natural growth state of fruits. Additionally, we propose a multi-scale knowledge distillation strategy, where the student network uses images with increased occlusions as inputs, while the teacher network employs images without natural occlusions. Through this setup, the strategy guides the student network to learn from the teacher across scales of semantic and local features alignment, effectively narrowing the feature distance between occluded and non-occluded targets and enhancing the robustness of object detection. Lastly, to improve the stability of the student network, we introduce the EMA strategy, which aids the student network in learning more generalized feature expressions that are less affected by the noise of individual image occlusions. Our method significantly outperforms current state-of-the-art techniques through extensive comparative experiments.",
        "published": "2024-10-31 04:00:00",
        "id": "92316012-5008-4f81-845f-dcbde1bcf45f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决自然环境中苹果检测易受遮挡影响的问题，提出遮挡增强蒸馏（OED）技术，设计遮挡增强数据集、多尺度知识蒸馏策略并引入EMA策略以提升目标检测鲁棒性并超越现有技术。"
        },
        "tokens": 890
    },
    {
        "title": "The Prevalence of Neural Collapse in Neural Multivariate Regression",
        "link": "https://arxiv.org/abs/2409.04180",
        "description": "arXiv:2409.04180v2 Announce Type: replace \nAbstract: Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the $n$ principal components of the feature vectors, where $n$ is the dimension of the targets (for univariate regression, $n=1$); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.",
        "published": "2024-10-31 04:00:00",
        "id": "bc776e72-cd2d-4f85-9a38-cd6d8a4085b0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "研究表明神经网络在多元回归任务中存在神经回归崩溃现象，从经验和理论上进行了研究，并通过无约束特征模型解释了该现象。"
        },
        "tokens": 941
    },
    {
        "title": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid Interconnects",
        "link": "https://arxiv.org/abs/2409.05404",
        "description": "arXiv:2409.05404v2 Announce Type: replace \nAbstract: Emerging interconnects, such as CXL and NVLink, have been integrated into the intra-host topology to scale more accelerators and facilitate efficient communication between them, such as GPUs. To keep pace with the accelerator's growing computing throughput, the interconnect has seen substantial enhancement in link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet and InfiniBand network links by an order of magnitude or more. Consequently, when data-intensive jobs, such as LLM training, scale across multiple hosts beyond the reach limit of the interconnect, the performance is significantly hindered by the limiting bandwidth of the network infrastructure. We address the problem by proposing DFabric, a two-tier interconnect architecture. We address the problem by proposing DFabric, a two-tier interconnect architecture. First, DFabric disaggregates rack's computing units with an interconnect fabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy intra-rack efficient interconnecting. Second, DFabric disaggregates NICs from hosts, and consolidates them to form a NIC pool with CXL fabric. By providing sufficient aggregated capacity comparable to interconnect bandwidth, the NIC pool bridges efficient communication across racks or beyond the reach limit of interconnect fabric. However, the local memory accessing becomes the bottleneck when enabling each host to utilize the NIC pool efficiently. To the end, DFabric builds a memory pool with sufficient bandwidth by disaggregating host local memory and adding more memory devices. We have implemented a prototype of DFabric that can run applications transparently. We validated its performance gain by running various microbenchmarks and compute-intensive applications such as DNN and graph.",
        "published": "2024-10-31 04:00:00",
        "id": "ad098c14-0a8e-460f-a1b4-12f99cc3de61",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出DFabric这一两层互连架构，通过多种方式解决数据密集型工作在多主机扩展时受网络基础设施带宽限制的问题，并已实现可透明运行应用的原型，通过微基准测试和计算密集型应用验证了性能增益。"
        },
        "tokens": 972
    },
    {
        "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
        "link": "https://arxiv.org/abs/2409.05798",
        "description": "arXiv:2409.05798v3 Announce Type: replace \nAbstract: Interactive preference learning systems present humans with queries as pairs of options; humans then select their preferred choice, allowing the system to infer preferences from these binary choices. While binary choice feedback is simple and widely used, it offers limited information about preference strength. To address this, we leverage human response times, which inversely correlate with preference strength, as complementary information. We introduce a computationally efficient method based on the EZ-diffusion model, combining choices and response times to estimate the underlying human utility function. Theoretical and empirical comparisons with traditional choice-only estimators show that for queries where humans have strong preferences (i.e., \"easy\" queries), response times provide valuable complementary information and enhance utility estimates. We integrate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that incorporating response times significantly accelerates preference learning.",
        "published": "2024-10-31 04:00:00",
        "id": "b4d7b4f8-92e8-48ab-9105-5e4032bdd3a5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究利用人类响应时间与偏好强度的反向关联补充信息，提出基于EZ - 扩散模型的高效计算方法，理论和实证对比显示在特定查询中响应时间可增强效用估计，将其集成到偏好线性老虎机用于固定预算最优臂识别，模拟显示纳入响应时间可加速偏好学习。"
        },
        "tokens": 804
    },
    {
        "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
        "link": "https://arxiv.org/abs/2409.06691",
        "description": "arXiv:2409.06691v2 Announce Type: replace \nAbstract: Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic. However, human preferences can vary across individuals, and therefore should be represented distributionally. In this work, we introduce the distributional soft preference labels and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function. This approach adjusts the scale of learning loss based on the soft labels such that the loss would approach zero when the responses are closer to equally preferred. This simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from. Our experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.",
        "published": "2024-10-31 04:00:00",
        "id": "fcf415dd-64bc-48ad-8011-6db600f863ef",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍分布性软偏好标签，通过在损失函数中对LLM输出似然加权几何平均改进直接偏好优化（DPO），实验表明几何平均可提升性能。"
        },
        "tokens": 783
    },
    {
        "title": "Implicit Reasoning in Deep Time Series Forecasting",
        "link": "https://arxiv.org/abs/2409.10840",
        "description": "arXiv:2409.10840v3 Announce Type: replace \nAbstract: Recently, time series foundation models have shown promising zero-shot forecasting performance on time series from a wide range of domains. However, it remains unclear whether their success stems from a true understanding of temporal dynamics or simply from memorizing the training data. While implicit reasoning in language models has been studied, similar evaluations for time series models have been largely unexplored. This work takes an initial step toward assessing the reasoning abilities of deep time series forecasting models. We find that certain linear, MLP-based, and patch-based Transformer models generalize effectively in systematically orchestrated out-of-distribution scenarios, suggesting underexplored reasoning capabilities beyond simple pattern memorization.",
        "published": "2024-10-31 04:00:00",
        "id": "d91d0cb0-ccf8-4acd-b6f2-7cd1be5ccafd",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文是对深度时间序列预测模型推理能力的初步评估，发现部分线性、基于MLP和基于补丁的Transformer模型在系统安排的分布外场景中有效泛化，表明其具备超越简单模式记忆的推理能力。"
        },
        "tokens": 734
    },
    {
        "title": "Still More Shades of Null: An Evaluation Suite for Responsible Missing Value Imputation",
        "link": "https://arxiv.org/abs/2409.07510",
        "description": "arXiv:2409.07510v2 Announce Type: replace \nAbstract: Data missingness is a practical challenge of sustained interest to the scientific community. In this paper, we present Shades-of-Null, an evaluation suite for responsible missing value imputation. Our work is novel in two ways (i) we model realistic and socially-salient missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR) settings, to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test) (ii) we evaluate imputers holistically, based on imputation quality, as well as on the predictive performance, fairness and stability of the models that are trained and tested on the data post-imputation.\n  We use Shades-of-Null to conduct a large-scale empirical study involving 23,940 experimental pipelines, and find that while there is no single best-performing imputation approach for all missingness types, interesting trade-offs arise between predictive performance, fairness and stability, based on the combination of missingness scenario, imputer choice, and the architecture of the predictive model. We make Shades-of-Null publicly available, to enable researchers to rigorously evaluate missing value imputation methods on a wide range of metrics in plausible and socially meaningful scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "d3d8805b-2098-4b05-8019-0ea6358df9f1",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出用于缺失值合理插补的评估套件Shades - of - Null，通过大规模实验研究发现不同缺失类型下预测性能、公平性和稳定性间存在权衡，并公开该套件。"
        },
        "tokens": 893
    },
    {
        "title": "L3Cube-IndicQuest: A Benchmark Question Answering Dataset for Evaluating Knowledge of LLMs in Indic Context",
        "link": "https://arxiv.org/abs/2409.08706",
        "description": "arXiv:2409.08706v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have made significant progress in incorporating Indic languages within multilingual models. However, it is crucial to quantitatively assess whether these languages perform comparably to globally dominant ones, such as English. Currently, there is a lack of benchmark datasets specifically designed to evaluate the regional knowledge of LLMs in various Indic languages. In this paper, we present the L3Cube-IndicQuest, a gold-standard factual question-answering benchmark dataset designed to evaluate how well multilingual LLMs capture regional knowledge across various Indic languages. The dataset contains 200 question-answer pairs, each for English and 19 Indic languages, covering five domains specific to the Indic region. We aim for this dataset to serve as a benchmark, providing ground truth for evaluating the performance of LLMs in understanding and representing knowledge relevant to the Indian context. The IndicQuest can be used for both reference-based evaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at https://github.com/l3cube-pune/indic-nlp .",
        "published": "2024-10-31 04:00:00",
        "id": "81377260-d241-49c9-b322-3fe0cb82ed16",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "L3Cube - IndicQuest是一个基准问答数据集，用于评估多语言大型语言模型（LLMs）在印度地区各种印度语中的区域知识，包含200个问答对，涵盖五个特定领域，可用于参考评估和LLM作为评判者的评估，数据集已公开共享。"
        },
        "tokens": 849
    },
    {
        "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
        "link": "https://arxiv.org/abs/2409.10372",
        "description": "arXiv:2409.10372v3 Announce Type: replace \nAbstract: This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.",
        "published": "2024-10-31 04:00:00",
        "id": "cae379b0-6b82-46d9-b8fe-ce0391201031",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种新框架，将大型语言模型（LLM）智能体作为人类策略行为的代理与强化学习（RL）相结合，通过验证迭代游戏，证明其有助于提高合作率并提供对AI介导社会动态的见解。"
        },
        "tokens": 763
    },
    {
        "title": "Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective",
        "link": "https://arxiv.org/abs/2409.07388",
        "description": "arXiv:2409.07388v2 Announce Type: replace \nAbstract: Multimodal affective computing (MAC) has garnered increasing attention due to its broad applications in analyzing human behaviors and intentions, especially in text-dominated multimodal affective computing field. This survey presents the recent trends of multimodal affective computing from NLP perspective through four hot tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis and multimodal multi-label emotion recognition. The goal of this survey is to explore the current landscape of multimodal affective research, identify development trends, and highlight the similarities and differences across various tasks, offering a comprehensive report on the recent progress in multimodal affective computing from an NLP perspective. This survey covers the formalization of tasks, provides an overview of relevant works, describes benchmark datasets, and details the evaluation metrics for each task. Additionally, it briefly discusses research in multimodal affective computing involving facial expressions, acoustic signals, physiological signals, and emotion causes. Additionally, we discuss the technical approaches, challenges, and future directions in multimodal affective computing. To support further research, we released a repository that compiles related works in multimodal affective computing, providing detailed resources and references for the community.",
        "published": "2024-10-31 04:00:00",
        "id": "588f085e-578b-4b05-a004-644d3d635a4a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "从NLP视角阐述多模态情感计算近期趋势的综述，涵盖多模态情感计算的四个热门任务、相关工作、数据集、评估指标、技术方法、挑战和未来方向等，并发布相关资源库。"
        },
        "tokens": 847
    },
    {
        "title": "GStex: Per-Primitive Texturing of 2D Gaussian Splatting for Decoupled Appearance and Geometry Modeling",
        "link": "https://arxiv.org/abs/2409.12954",
        "description": "arXiv:2409.12954v2 Announce Type: replace \nAbstract: Gaussian splatting has demonstrated excellent performance for view synthesis and scene reconstruction. The representation achieves photorealistic quality by optimizing the position, scale, color, and opacity of thousands to millions of 2D or 3D Gaussian primitives within a scene. However, since each Gaussian primitive encodes both appearance and geometry, these attributes are strongly coupled--thus, high-fidelity appearance modeling requires a large number of Gaussian primitives, even when the scene geometry is simple (e.g., for a textured planar surface). We propose to texture each 2D Gaussian primitive so that even a single Gaussian can be used to capture appearance details. By employing per-primitive texturing, our appearance representation is agnostic to the topology and complexity of the scene's geometry. We show that our approach, GStex, yields improved visual quality over prior work in texturing Gaussian splats. Furthermore, we demonstrate that our decoupling enables improved novel view synthesis performance compared to 2D Gaussian splatting when reducing the number of Gaussian primitives, and that GStex can be used for scene appearance editing and re-texturing.",
        "published": "2024-10-31 04:00:00",
        "id": "45e564f3-7002-461f-b960-f08cae08ddb0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "GStex通过对2D高斯图元进行纹理处理，将外观与几何属性解耦，在纹理高斯图元方面视觉质量更好，减少图元数量时能提高新视图合成性能，还可用于场景外观编辑和重新纹理化。"
        },
        "tokens": 852
    },
    {
        "title": "Knowledge in Triples for LLMs: Enhancing Table QA Accuracy with Semantic Extraction",
        "link": "https://arxiv.org/abs/2409.14192",
        "description": "arXiv:2409.14192v2 Announce Type: replace \nAbstract: Integrating structured knowledge from tabular formats poses significant challenges within natural language processing (NLP), mainly when dealing with complex, semi-structured tables like those found in the FeTaQA dataset. These tables require advanced methods to interpret and generate meaningful responses accurately. Traditional approaches, such as SQL and SPARQL, often fail to fully capture the semantics of such data, especially in the presence of irregular table structures like web tables. This paper addresses these challenges by proposing a novel approach that extracts triples straightforward from tabular data and integrates it with a retrieval-augmented generation (RAG) model to enhance the accuracy, coherence, and contextual richness of responses generated by a fine-tuned GPT-3.5-turbo-0125 model. Our approach significantly outperforms existing baselines on the FeTaQA dataset, particularly excelling in Sacre-BLEU and ROUGE metrics. It effectively generates contextually accurate and detailed long-form answers from tables, showcasing its strength in complex data interpretation.",
        "published": "2024-10-31 04:00:00",
        "id": "59f1e95f-d7b9-49b4-8cc1-a65458e6cf67",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种从表格数据中提取三元组并与检索增强生成模型集成的新方法，以提高GPT - 3.5 - turbo - 0125模型在FeTaQA数据集上回答表格问答的准确性、连贯性和上下文丰富性，效果优于现有基准。"
        },
        "tokens": 831
    },
    {
        "title": "Uncovering Coordinated Cross-Platform Information Operations Threatening the Integrity of the 2024 U.S. Presidential Election Online Discussion",
        "link": "https://arxiv.org/abs/2409.15402",
        "description": "arXiv:2409.15402v2 Announce Type: replace \nAbstract: Information Operations (IOs) pose a significant threat to the integrity of democratic processes, with the potential to influence election-related online discourse. In anticipation of the 2024 U.S. presidential election, we present a study aimed at uncovering the digital traces of coordinated IOs on $\\mathbb{X}$ (formerly Twitter). Using our machine learning framework for detecting online coordination, we analyze a dataset comprising election-related conversations on $\\mathbb{X}$ from May 2024. This reveals a network of coordinated inauthentic actors, displaying notable similarities in their link-sharing behaviors. Our analysis shows concerted efforts by these accounts to disseminate misleading, redundant, and biased information across the Web through a coordinated cross-platform information operation: The links shared by this network frequently direct users to other social media platforms or suspicious websites featuring low-quality political content and, in turn, promoting the same $\\mathbb{X}$ and YouTube accounts. Members of this network also shared deceptive images generated by AI, accompanied by language attacking political figures and symbolic imagery intended to convey power and dominance. While $\\mathbb{X}$ has suspended a subset of these accounts, more than 75% of the coordinated network remains active. Our findings underscore the critical role of developing computational models to scale up the detection of threats on large social media platforms, and emphasize the broader implications of these techniques to detect IOs across the wider Web.",
        "published": "2024-10-31 04:00:00",
        "id": "0fd63751-acb8-4855-91b9-4c8216eb1f0d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对2024年美国总统选举，研究人员利用机器学习框架分析推特选举相关话题，发现协调不真实行为者网络在跨平台信息操作中传播误导性内容，多数仍活跃，凸显开发计算模型检测威胁的重要性。"
        },
        "tokens": 916
    },
    {
        "title": "M$^2$PT: Multimodal Prompt Tuning for Zero-shot Instruction Learning",
        "link": "https://arxiv.org/abs/2409.15657",
        "description": "arXiv:2409.15657v4 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (M$^2$PT) approach for efficient instruction tuning of MLLMs. M$^2$PT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.",
        "published": "2024-10-31 04:00:00",
        "id": "1c387744-63ce-439f-ab8a-ac20d77eaa9e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新的多模态提示调整（M²PT）方法用于多模态大语言模型（MLLMs）的有效指令调整，实证结果显示其性能优于一些现有基线。"
        },
        "tokens": 827
    },
    {
        "title": "Self-Supervised Graph Embedding Clustering",
        "link": "https://arxiv.org/abs/2409.15887",
        "description": "arXiv:2409.15887v2 Announce Type: replace \nAbstract: The K-means one-step dimensionality reduction clustering method has made some progress in addressing the curse of dimensionality in clustering tasks. However, it combines the K-means clustering and dimensionality reduction processes for optimization, leading to limitations in the clustering effect due to the introduced hyperparameters and the initialization of clustering centers. Moreover, maintaining class balance during clustering remains challenging. To overcome these issues, we propose a unified framework that integrates manifold learning with K-means, resulting in the self-supervised graph embedding framework. Specifically, we establish a connection between K-means and the manifold structure, allowing us to perform K-means without explicitly defining centroids. Additionally, we use this centroid-free K-means to generate labels in low-dimensional space and subsequently utilize the label information to determine the similarity between samples. This approach ensures consistency between the manifold structure and the labels. Our model effectively achieves one-step clustering without the need for redundant balancing hyperparameters. Notably, we have discovered that maximizing the $\\ell_{2,1}$-norm naturally maintains class balance during clustering, a result that we have theoretically proven. Finally, experiments on multiple datasets demonstrate that the clustering results of Our-LPP and Our-MFA exhibit excellent and reliable performance.",
        "published": "2024-10-31 04:00:00",
        "id": "020fb6aa-55bc-46f2-a71e-1c86556b810f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为克服K - means一步降维聚类方法的局限性，提出将流形学习与K - means集成的自监督图嵌入框架，理论证明最大化l2,1 - 范数可在聚类时保持类平衡，实验表明相关聚类结果性能良好可靠。"
        },
        "tokens": 862
    },
    {
        "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
        "link": "https://arxiv.org/abs/2409.17515",
        "description": "arXiv:2409.17515v3 Announce Type: replace \nAbstract: This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.",
        "published": "2024-10-31 04:00:00",
        "id": "d630a563-e4c5-416c-847c-8c18b90d8d15",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出一种新方法，利用大型语言模型和生成式代理通过跨文本和时间序列数据推理来增强时间序列预测，结果表明在预测准确性上有显著提高。"
        },
        "tokens": 790
    },
    {
        "title": "ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue",
        "link": "https://arxiv.org/abs/2409.17610",
        "description": "arXiv:2409.17610v2 Announce Type: replace \nAbstract: The rocketing prosperity of large language models (LLMs) in recent years has boosted the prevalence of vision-language models (VLMs) in the medical sector. In our online medical consultation scenario, a doctor responds to the texts and images provided by a patient in multiple rounds to diagnose her/his health condition, forming a multi-turn multimodal medical dialogue format. Unlike high-quality images captured by professional equipment in traditional medical visual question answering (Med-VQA), the images in our case are taken by patients' mobile phones. These images have poor quality control, with issues such as excessive background elements and the lesion area being significantly off-center, leading to degradation of vision-language alignment in the model training phase. In this paper, we propose ZALM3, a Zero-shot strategy to improve vision-language ALignment in Multi-turn Multimodal Medical dialogue. Since we observe that the preceding text conversations before an image can infer the regions of interest (RoIs) in the image, ZALM3 employs an LLM to summarize the keywords from the preceding context and a visual grounding model to extract the RoIs. The updated images eliminate unnecessary background noise and provide more effective vision-language alignment. To better evaluate our proposed method, we design a new subjective assessment metric for multi-turn unimodal/multimodal medical dialogue to provide a fine-grained performance comparison. Our experiments across three different clinical departments remarkably demonstrate the efficacy of ZALM3 with statistical significance.",
        "published": "2024-10-31 04:00:00",
        "id": "b8d6eb0f-4b0d-4def-b7b4-5648c90258c5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对多轮多模态医疗对话中图像质量差影响视觉 - 语言对齐的问题，提出ZALM3零射击策略，通过LLM和视觉基础模型处理图像以改善对齐，还设计了新主观评估指标，实验证明其有效性。"
        },
        "tokens": 923
    },
    {
        "title": "Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling",
        "link": "https://arxiv.org/abs/2409.18269",
        "description": "arXiv:2409.18269v2 Announce Type: replace \nAbstract: Prophet inequality concerns a basic optimal stopping problem and states that simple threshold stopping policies -- i.e., accepting the first reward larger than a certain threshold -- can achieve tight $\\frac{1}{2}$-approximation to the optimal prophet value. Motivated by its economic applications, this paper studies the robustness of this approximation to natural strategic manipulations in which each random reward is associated with a self-interested player who may selectively reveal his realized reward to the searcher in order to maximize his probability of being selected.\n  We say a threshold policy is $\\alpha$(-strategically)-robust if it (a) achieves the $\\alpha$-approximation to the prophet value for strategic players; and (b) meanwhile remains a $\\frac{1}{2}$-approximation in the standard non-strategic setting. Starting with a characterization of each player's optimal information revealing strategy, we demonstrate the intrinsic robustness of prophet inequalities to strategic reward signaling through the following results: (1) for arbitrary reward distributions, there is a threshold policy that is $\\frac{1-\\frac{1}{e}}{2}$-robust, and this ratio is tight; (2) for i.i.d. reward distributions, there is a threshold policy that is $\\frac{1}{2}$-robust, which is tight for the setting; and (3) for log-concave (but non-identical) reward distributions, the $\\frac{1}{2}$-robustness can also be achieved under certain regularity assumptions.",
        "published": "2024-10-31 04:00:00",
        "id": "78822d3c-2f48-4012-a2c7-056ecc22496a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": false,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 0
            },
            "keyFacts": "论文研究先知不等式对策略性奖励信号的内在鲁棒性，包括对任意奖励分布、独立同分布奖励分布、对数凹奖励分布下的相关情况。"
        },
        "tokens": 921
    },
    {
        "title": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
        "link": "https://arxiv.org/abs/2409.20288",
        "description": "arXiv:2409.20288v3 Announce Type: replace \nAbstract: Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain. However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice. To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval. This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application. We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at \\url{https://github.com/CSHaitao/LexEval} and will be continuously updated.",
        "published": "2024-10-31 04:00:00",
        "id": "f525c553-6c54-4275-ae26-0be3243d6e0a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一个名为LexEval的中文法律基准，包含23个任务和14150个问题，用于评估大型语言模型在法律领域的能力，对38个开源和商业LLMs进行了评估并得出有趣发现，数据集和排行榜公开且会持续更新。"
        },
        "tokens": 907
    },
    {
        "title": "Causal Deciphering and Inpainting in Spatio-Temporal Dynamics via Diffusion Model",
        "link": "https://arxiv.org/abs/2409.19608",
        "description": "arXiv:2409.19608v2 Announce Type: replace \nAbstract: Spatio-temporal (ST) prediction has garnered a De facto attention in earth sciences, such as meteorological prediction, human mobility perception. However, the scarcity of data coupled with the high expenses involved in sensor deployment results in notable data imbalances. Furthermore, models that are excessively customized and devoid of causal connections further undermine the generalizability and interpretability. To this end, we establish a causal framework for ST predictions, termed CaPaint, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Going beyond this process, we utilize the back-door adjustment to specifically address the sub-regions identified as non-causal in the upstream phase. Specifically, we employ a novel image inpainting technique. By using a fine-tuned unconditional Diffusion Probabilistic Model (DDPM) as the generative prior, we in-fill the masks defined as environmental parts, offering the possibility of reliable extrapolation for potential data distributions. CaPaint overcomes the high complexity dilemma of optimal ST causal discovery models by reducing the data generation complexity from exponential to quasi-linear levels. Extensive experiments conducted on five real-world ST benchmarks demonstrate that integrating the CaPaint concept allows models to achieve improvements ranging from 4.3% to 77.3%. Moreover, compared to traditional mainstream ST augmenters, CaPaint underscores the potential of diffusion models in ST enhancement, offering a novel paradigm for this field. Our project is available at https://anonymous.4open.science/r/12345-DFCC.",
        "published": "2024-10-31 04:00:00",
        "id": "9044f2c6-34df-4698-989c-841817a17821",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为CaPaint的时空预测因果框架，利用后门调整和新的图像修复技术，通过实验证明其可提升模型性能，克服最优时空因果发现模型的高复杂性困境。"
        },
        "tokens": 924
    },
    {
        "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs",
        "link": "https://arxiv.org/abs/2409.19759",
        "description": "arXiv:2409.19759v3 Announce Type: replace \nAbstract: As large language models (LLMs) are applied to more use cases, creating high quality, task-specific datasets for fine-tuning becomes a bottleneck for model improvement. Using high quality human data has been the most common approach to unlock model performance, but is prohibitively expensive in many scenarios. Several alternative methods have also emerged, such as generating synthetic or hybrid data, but the effectiveness of these approaches remain unclear, especially in resource-constrained scenarios and tasks that are not easily verified. To investigate this, we group various synthetic data generation strategies into three representative categories -- Answer Augmentation, Question Rephrase and New Question -- and study the performance of student LLMs trained under various constraints, namely seed instruction set size and query budget. We demonstrate that these strategies are not equally effective across settings. Notably, the optimal data generation strategy depends strongly on the ratio between the available teacher query budget and the size of the seed instruction set. When this ratio is low, generating new answers to existing questions proves most effective, but as this ratio increases, generating new questions becomes optimal. Across all tasks, we find that choice of augmentation method and other design choices matter substantially more in low to mid data regimes than in high data regimes. We provide a practical framework for selecting the appropriate augmentation method across settings, taking into account additional factors such as the scalability of each method, the importance of verifying synthetic data, and the use of different LLMs for synthetic data generation.",
        "published": "2024-10-31 04:00:00",
        "id": "6abbd80b-07e0-4413-b501-4fbe749570de",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究大型语言模型(LLMs)不同合成数据生成策略在不同约束下的有效性，将策略分为三类，发现最优策略取决于教师查询预算与种子指令集大小的比率，且在低到中数据量时策略选择更重要，还提供了选择策略的框架。"
        },
        "tokens": 913
    },
    {
        "title": "CycleCrash: A Dataset of Bicycle Collision Videos for Collision Prediction and Analysis",
        "link": "https://arxiv.org/abs/2409.19942",
        "description": "arXiv:2409.19942v2 Announce Type: replace \nAbstract: Self-driving research often underrepresents cyclist collisions and safety. To address this, we present CycleCrash, a novel dataset consisting of 3,000 dashcam videos with 436,347 frames that capture cyclists in a range of critical situations, from collisions to safe interactions. This dataset enables 9 different cyclist collision prediction and classification tasks focusing on potentially hazardous conditions for cyclists and is annotated with collision-related, cyclist-related, and scene-related labels. Next, we propose VidNeXt, a novel method that leverages a ConvNeXt spatial encoder and a non-stationary transformer to capture the temporal dynamics of videos for the tasks defined in our dataset. To demonstrate the effectiveness of our method and create additional baselines on CycleCrash, we apply and compare 7 models along with a detailed ablation. We release the dataset and code at https://github.com/DeSinister/CycleCrash/ .",
        "published": "2024-10-31 04:00:00",
        "id": "398afb9f-cc6c-4e72-a3d7-cc0ac2426c22",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍CycleCrash数据集包含3000个行车记录仪视频用于自行车碰撞预测和分析，提出VidNeXt方法，还应用比较7种模型并进行消融实验，数据集和代码已开源。"
        },
        "tokens": 807
    },
    {
        "title": "Localizing Memorization in SSL Vision Encoders",
        "link": "https://arxiv.org/abs/2409.19069",
        "description": "arXiv:2409.19069v2 Announce Type: replace \nAbstract: Recent work on studying memorization in self-supervised learning (SSL) suggests that even though SSL encoders are trained on millions of images, they still memorize individual data points. While effort has been put into characterizing the memorized data and linking encoder memorization to downstream utility, little is known about where the memorization happens inside SSL encoders. To close this gap, we propose two metrics for localizing memorization in SSL encoders on a per-layer (layermem) and per-unit basis (unitmem). Our localization methods are independent of the downstream task, do not require any label information, and can be performed in a forward pass. By localizing memorization in various encoder architectures (convolutional and transformer-based) trained on diverse datasets with contrastive and non-contrastive SSL frameworks, we find that (1) while SSL memorization increases with layer depth, highly memorizing units are distributed across the entire encoder, (2) a significant fraction of units in SSL encoders experiences surprisingly high memorization of individual data points, which is in contrast to models trained under supervision, (3) atypical (or outlier) data points cause much higher layer and unit memorization than standard data points, and (4) in vision transformers, most memorization happens in the fully-connected layers. Finally, we show that localizing memorization in SSL has the potential to improve fine-tuning and to inform pruning strategies.",
        "published": "2024-10-31 04:00:00",
        "id": "8fdb3ade-9b3b-46de-b522-a8d66efe90e6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出两种度量标准以定位SSL编码器中的记忆，发现SSL记忆随层深增加、异常数据点会引起更高记忆、在视觉变换器中多数记忆发生在全连接层，且定位记忆有助于微调与修剪策略。"
        },
        "tokens": 891
    },
    {
        "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
        "link": "https://arxiv.org/abs/2410.00025",
        "description": "arXiv:2410.00025v2 Announce Type: replace \nAbstract: Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
        "published": "2024-10-31 04:00:00",
        "id": "81b90139-06c5-44e5-9a45-46ce883ea6a0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究表明在音素分类上微调语音表示模型可产生更多上下文不变表示，基于这些单元训练的语言模型在词汇理解方面可比基于百倍数据训练的模型，这有助于直接从语音学习语言的口语语言建模发展。"
        },
        "tokens": 752
    },
    {
        "title": "A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning",
        "link": "https://arxiv.org/abs/2410.00485",
        "description": "arXiv:2410.00485v2 Announce Type: replace \nAbstract: Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas. As such, there is a need for a methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate these capabilities. Previous efforts for unified benchmarks in deepfake detection have focused on the simpler binary task, overlooking evaluation protocols for fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary decision paradigm to address this gap. In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark. We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets. \\url{https://nickyfot.github.io/hitchhickersguide.github.io/}",
        "published": "2024-10-31 04:00:00",
        "id": "ee8157c5-eeae-4a4d-a63f-d107c4842e4a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种多阶段方法将人脸伪造检测转换为视觉问答任务，以评估模型在二元任务、细粒度检测方面的性能，并对基准中的视觉和大型语言模型的细粒度响应进行定性评估，应用该基准对多个流行模型进行评估。"
        },
        "tokens": 966
    },
    {
        "title": "Comparison of Autoencoder Encodings for ECG Representation in Downstream Prediction Tasks",
        "link": "https://arxiv.org/abs/2410.02937",
        "description": "arXiv:2410.02937v2 Announce Type: replace \nAbstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for cardiovascular assessment. Despite its standardized format and small file size, the high complexity and inter-individual variability of ECG signals (typically a 60,000-size vector) make it challenging to use in deep learning models, especially when only small datasets are available. This study addresses these challenges by exploring feature generation methods from representative beat ECGs, focusing on Principal Component Analysis (PCA) and Autoencoders to reduce data complexity. We introduce three novel Variational Autoencoder (VAE) variants: Stochastic Autoencoder (SAE), Annealed beta-VAE (Abeta-VAE), and cyclical beta-VAE (Cbeta-VAE), and compare their effectiveness in maintaining signal fidelity and enhancing downstream prediction tasks. The Abeta-VAE achieved superior signal reconstruction, reducing the mean absolute error (MAE) to 15.7 plus-minus 3.2 microvolts, which is at the level of signal noise. Moreover, the SAE encodings, when combined with ECG summary features, improved the prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an area under the receiver operating characteristic curve (AUROC) of 0.901. This performance nearly matches the 0.910 AUROC of state-of-the-art CNN models but requires significantly less data and computational resources. Our findings demonstrate that these VAE encodings are not only effective in simplifying ECG data but also provide a practical solution for applying deep learning in contexts with limited-scale labeled training data.",
        "published": "2024-10-31 04:00:00",
        "id": "dc4b1c02-5752-4e31-909b-aa7063716da9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过探索代表性心跳心电图的特征生成方法，引入三种新型变分自动编码器变体，比较它们在保持信号保真度和增强下游预测任务方面的有效性，发现其编码在简化心电图数据方面有效且为小样本深度学习提供解决方案。"
        },
        "tokens": 944
    },
    {
        "title": "Resource-aware Mixed-precision Quantization for Enhancing Deployability of Transformers for Time-series Forecasting on Embedded FPGAs",
        "link": "https://arxiv.org/abs/2410.03294",
        "description": "arXiv:2410.03294v3 Announce Type: replace \nAbstract: This study addresses the deployment challenges of integer-only quantized Transformers on resource-constrained embedded FPGAs (Xilinx Spartan-7 XC7S15). We enhanced the flexibility of our VHDL template by introducing a selectable resource type for storing intermediate results across model layers, thereby breaking the deployment bottleneck by utilizing BRAM efficiently. Moreover, we developed a resource-aware mixed-precision quantization approach that enables researchers to explore hardware-level quantization strategies without requiring extensive expertise in Neural Architecture Search. This method provides accurate resource utilization estimates with a precision discrepancy as low as 3%, compared to actual deployment metrics. Compared to previous work, our approach has successfully facilitated the deployment of model configurations utilizing mixed-precision quantization, thus overcoming the limitations inherent in five previously non-deployable configurations with uniform quantization bitwidths. Consequently, this research enhances the applicability of Transformers in embedded systems, facilitating a broader range of Transformer-powered applications on edge devices.",
        "published": "2024-10-31 04:00:00",
        "id": "3426c6c0-1989-4f19-ab94-da171a47f88f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究解决整数量化Transformer在资源受限的嵌入式FPGA上的部署挑战，通过引入可选择资源类型和资源感知混合精度量化方法，提升了Transformer在嵌入式系统中的适用性。"
        },
        "tokens": 804
    },
    {
        "title": "PRF: Parallel Resonate and Fire Neuron for Long Sequence Learning in Spiking Neural Networks",
        "link": "https://arxiv.org/abs/2410.03530",
        "description": "arXiv:2410.03530v2 Announce Type: replace \nAbstract: Recently, there is growing demand for effective and efficient long sequence modeling, with State Space Models (SSMs) proving to be effective for long sequence tasks. To further reduce energy consumption, SSMs can be adapted to Spiking Neural Networks (SNNs) using spiking functions. However, current spiking-formalized SSMs approaches still rely on float-point matrix-vector multiplication during inference, undermining SNNs' energy advantage. In this work, we address the efficiency and performance challenges of long sequence learning in SNNs simultaneously. First, we propose a decoupled reset method for parallel spiking neuron training, reducing the typical Leaky Integrate-and-Fire (LIF) model's training time from $O(L^2)$ to $O(L\\log L)$, effectively speeding up the training by $6.57 \\times$ to $16.50 \\times$ on sequence lengths $1,024$ to $32,768$. To our best knowledge, this is the first time that parallel computation with a reset mechanism is implemented achieving equivalence to its sequential counterpart. Secondly, to capture long-range dependencies, we propose a Parallel Resonate and Fire (PRF) neuron, which leverages an oscillating membrane potential driven by a resonate mechanism from a differentiable reset function in the complex domain. The PRF enables efficient long sequence learning while maintaining parallel training. Finally, we demonstrate that the proposed spike-driven architecture using PRF achieves performance comparable to Structured SSMs (S4), with two orders of magnitude reduction in energy consumption, outperforming Transformer on Long Range Arena tasks.",
        "published": "2024-10-31 04:00:00",
        "id": "0e0cf8eb-6ce1-4510-99e8-a30d8afbd266",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种用于脉冲神经网络长序列学习的并行共振激发神经元（PRF），包括解耦复位方法加速训练，PRF神经元利用振荡膜电位捕获长程依赖关系，该架构性能与结构化状态空间模型相当且能耗大幅降低。"
        },
        "tokens": 955
    },
    {
        "title": "AraSync: Precision Time Synchronization in Rural Wireless Living Lab",
        "link": "https://arxiv.org/abs/2410.03583",
        "description": "arXiv:2410.03583v2 Announce Type: replace \nAbstract: Time synchronization is a critical component in network operation and management, and it is also required by Ultra-Reliable, Low-Latency Communications (URLLC) in next-generation wireless systems such as those of 5G, 6G, and Open RAN. In this context, we design and implement AraSync as an end-to-end time synchronization system in the ARA wireless living lab to enable advanced wireless experiments and applications involving stringent time constraints. We make use of Precision Time Protocol (PTP) at different levels to achieve synchronization accuracy in the order of nanoseconds. Along with fiber networks, AraSync enables time synchronization across the AraHaul wireless x-haul network consisting of long-range, high-capacity mmWave and microwave links. In this paper, we present the detailed design and implementation of AraSync, including its hardware and software components and the PTP network topology. Further, we experimentally characterize the performance of AraSync from spatial and temporal dimensions. Our measurement and analysis of the clock offset and mean path delay show the impact of the wireless channel and weather conditions on the PTP synchronization accuracy.",
        "published": "2024-10-31 04:00:00",
        "id": "e6bde01f-9f8a-4bce-b6c8-4c5e68f22e3d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "AraSync是ARA无线实验室中的端到端时间同步系统，利用PTP实现纳秒级同步精度，论文介绍其设计、实现并实验性地从时空维度表征其性能。"
        },
        "tokens": 829
    },
    {
        "title": "System 2 Reasoning Capabilities Are Nigh",
        "link": "https://arxiv.org/abs/2410.03662",
        "description": "arXiv:2410.03662v2 Announce Type: replace \nAbstract: In recent years, machine learning models have made strides towards human-like reasoning capabilities from several directions. In this work, we review the current state of the literature and describe the remaining steps to achieve a neural model which can perform System~2 reasoning analogous to a human. We argue that if current models are insufficient to be classed as performing reasoning, there remains very little additional progress needed to attain that goal.",
        "published": "2024-10-31 04:00:00",
        "id": "fdc72025-5198-4c88-bbbe-ba9641eede98",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "回顾机器学习模型在类人推理能力方面的现状，阐述实现类似人类System 2推理的神经模型还需的步骤，认为当前模型距离实现推理目标只差很少的进步。"
        },
        "tokens": 677
    },
    {
        "title": "Disruption Risk Evaluation on Large-scale Production Network with Establishments and Products",
        "link": "https://arxiv.org/abs/2410.05595",
        "description": "arXiv:2410.05595v2 Announce Type: replace \nAbstract: We constructed an establishment-level production network where each establishment inputs and outputs multiple products, using data that includes the firm-level production network and establishments covering nearly all Japanese entities. The network represents the manufacturing sector with 183,951 establishments across 157,537 firms and 919,982 inter-establishment linkages. A probabilistic model of supply chain disruptions was applied to this network. The key findings are as follows: (1) The establishment-level network exhibits greater shock propagation compared to the firm-level network. (2) Incorporating actual product information leads to a larger impact on propagation compared to using industry-level information. (3) Regional shock simulations reveal that while the firm-level network shows greater shock propagation when the shock originates in Tokyo, no such difference is observed in the establishment-level network.",
        "published": "2024-10-31 04:00:00",
        "id": "585c9998-dd2a-4eb7-b647-4e835927628d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "使用涵盖几乎所有日本实体的数据构建企业层级生产网络，应用供应链中断概率模型，发现企业层级网络相比公司层级网络有更大冲击传播等结果。"
        },
        "tokens": 768
    },
    {
        "title": "LeanAgent: Lifelong Learning for Formal Theorem Proving",
        "link": "https://arxiv.org/abs/2410.06209",
        "description": "arXiv:2410.06209v5 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been successful in mathematical reasoning tasks such as formal theorem proving when integrated with interactive proof assistants like Lean. Existing approaches involve training or fine-tuning an LLM on a specific dataset to perform well on particular domains, such as undergraduate-level mathematics. These methods struggle with generalizability to advanced mathematics. A fundamental limitation is that these approaches operate on static domains, failing to capture how mathematicians often work across multiple domains and projects simultaneously or cyclically. We present LeanAgent, a novel lifelong learning framework for theorem proving that continuously generalizes to and improves on ever-expanding mathematical knowledge without forgetting previously learned knowledge. LeanAgent introduces several key innovations, including a curriculum learning strategy that optimizes the learning trajectory in terms of mathematical difficulty, a dynamic database for efficient management of evolving mathematical knowledge, and progressive training to balance stability and plasticity. LeanAgent successfully proves 162 theorems previously unproved by humans across 23 diverse Lean repositories, many from advanced mathematics. It performs significantly better than the static LLM baseline, proving challenging theorems in domains like abstract algebra and algebraic topology while showcasing a clear progression of learning from basic concepts to advanced topics. In addition, we analyze LeanAgent's superior performance on key lifelong learning metrics. LeanAgent achieves exceptional scores in stability and backward transfer, where learning new tasks improves performance on previously learned tasks. This emphasizes LeanAgent's continuous generalizability and improvement, explaining its superior theorem-proving performance.",
        "published": "2024-10-31 04:00:00",
        "id": "a2ca5bf8-04e4-47e1-a0a1-fcb339105e46",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "LeanAgent是一种用于定理证明的终身学习框架，引入创新策略，能不断泛化并提升数学知识，成功证明23个不同Lean存储库中162个人类未证明的定理，在关键终身学习指标上表现出色。"
        },
        "tokens": 906
    },
    {
        "title": "ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion",
        "link": "https://arxiv.org/abs/2410.06613",
        "description": "arXiv:2410.06613v2 Announce Type: replace \nAbstract: Accurate and affordable indoor 3D reconstruction is critical for effective robot navigation and interaction. Traditional LiDAR-based mapping provides high precision but is costly, heavy, and power-intensive, with limited ability for novel view rendering. Vision-based mapping, while cost-effective and capable of capturing visual data, often struggles with high-quality 3D reconstruction due to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a low-altitude camera and single-line LiDAR for high-quality 3D indoor reconstruction. Our system features Visual Error Construction (VEC) to enhance sparse point clouds by identifying and correcting areas with insufficient geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS initialization method guided by single-line LiDAR, overcoming the limitations of traditional multi-view setups and enabling effective reconstruction in resource-constrained environments. Extensive experimental results on our new Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian outperforms existing methods, particularly in challenging scenarios. The project page is available at https://chenlu-china.github.io/ES-Gaussian/.",
        "published": "2024-10-31 04:00:00",
        "id": "f61abf38-a6b8-4c77-b58f-d19804a244ca",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "ES - Gaussian是一种端到端系统，利用低空相机和单线激光雷达进行高质量的室内3D重建，其视觉误差构建可增强稀疏点云，新的3DGS初始化方法可克服传统多视图设置的局限性，实验表明它在挑战性场景中表现优于现有方法。"
        },
        "tokens": 857
    },
    {
        "title": "Continual Learning in the Frequency Domain",
        "link": "https://arxiv.org/abs/2410.06645",
        "description": "arXiv:2410.06645v3 Announce Type: replace \nAbstract: Continual learning (CL) is designed to learn new tasks while preserving existing knowledge. Replaying samples from earlier tasks has proven to be an effective method to mitigate the forgetting of previously acquired knowledge. However, the current research on the training efficiency of rehearsal-based methods is insufficient, which limits the practical application of CL systems in resource-limited scenarios. The human visual system (HVS) exhibits varying sensitivities to different frequency components, enabling the efficient elimination of visually redundant information. Inspired by HVS, we propose a novel framework called Continual Learning in the Frequency Domain (CLFD). To our knowledge, this is the first study to utilize frequency domain features to enhance the performance and efficiency of CL training on edge devices. For the input features of the feature extractor, CLFD employs wavelet transform to map the original input image into the frequency domain, thereby effectively reducing the size of input feature maps. Regarding the output features of the feature extractor, CLFD selectively utilizes output features for distinct classes for classification, thereby balancing the reusability and interference of output features based on the frequency domain similarity of the classes across various tasks. Optimizing only the input and output features of the feature extractor allows for seamless integration of CLFD with various rehearsal-based methods. Extensive experiments conducted in both cloud and edge environments demonstrate that CLFD consistently improves the performance of state-of-the-art (SOTA) methods in both precision and training efficiency. Specifically, CLFD can increase the accuracy of the SOTA CL method by up to 6.83% and reduce the training time by 2.6$\\times$.",
        "published": "2024-10-31 04:00:00",
        "id": "407ae650-008c-4ecc-9aa7-2ce2856d06f9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "受人类视觉系统启发，提出频域持续学习框架CLFD，优化特征提取器输入输出特征，在云与边缘环境实验中提升现有方法性能与训练效率。"
        },
        "tokens": 921
    },
    {
        "title": "Benchmarking Agentic Workflow Generation",
        "link": "https://arxiv.org/abs/2410.07869",
        "description": "arXiv:2410.07869v2 Announce Type: replace \nAbstract: Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset are available at https://github.com/zjunlp/WorFBench.",
        "published": "2024-10-31 04:00:00",
        "id": "03ba89b4-d5a6-4bd8-af78-5b0898f3b29c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍WorFBench统一工作流生成基准与WorFEval评估协议，通过评估不同类型LLMs发现LLM代理的序列和图规划能力存在差距，训练两个开源模型并评估其泛化能力，且发现生成的工作流可提升下游任务性能。"
        },
        "tokens": 852
    },
    {
        "title": "Lambda-Skip Connections: the architectural component that prevents Rank Collapse",
        "link": "https://arxiv.org/abs/2410.10609",
        "description": "arXiv:2410.10609v2 Announce Type: replace \nAbstract: Rank collapse, a phenomenon where embedding vectors in sequence models rapidly converge to a uniform token or equilibrium state, has recently gained attention in the deep learning literature. This phenomenon leads to reduced expressivity and potential training instabilities due to vanishing gradients. Empirical evidence suggests that architectural components like skip connections, LayerNorm, and MultiLayer Perceptrons (MLPs) play critical roles in mitigating rank collapse. While this issue is well-documented for transformers, alternative sequence models, such as State Space Models (SSMs), which have recently gained prominence, have not been thoroughly examined for similar vulnerabilities. This paper extends the theory of rank collapse from transformers to SSMs using a unifying framework that captures both architectures. We study how a parametrized version of the classic skip connection component, which we call \\emph{lambda-skip connections}, provides guarantees for rank collapse prevention. Through analytical results, we present a sufficient condition to guarantee prevention of rank collapse across all the aforementioned architectures. We also study the necessity of this condition via ablation studies and analytical examples. To our knowledge, this is the first study that provides a general guarantee to prevent rank collapse, and that investigates rank collapse in the context of SSMs, offering valuable understanding for both theoreticians and practitioners. Finally, we validate our findings with experiments demonstrating the crucial role of architectural components such as skip connections and gating mechanisms in preventing rank collapse.",
        "published": "2024-10-31 04:00:00",
        "id": "f925bcbb-f907-4fa0-acfa-8f2920f15e2b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文研究了rank collapse现象，通过统一框架将其理论从transformers扩展到SSMs，探讨lambda - skip connections防止rank collapse的作用，通过分析结果给出预防的充分条件并研究必要性，还进行了实验验证。"
        },
        "tokens": 893
    },
    {
        "title": "Dynamical loss functions shape landscape topography and improve learning in artificial neural networks",
        "link": "https://arxiv.org/abs/2410.10690",
        "description": "arXiv:2410.10690v2 Announce Type: replace \nAbstract: Dynamical loss functions are derived from standard loss functions used in supervised classification tasks, but they are modified such that the contribution from each class periodically increases and decreases. These oscillations globally alter the loss landscape without affecting the global minima. In this paper, we demonstrate how to transform cross-entropy and mean squared error into dynamical loss functions. We begin by discussing the impact of increasing the size of the neural network or the learning rate on the learning process. Building on this intuition, we propose several versions of dynamical loss functions and show how they significantly improve validation accuracy for networks of varying sizes. Finally, we explore how the landscape of these dynamical loss functions evolves during training, highlighting the emergence of instabilities that may be linked to edge-of-instability minimization.",
        "published": "2024-10-31 04:00:00",
        "id": "ee9b98c5-f17a-4fbb-9194-0bab9f205876",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章介绍了动态损失函数（由监督分类任务中的标准损失函数衍生而来），阐述其对神经网络学习过程的影响，展示不同版本提升网络验证准确性的情况，并探索其在训练中的景观演变。"
        },
        "tokens": 760
    },
    {
        "title": "Fair Interest Rates Are Impossible for Lending Pools: Results from Options Pricing",
        "link": "https://arxiv.org/abs/2410.11053",
        "description": "arXiv:2410.11053v2 Announce Type: replace \nAbstract: Cryptocurrency lending pools are services that allow lenders to pool together assets in one cryptocurrency and loan it out to borrowers who provide collateral worth more (than the loan) in a separate cryptocurrency. Borrowers can repay their loans to reclaim their collateral unless their loan was liquidated, which happens when the value of the collateral dips significantly. Interest rates for these pools are currently set via supply and demand heuristics, which have several downsides, including inefficiency, inflexibility, and being vulnerable to manipulation. Here, we reduce lending pools to options, and then use ideas from options pricing to search for fair interest rates for lending pools. In a simplified model where the loans have a fixed duration and can only be repaid at the end of the term, we obtain analytical pricing results. We then consider a more realistic model, where loans can be repaid dynamically and without expiry. Our main theoretical contribution is to show that fair interest rates do not exist in this setting. We then show that impossibility results generalize even to models of lending pools which have no obvious reduction to options. To address these negative results, we introduce a model of lending pools with fixed fees, and model the ability of borrowers to top-up their loans to reduce the risk of liquidation. As a proof of concept, we use simulations to show how our model's predicted interest rates compare to interest rates in practice.",
        "published": "2024-10-31 04:00:00",
        "id": "1fae819e-c5da-4aa3-87db-1ba0eba56aed",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究表明加密货币借贷池按供需启发式设定利率有诸多弊端，将借贷池简化为期权来寻找公平利率，在简化模型中得到分析定价结果，在更现实模型中表明公平利率不存在，还将不可能性结果推广到其他借贷池模型，最后引入固定费用模型并模拟以对比预测利率与实际利率。"
        },
        "tokens": 909
    },
    {
        "title": "DreamSteerer: Enhancing Source Image Conditioned Editability using Personalized Diffusion Models",
        "link": "https://arxiv.org/abs/2410.11208",
        "description": "arXiv:2410.11208v2 Announce Type: replace \nAbstract: Recent text-to-image personalization methods have shown great promise in teaching a diffusion model user-specified concepts given a few images for reusing the acquired concepts in a novel context. With massive efforts being dedicated to personalized generation, a promising extension is personalized editing, namely to edit an image using personalized concepts, which can provide a more precise guidance signal than traditional textual guidance. To address this, a straightforward solution is to incorporate a personalized diffusion model with a text-driven editing framework. However, such a solution often shows unsatisfactory editability on the source image. To address this, we propose DreamSteerer, a plug-in method for augmenting existing T2I personalization methods. Specifically, we enhance the source image conditioned editability of a personalized diffusion model via a novel Editability Driven Score Distillation (EDSD) objective. Moreover, we identify a mode trapping issue with EDSD, and propose a mode shifting regularization with spatial feature guided sampling to avoid such an issue. We further employ two key modifications to the Delta Denoising Score framework that enable high-fidelity local editing with personalized concepts. Extensive experiments validate that DreamSteerer can significantly improve the editability of several T2I personalization baselines while being computationally efficient.",
        "published": "2024-10-31 04:00:00",
        "id": "5bfb6612-f6dc-44a1-9a74-de111f2412b5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出DreamSteerer插件方法，通过Editability Driven Score Distillation目标增强个性化扩散模型的源图像条件可编辑性，还提出模式转移正则化并对Delta Denoising Score框架进行修改，以提高可编辑性并实现高保真局部编辑。"
        },
        "tokens": 872
    },
    {
        "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate through LLMs",
        "link": "https://arxiv.org/abs/2410.11387",
        "description": "arXiv:2410.11387v3 Announce Type: replace \nAbstract: Robot swarms are composed of many simple robots that communicate and collaborate to fulfill complex tasks. Robot controllers usually need to be specified by experts on a case-by-case basis via programming code. This process is time-consuming, prone to errors, and unable to take into account all situations that may be encountered during deployment. On the other hand, recent Large Language Models (LLMs) have demonstrated reasoning and planning capabilities, introduced new ways to interact with and program machines, and incorporate both domain-specific and commonsense knowledge. Hence, we propose to address the aforementioned challenges by integrating LLMs with robot swarms and show the potential in proofs of concept (showcases). For this integration, we explore two approaches. The first approach is 'indirect integration,' where LLMs are used to synthesize and validate the robot controllers. This approach may reduce development time and human error before deployment. Moreover, during deployment, it could be used for on-the-fly creation of new robot behaviors. The second approach is 'direct integration,' where each robot locally executes a separate LLM instance during deployment for robot-robot collaboration and human-swarm interaction. These local LLM instances enable each robot to reason, plan, and collaborate using natural language, as demonstrated in our showcases where the robots are able to detect a variety of anomalies, without prior information about the nature of these anomalies. To enable further research on our mainly conceptual contribution, we release the software and videos for our LLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
        "published": "2024-10-31 04:00:00",
        "id": "c3b24e02-c3a2-4dd1-829d-75c2d61eed01",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出将大型语言模型（LLMs）与机器人群集成的两种方法（间接集成和直接集成），以解决机器人群控制开发耗时、易出错等问题，并为LLM2Swarm系统发布软件和视频。"
        },
        "tokens": 931
    },
    {
        "title": "Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?",
        "link": "https://arxiv.org/abs/2410.11443",
        "description": "arXiv:2410.11443v3 Announce Type: replace \nAbstract: Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including $k$-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while maintaining EGNN's efficiency through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric structures, but also shows substantial improvements on more complicated datasets such as $N$-body and MD17. Our theoretical findings and empirical results potentially open up new possibilities for the research of equivariant GNNs.",
        "published": "2024-10-31 04:00:00",
        "id": "b013167a-0792-42f1-b8e5-e9ec151d0d94",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文反驳了等变图神经网络中高次表示不必要的观点，从理论上证明若输出表示的次数固定为1或其他特定值时网络会退化为零函数，还提出了HEGNN增加表达性，实验表明HEGNN在对称结构数据集和复杂数据集上效果良好。"
        },
        "tokens": 907
    },
    {
        "title": "Degradation Oriented and Regularized Network for Real-World Depth Super-Resolution",
        "link": "https://arxiv.org/abs/2410.11666",
        "description": "arXiv:2410.11666v2 Announce Type: replace \nAbstract: Recent RGB-guided depth super-resolution methods have achieved impressive performance under the assumption of fixed and known degradation (e.g., bicubic downsampling). However, in real-world scenarios, captured depth data often suffer from unconventional and unknown degradation due to sensor limitations and complex imaging environments (e.g., low reflective surfaces, varying illumination). Consequently, the performance of these methods significantly declines when real-world degradation deviate from their assumptions. In this paper, we propose the Degradation Oriented and Regularized Network (DORNet), a novel framework designed to adaptively address unknown degradation in real-world scenes through implicit degradation representations. Our approach begins with the development of a self-supervised degradation learning strategy, which models the degradation representations of low-resolution depth data using routing selection-based degradation regularization. To facilitate effective RGB-D fusion, we further introduce a degradation-oriented feature transformation module that selectively propagates RGB content into the depth data based on the learned degradation priors. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of our DORNet. The code is available at https://github.com/yanzq95/DORNet.",
        "published": "2024-10-31 04:00:00",
        "id": "98736db4-97bd-426a-b426-412b80d29ee5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种面向退化和正则化的网络（DORNet）以自适应解决现实场景中的未知退化，采用自监督退化学习策略并引入面向退化的特征转换模块，实验结果证明其优越性。"
        },
        "tokens": 837
    },
    {
        "title": "Utilizing Large Language Models in an iterative paradigm with Domain feedback for Zero-shot Molecule optimization",
        "link": "https://arxiv.org/abs/2410.13147",
        "description": "arXiv:2410.13147v4 Announce Type: replace \nAbstract: Molecule optimization is a critical task in drug discovery to optimize desired properties of a given molecule through chemical modification. Despite Large Language Models (LLMs) holding the potential to efficiently simulate this task by using natural language to direct the optimization, straightforwardly utilizing shows limited performance. In this work, we facilitate utilizing LLMs in an iterative paradigm by proposing a simple yet highly effective domain feedback provider, namely $\\text{Re}^3$DF. In detail, $\\text{Re}^3$DF harnesses an external toolkit, RDKit, to handle the molecule hallucination, if the modified molecule is chemically invalid. Otherwise, its desired properties are computed and compared to the original one, establishing reliable domain feedback with correct direction and distance towards the objective, followed by a retrieved example, to explicitly guide the LLM to refine the modified molecule. We conduct experiments across both single- and multi-property objectives with 2 thresholds, where $\\text{Re}^3$DF shows significant improvements. Particularly, for 20 single-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds, respectively. For 32 multi-property objectives, $\\text{Re}^3$DF enhances Hit ratio by 6.04% and 5.25%.",
        "published": "2024-10-31 04:00:00",
        "id": "a093f46f-86c3-4d7c-8319-352443874759",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为Re3DF的域反馈提供者，以迭代范式利用大型语言模型进行零样本分子优化，在单属性和多属性目标实验中有显著改进。"
        },
        "tokens": 887
    },
    {
        "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents",
        "link": "https://arxiv.org/abs/2410.13185",
        "description": "arXiv:2410.13185v5 Announce Type: replace \nAbstract: Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to generate a candidate idea and its corresponding experimental design.",
        "published": "2024-10-31 04:00:00",
        "id": "ac5ed9f2-09c6-42b9-897c-5953eace2f47",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于大型语言模型（LLM）的思想链（CoI）代理，通过链结构组织相关文献以提升研究创意能力，还提出Idea Arena评估协议，实验结果表明CoI代理在研究想法生成方面表现出色且成本低。"
        },
        "tokens": 865
    },
    {
        "title": "aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion",
        "link": "https://arxiv.org/abs/2410.13187",
        "description": "arXiv:2410.13187v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have been widely used in code completion, and researchers are focusing on scaling up LLMs to improve their accuracy. However, larger LLMs will increase the response time of code completion and decrease the developers' productivity. In this paper, we propose a lightweight and effective LLM for code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B achieves higher code completion accuracy while having smaller scales (i.e., 7 billion parameters). We attribute the superiority of aiXcoder-7B to three key factors: (1) Multi-objective training. We employ three training objectives, one of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers the syntax structures in code and effectively improves the performance of LLMs for code. (2) Diverse data sampling strategies. They consider inter-file relationships and enhance the capability of LLMs in understanding cross-file contexts. (3) Extensive high-quality data. We establish a rigorous data collection pipeline and consume a total of 1.2 trillion unique tokens for training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a broad distribution of code. We evaluate aiXcoder-7B in five popular code completion benchmarks and a new benchmark collected by this paper. The results show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B), positioning aiXcoder-7B as a lightweight and effective LLM for academia and industry. Finally, we summarize three valuable insights for helping practitioners train the next generations of LLMs for code. aiXcoder-7B has been open-souced and gained significant attention. As of the submission date, aiXcoder-7B has received 2,193 GitHub Stars.",
        "published": "2024-10-31 04:00:00",
        "id": "4efce000-01e7-4c76-b1ec-09ceba88f6f6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "aiXcoder - 7B是一种轻量高效的大型语言模型，通过多目标训练、多样数据采样策略和大量高质量数据在代码补全任务中实现高准确性，已开源并获关注。"
        },
        "tokens": 1028
    },
    {
        "title": "Opportunities and Challenges of Generative-AI in Finance",
        "link": "https://arxiv.org/abs/2410.15653",
        "description": "arXiv:2410.15653v2 Announce Type: replace \nAbstract: Gen-AI techniques are able to improve understanding of context and nuances in language modeling, translation between languages, handle large volumes of data, provide fast, low-latency responses and can be fine-tuned for various tasks and domains.\n  In this manuscript, we present a comprehensive overview of the applications of Gen-AI techniques in the finance domain. In particular, we present the opportunities and challenges associated with the usage of Gen-AI techniques. We also illustrate the various methodologies which can be used to train Gen-AI techniques and present the various application areas of Gen-AI technologies in the finance ecosystem.\n  To the best of our knowledge, this work represents the most comprehensive summarization of Gen-AI techniques within the financial domain. The analysis is designed for a deep overview of areas marked for substantial advancement while simultaneously pin-point those warranting future prioritization. We also hope that this work would serve as a conduit between finance and other domains, thus fostering the cross-pollination of innovative concepts and practices.",
        "published": "2024-10-31 04:00:00",
        "id": "a3cef73b-bfa8-44f9-9c49-e688ccf05080",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章对生成式人工智能（Gen - AI）技术在金融领域的应用进行了全面概述，阐述了其机遇与挑战、训练方法以及在金融生态系统中的应用领域。"
        },
        "tokens": 801
    },
    {
        "title": "Do LLMs \"know\" internally when they follow instructions?",
        "link": "https://arxiv.org/abs/2410.14516",
        "description": "arXiv:2410.14516v4 Announce Type: replace \nAbstract: Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.",
        "published": "2024-10-31 04:00:00",
        "id": "314afd1b-1bdb-4937-9b84-5390b4e98f95",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现大型语言模型（LLMs）输入嵌入空间中存在与成功遵循指令相关的维度，沿此维度修改表示可提高指令遵循成功率且不影响响应质量，还解释了LLMs有时无法遵循指令及提示工程有效的原因。"
        },
        "tokens": 818
    },
    {
        "title": "Can Knowledge Editing Really Correct Hallucinations?",
        "link": "https://arxiv.org/abs/2410.16251",
        "description": "arXiv:2410.16251v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing.",
        "published": "2024-10-31 04:00:00",
        "id": "d097c22f-7ced-400a-92fa-e7fdbb2060e6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对大型语言模型存在幻觉（生成内容包含非事实信息）的问题，提出HalluEditBench全面评估知识编辑方法纠正现实幻觉的性能，包括构建大规模幻觉数据集并从五个维度评估知识编辑方法。"
        },
        "tokens": 884
    },
    {
        "title": "Erasing Undesirable Concepts in Diffusion Models with Adversarial Preservation",
        "link": "https://arxiv.org/abs/2410.15618",
        "description": "arXiv:2410.15618v2 Announce Type: replace \nAbstract: Diffusion models excel at generating visually striking content from text but can inadvertently produce undesirable or harmful content when trained on unfiltered internet data. A practical solution is to selectively removing target concepts from the model, but this may impact the remaining concepts. Prior approaches have tried to balance this by introducing a loss term to preserve neutral content or a regularization term to minimize changes in the model parameters, yet resolving this trade-off remains challenging. In this work, we propose to identify and preserving concepts most affected by parameter changes, termed as \\textit{adversarial concepts}. This approach ensures stable erasure with minimal impact on the other concepts. We demonstrate the effectiveness of our method using the Stable Diffusion model, showing that it outperforms state-of-the-art erasure methods in eliminating unwanted content while maintaining the integrity of other unrelated elements. Our code is available at \\url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.",
        "published": "2024-10-31 04:00:00",
        "id": "3f3ca41c-dbfa-474b-be99-d8d8c0489100",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对扩散模型训练时可能产生不良内容的问题，本文提出一种识别和保留受参数变化影响最大概念的方法，经稳定扩散模型验证，该方法在消除不良内容同时能保持其他元素完整性，且优于现有消除方法，代码已开源。"
        },
        "tokens": 813
    },
    {
        "title": "Dynamic Input Mapping Inversion for Algebraic Loop-Free Control in Hydraulic Actuators",
        "link": "https://arxiv.org/abs/2410.13389",
        "description": "arXiv:2410.13389v2 Announce Type: replace \nAbstract: The application of nonlinear control schemes to electro-hydraulic actuators often requires several alterations in the design of the controllers during their implementation. This is to overcome the challenges that frequently arise from the inherent complexity of such control algorithms owning to model nonlinearities. Moreover, advanced control solutions for this type of systems often introduce input algebraic loops and chatter, which considerably degrade the tracking performance. This study presents a nonlinear control architecture for hydraulic actuators that comprises low-complexity modules, based on well-established designs that facilitate robust high performance in tracking without introducing the aforementioned limitations. Specifically, the proposed solution consists of two variants of a position controller for the hydraulic cylinder and a dynamic input-mapping inversion module to avoid algebraic loops in the control input. The stability of the closed-loop system is analysed using arguments from Lyapunov theory for cascaded non-autonomous nonlinear systems. The effectiveness of the proposed solution is evaluated on a high-fidelity simulator of a wind turbine pitch system. Appropriate quantitative metrics are finally defined to evaluate the closed-loop system performance in comparison to state-of-the-art nonlinear design.",
        "published": "2024-10-31 04:00:00",
        "id": "6f5220ed-1f57-4831-854f-d09102fbbcba",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种用于液压执行器的非线性控制架构，包含低复杂度模块，有两种位置控制器变体和动态输入映射反演模块以避免控制输入中的代数环，通过李雅普诺夫理论分析闭环系统稳定性，并在风力涡轮机桨距系统模拟器上评估有效性。"
        },
        "tokens": 842
    },
    {
        "title": "TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling",
        "link": "https://arxiv.org/abs/2410.16033",
        "description": "arXiv:2410.16033v3 Announce Type: replace \nAbstract: Inference-time alignment enhances the performance of large language models without requiring additional training or fine-tuning but presents challenges due to balancing computational efficiency with high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates multiple responses and selects the best one, achieving improved performance but with a high computational cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and pruning low-quality responses, thereby reducing computational overhead while maintaining high output quality. Our approach also leverages token-level rewards from Direct Preference Optimization (DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and TutorEval datasets, demonstrating consistent improvements. Specifically, TreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win rates across other different datasets, outperforming standard BoN with the same computational cost and showcasing its scalability and alignment efficacy.",
        "published": "2024-10-31 04:00:00",
        "id": "4645d3b0-6520-4140-9225-9cce23a0879a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "TreeBoN通过将推测性树搜索策略集成到Best-of-N采样中减少计算开销并保持高质量输出，利用DPO的令牌级奖励引导树扩展和修剪低质量路径，在多个数据集上有性能提升。"
        },
        "tokens": 856
    },
    {
        "title": "Improving Causal Reasoning in Large Language Models: A Survey",
        "link": "https://arxiv.org/abs/2410.16676",
        "description": "arXiv:2410.16676v2 Announce Type: replace \nAbstract: Causal reasoning (CR) is a crucial aspect of intelligence, essential for problem-solving, decision-making, and understanding the world. While large language models (LLMs) can generate rationales for their outputs, their ability to reliably perform causal reasoning remains uncertain, often falling short in tasks requiring a deep understanding of causality. In this survey, we provide a comprehensive review of research aimed at enhancing LLMs for causal reasoning. We categorize existing methods based on the role of LLMs: either as reasoning engines or as helpers providing knowledge or data to traditional CR methods, followed by a detailed discussion of the methodologies in each category. We then evaluate the performance of LLMs on various causal reasoning tasks, providing key findings and in-depth analysis. Finally, we provide insights from current studies and highlight promising directions for future research. We aim for this work to serve as a comprehensive resource, fostering further advancements in causal reasoning with LLMs. Resources are available at https://github.com/chendl02/Awesome-LLM-causal-reasoning.",
        "published": "2024-10-31 04:00:00",
        "id": "61d578ed-b63a-4acf-a703-42c74526e472",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "该文综述了旨在增强大型语言模型因果推理能力的研究，对现有方法分类并详细讨论，评估模型在因果推理任务中的性能，提供见解并指出未来研究方向。"
        },
        "tokens": 812
    },
    {
        "title": "Hardware-Software Co-optimised Fast and Accurate Deep Reconfigurable Spiking Inference Accelerator Architecture Design Methodology",
        "link": "https://arxiv.org/abs/2410.16298",
        "description": "arXiv:2410.16298v2 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs) have emerged as a promising approach to improve the energy efficiency of machine learning models, as they naturally implement event-driven computations while avoiding expensive multiplication operations. In this paper, we develop a hardware-software co-optimisation strategy to port software-trained deep neural networks (DNN) to reduced-precision spiking models demonstrating fast and accurate inference in a novel event-driven CMOS reconfigurable spiking inference accelerator. Experimental results show that a reduced-precision Resnet-18 and VGG-11 SNN models achieves classification accuracy within 1% of the baseline full-precision DNN model within 8 spike timesteps. We also demonstrate an FPGA prototype implementation of the spiking inference accelerator with a throughput of 38.4 giga operations per second (GOPS) consuming 1.54 Watts on PYNQ-Z2 FPGA. This corresponds to 0.6 GOPS per processing element and 2.25,GOPS/DSP slice, which is 2x and 4.5x higher utilisation efficiency respectively compared to the state-of-the-art. Our co-optimisation strategy can be employed to develop deep reduced precision SNN models and port them to resource-efficient event-driven hardware accelerators for edge applications.",
        "published": "2024-10-31 04:00:00",
        "id": "e6a2223a-0748-403f-85bc-869a2fc39f4e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出一种软硬件协同优化策略将软件训练的深度神经网络移植到低精度脉冲模型，在新型事件驱动的CMOS可重构脉冲推理加速器中实现快速准确推理，给出实验结果并展示FPGA原型实现。"
        },
        "tokens": 885
    },
    {
        "title": "Fair Bilevel Neural Network (FairBiNN): On Balancing fairness and accuracy via Stackelberg Equilibrium",
        "link": "https://arxiv.org/abs/2410.16432",
        "description": "arXiv:2410.16432v2 Announce Type: replace \nAbstract: The persistent challenge of bias in machine learning models necessitates robust solutions to ensure parity and equal treatment across diverse groups, particularly in classification tasks. Current methods for mitigating bias often result in information loss and an inadequate balance between accuracy and fairness. To address this, we propose a novel methodology grounded in bilevel optimization principles. Our deep learning-based approach concurrently optimizes for both accuracy and fairness objectives, and under certain assumptions, achieving proven Pareto optimal solutions while mitigating bias in the trained model. Theoretical analysis indicates that the upper bound on the loss incurred by this method is less than or equal to the loss of the Lagrangian approach, which involves adding a regularization term to the loss function. We demonstrate the efficacy of our model primarily on tabular datasets such as UCI Adult and Heritage Health. When benchmarked against state-of-the-art fairness methods, our model exhibits superior performance, advancing fairness-aware machine learning solutions and bridging the accuracy-fairness gap. The implementation of FairBiNN is available on https://github.com/yazdanimehdi/FairBiNN.",
        "published": "2024-10-31 04:00:00",
        "id": "e832653e-ec81-4dc5-b6a8-b53c77a14cc0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于双层优化原理的FairBiNN方法，用于在分类任务中平衡准确性和公平性，理论分析显示其损失上限不高于拉格朗日方法，在UCI Adult和Heritage Health等数据集上表现优于现有公平性方法。"
        },
        "tokens": 837
    },
    {
        "title": "SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects",
        "link": "https://arxiv.org/abs/2410.16499",
        "description": "arXiv:2410.16499v2 Announce Type: replace \nAbstract: We address the challenge of creating 3D assets for household articulated objects from a single image. Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process. These limitations hinder the scalability and practicality for articulated object modeling. In this work, we propose a method to generate articulated objects from a single image. Observing the object in resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image. To capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics. To tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies. Our experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality.",
        "published": "2024-10-31 04:00:00",
        "id": "6247003e-ccba-4567-8d0f-61162c5f7a4f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种从单张图像生成关节物体的方法，通过设计扩散模型和处理流程来克服现有技术在关节物体创建方面的局限性，实验表明该方法在多方面优于现有技术。"
        },
        "tokens": 838
    },
    {
        "title": "SEA: State-Exchange Attention for High-Fidelity Physics Based Transformers",
        "link": "https://arxiv.org/abs/2410.15495",
        "description": "arXiv:2410.15495v2 Announce Type: replace \nAbstract: Current approaches using sequential networks have shown promise in estimating field variables for dynamical systems, but they are often limited by high rollout errors. The unresolved issue of rollout error accumulation results in unreliable estimations as the network predicts further into the future, with each step's error compounding and leading to an increase in inaccuracy. Here, we introduce the State-Exchange Attention (SEA) module, a novel transformer-based module enabling information exchange between encoded fields through multi-head cross-attention. The cross-field multidirectional information exchange design enables all state variables in the system to exchange information with one another, capturing physical relationships and symmetries between fields. Additionally, we introduce an efficient ViT-like mesh autoencoder to generate spatially coherent mesh embeddings for a large number of meshing cells. The SEA integrated transformer demonstrates the state-of-the-art rollout error compared to other competitive baselines. Specifically, we outperform PbGMR-GMUS Transformer-RealNVP and GMR-GMUS Transformer, with a reduction in error of 88% and 91%, respectively. Furthermore, we demonstrate that the SEA module alone can reduce errors by 97% for state variables that are highly dependent on other states of the system. The repository for this work is available at: https://github.com/ParsaEsmati/SEA",
        "published": "2024-10-31 04:00:00",
        "id": "2b8e9c72-2cef-4dcb-91d1-a600836577d3",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出基于Transformer的State - Exchange Attention (SEA)模块，其通过多头交叉注意力实现编码字段间信息交换以降低滚动误差，还介绍了类似ViT的网格自动编码器，SEA集成Transformer的滚动误差相比其他基准更低。"
        },
        "tokens": 888
    },
    {
        "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
        "link": "https://arxiv.org/abs/2410.17215",
        "description": "arXiv:2410.17215v2 Announce Type: replace \nAbstract: Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.",
        "published": "2024-10-31 04:00:00",
        "id": "262b12b4-f310-45e1-af96-acf454b40ddf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "MiniPLM是一种知识蒸馏框架用于预训练语言模型，通过教师知识优化训练数据分布，具有高效、灵活、有效等特点，实验证明其提升学生模型性能、语言建模能力并减少预训练计算。"
        },
        "tokens": 910
    },
    {
        "title": "A Systematic Survey on Instructional Text: From Representation Formats to Downstream NLP Tasks",
        "link": "https://arxiv.org/abs/2410.18529",
        "description": "arXiv:2410.18529v2 Announce Type: replace \nAbstract: Recent advances in large language models have demonstrated promising capabilities in following simple instructions through instruction tuning. However, real-world tasks often involve complex, multi-step instructions that remain challenging for current NLP systems. Despite growing interest in this area, there lacks a comprehensive survey that systematically analyzes the landscape of complex instruction understanding and processing. Through a systematic review of the literature, we analyze available resources, representation schemes, and downstream tasks related to instructional text. Our study examines 177 papers, identifying trends, challenges, and opportunities in this emerging field. We provide AI/NLP researchers with essential background knowledge and a unified view of various approaches to complex instruction understanding, bridging gaps between different research directions and highlighting future research opportunities.",
        "published": "2024-10-31 04:00:00",
        "id": "251a5989-ed39-43f3-bd63-d2c8568dd33f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文对与指令性文本相关的资源、表示方案和下游任务进行系统性回顾，分析177篇论文，为AI/NLP研究者提供背景知识、对复杂指令理解的多种方法的统一观点等。"
        },
        "tokens": 755
    },
    {
        "title": "A Kernel Perspective on Distillation-based Collaborative Learning",
        "link": "https://arxiv.org/abs/2410.17592",
        "description": "arXiv:2410.17592v2 Announce Type: replace \nAbstract: Over the past decade, there is a growing interest in collaborative learning that can enhance AI models of multiple parties. However, it is still challenging to enhance performance them without sharing private data and models from individual parties. One recent promising approach is to develop distillation-based algorithms that exploit unlabeled public data but the results are still unsatisfactory in both theory and practice. To tackle this problem, we rigorously analyze a representative distillation-based algorithm in the view of kernel regression. This work provides the first theoretical results to prove the (nearly) minimax optimality of the nonparametric collaborative learning algorithm that does not directly share local data or models in massively distributed statistically heterogeneous environments. Inspired by our theoretical results, we also propose a practical distillation-based collaborative learning algorithm based on neural network architecture. Our algorithm successfully bridges the gap between our theoretical assumptions and practical settings with neural networks through feature kernel matching. We simulate various regression tasks to verify our theory and demonstrate the practical feasibility of our proposed algorithm.",
        "published": "2024-10-31 04:00:00",
        "id": "94defd79-921f-4a54-8aab-9270661c349f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "通过核回归视角分析代表性蒸馏算法，提供非参数协同学习算法的理论结果，提出基于神经网络架构的实用蒸馏协同学习算法，并通过模拟回归任务验证理论和算法的可行性。"
        },
        "tokens": 800
    },
    {
        "title": "VISAGE: Video Synthesis using Action Graphs for Surgery",
        "link": "https://arxiv.org/abs/2410.17751",
        "description": "arXiv:2410.17751v2 Announce Type: replace \nAbstract: Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills. However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods. In this work, we introduce the novel task of future video generation in laparoscopic surgery. This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery. Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences. VISAGE predicts the future frames given only a single initial frame, and the action graph triplets. By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures. The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS.",
        "published": "2024-10-31 04:00:00",
        "id": "3b80033a-7556-4a4d-817d-1edb91db949e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "在手术数据科学领域，由于手术数据的稀缺、异构和复杂，现有机器学习方法受限，论文提出VISAGE方法用于腹腔镜手术中的未来视频生成任务，该方法利用动作场景图和扩散模型生成高保真视频。"
        },
        "tokens": 854
    },
    {
        "title": "Scalable Ranked Preference Optimization for Text-to-Image Generation",
        "link": "https://arxiv.org/abs/2410.18013",
        "description": "arXiv:2410.18013v2 Announce Type: replace \nAbstract: Direct Preference Optimization (DPO) has emerged as a powerful approach to align text-to-image (T2I) models with human feedback. Unfortunately, successful application of DPO to T2I models requires a huge amount of resources to collect and label large-scale datasets, e.g., millions of generated paired images annotated with human preferences. In addition, these human preference datasets can get outdated quickly as the rapid improvements of T2I models lead to higher quality images. In this work, we investigate a scalable approach for collecting large-scale and fully synthetic datasets for DPO training. Specifically, the preferences for paired images are generated using a pre-trained reward function, eliminating the need for involving humans in the annotation process, greatly improving the dataset collection efficiency. Moreover, we demonstrate that such datasets allow averaging predictions across multiple models and collecting ranked preferences as opposed to pairwise preferences. Furthermore, we introduce RankDPO to enhance DPO-based methods using the ranking feedback. Applying RankDPO on SDXL and SD3-Medium models with our synthetically generated preference dataset \"Syn-Pic\" improves both prompt-following (on benchmarks like T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user studies). This pipeline presents a practical and scalable solution to develop better preference datasets to enhance the performance of text-to-image models.",
        "published": "2024-10-31 04:00:00",
        "id": "2244b664-e411-40f8-aa44-11cfc75afe85",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究提出一种可扩展的方法收集大规模全合成数据集用于DPO训练，用预训练奖励函数生成成对图像偏好，还引入RankDPO，在合成偏好数据集上应用RankDPO可提升文本到图像模型性能。"
        },
        "tokens": 884
    },
    {
        "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
        "link": "https://arxiv.org/abs/2410.17401",
        "description": "arXiv:2410.17401v2 Announce Type: replace \nAbstract: Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity. However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment. To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents. AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences. With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent. Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency. We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses. Our code and data are available at https://ai-secure.github.io/AdvWeb/ .",
        "published": "2024-10-31 04:00:00",
        "id": "ed4871d9-c9f7-40a5-b680-53d0792fbe69",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "AdvWeb是针对网络代理的新型黑盒攻击框架，通过训练对抗性提示模型生成并注入对抗性提示，误导网络代理执行目标对抗行动，在攻击基于GPT - 4V的SOTA VLM代理方面成功率高，暴露了现有基于LLM/VLM代理的关键漏洞。"
        },
        "tokens": 967
    },
    {
        "title": "Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality",
        "link": "https://arxiv.org/abs/2410.18784",
        "description": "arXiv:2410.18784v2 Announce Type: replace \nAbstract: The denoising diffusion probabilistic model (DDPM) has emerged as a mainstream generative model in generative AI. While sharp convergence guarantees have been established for the DDPM, the iteration complexity is, in general, proportional to the ambient data dimension, resulting in overly conservative theory that fails to explain its practical efficiency. This has motivated the recent work Li and Yan (2024a) to investigate how the DDPM can achieve sampling speed-ups through automatic exploitation of intrinsic low dimensionality of data. We strengthen this line of work by demonstrating, in some sense, optimal adaptivity to unknown low dimensionality. For a broad class of data distributions with intrinsic dimension $k$, we prove that the iteration complexity of the DDPM scales nearly linearly with $k$, which is optimal when using KL divergence to measure distributional discrepancy. Notably, our work is closely aligned with the independent concurrent work Potaptchik et al. (2024) -- posted two weeks prior to ours -- in establishing nearly linear-$k$ convergence guarantees for the DDPM.",
        "published": "2024-10-31 04:00:00",
        "id": "f77107f4-65af-4fc5-ba26-914fc5b79e06",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文证明对于内在维度为k的广泛数据分布，去噪扩散概率模型（DDPM）的迭代复杂度几乎与k成线性比例，展示其对未知低维度的最优适应性。"
        },
        "tokens": 819
    },
    {
        "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
        "link": "https://arxiv.org/abs/2410.19572",
        "description": "arXiv:2410.19572v3 Announce Type: replace \nAbstract: Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.",
        "published": "2024-10-31 04:00:00",
        "id": "38bc2639-ce7e-45df-b8c7-0eca77c05425",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出LLM驱动的块过滤框架ChunkRAG，通过块级评估和过滤检索信息增强RAG系统，减少幻觉并提高事实准确性，实验显示其优于现有RAG模型。"
        },
        "tokens": 785
    },
    {
        "title": "DiffGS: Functional Gaussian Splatting Diffusion",
        "link": "https://arxiv.org/abs/2410.19657",
        "description": "arXiv:2410.19657v2 Announce Type: replace \nAbstract: 3D Gaussian Splatting (3DGS) has shown convincing performance in rendering speed and fidelity, yet the generation of Gaussian Splatting remains a challenge due to its discreteness and unstructured nature. In this work, we propose DiffGS, a general Gaussian generator based on latent diffusion models. DiffGS is a powerful and efficient 3D generative model which is capable of generating Gaussian primitives at arbitrary numbers for high-fidelity rendering with rasterization. The key insight is to represent Gaussian Splatting in a disentangled manner via three novel functions to model Gaussian probabilities, colors and transforms. Through the novel disentanglement of 3DGS, we represent the discrete and unstructured 3DGS with continuous Gaussian Splatting functions, where we then train a latent diffusion model with the target of generating these Gaussian Splatting functions both unconditionally and conditionally. Meanwhile, we introduce a discretization algorithm to extract Gaussians at arbitrary numbers from the generated functions via octree-guided sampling and optimization. We explore DiffGS for various tasks, including unconditional generation, conditional generation from text, image, and partial 3DGS, as well as Point-to-Gaussian generation. We believe that DiffGS provides a new direction for flexibly modeling and generating Gaussian Splatting.",
        "published": "2024-10-31 04:00:00",
        "id": "4a202419-03a1-4144-8a2c-b0ccd01edb25",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "DiffGS是一种基于潜在扩散模型的通用高斯生成器，能生成用于高保真渲染的高斯图元，通过新的解耦表示3D高斯溅射，训练潜在扩散模型，并介绍了离散化算法来提取高斯，可用于多种任务。"
        },
        "tokens": 882
    },
    {
        "title": "On a Geometric Interpretation Of the Subset Sum Problem",
        "link": "https://arxiv.org/abs/2410.19024",
        "description": "arXiv:2410.19024v2 Announce Type: replace \nAbstract: For $S \\in \\mathbb{N}^n$ and $T \\in \\mathbb{N}$, the Subset Sum Problem (SSP) $\\exists^? x \\in \\{0,1\\}^n $ such that $S^T\\cdot x = T$ can be interpreted as the problem of deciding whether the intersection of the positive unit hypercube $Q_n = [0,1]^n$ with the hyperplane $S^T\\cdot \\left(x - \\frac{S}{\\|S\\|^2 }\\cdot T \\right) = 0$ contains at least a vertex. In this paper, we give an algorithm of complexity $\\mathcal{O}\\left( \\frac{1}{\\epsilon}\\cdot n^b \\right)$, for some absolute constant $b$, which either proves that there are no vertices in a slab of thickness $\\epsilon$ either finds a vertex in the slab of thickness $4\\cdot \\epsilon$. It is shown that any vertex $P$ in a slab of thickness $\\epsilon$ meets $\\left| \\frac{S^T\\cdot P}{T} - 1 \\right| \\leq \\epsilon$, therefore making the proposed algorithm a FPTAS for the SSP. The results are then applied to the study of the so called Simultaneous Subset-Sum Problem (SSSP).",
        "published": "2024-10-31 04:00:00",
        "id": "36bd6a5d-667f-4680-a39f-b0269433dda5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "论文给出一种复杂度为 $\\mathcal{O}\\left( \\frac{1}{\\epsilon}\\cdot n^b \\right)$ 的算法，用于解决子集和问题（SSP），并可应用于同时子集和问题（SSSP）的研究。"
        },
        "tokens": 900
    },
    {
        "title": "Mirror Matrix on the Wall: coding and vector notation as tools for introspection",
        "link": "https://arxiv.org/abs/2410.19549",
        "description": "arXiv:2410.19549v2 Announce Type: replace \nAbstract: The vector notation adopted by GNU Octave plays a significant role as a tool for introspection, aligning itself with the vision of Kenneth E. Iverson. He believed that, just like mathematics, a programming language should be an effective thinking tool for representing and reasoning about problems we wish to address. This work aims to explore the use of vector notation in GNU Octave through the analysis of operators and functions, providing a closer alignment with mathematical notation and enhancing code efficiency. We will delve into fundamental concepts such as indexing, broadcasting, and function handles, and present case studies for a deeper understanding of these concepts. By adopting vector notation, GNU Octave becomes a powerful tool for mathematicians, scientists and engineers, enabling them to express and solve complex problems more effectively and intuitively.",
        "published": "2024-10-31 04:00:00",
        "id": "f7117139-777c-4fab-8416-b471abfbc8f8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "软件",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章探讨GNU Octave中的向量符号，分析操作符和函数以增强代码效率，使其成为数学家、科学家和工程师有效解决复杂问题的工具。"
        },
        "tokens": 754
    },
    {
        "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
        "link": "https://arxiv.org/abs/2410.18952",
        "description": "arXiv:2410.18952v2 Announce Type: replace \nAbstract: Increasing the size of large language models (LLMs) has been shown to lead to better performance. However, this comes at the cost of slower and more expensive inference. Early-exiting is a promising approach for improving the efficiency of LLM inference by enabling next token prediction at intermediate layers. Yet, the large vocabulary size in modern LLMs makes the confidence estimation required for exit decisions computationally expensive, diminishing the efficiency gains. To address this, we propose dynamically pruning the vocabulary at test time for each token. Specifically, the vocabulary is pruned at one of the initial layers, and the smaller vocabulary is then used throughout the rest of the forward pass. Our experiments demonstrate that such post-hoc dynamic vocabulary pruning improves the efficiency of confidence estimation in early-exit LLMs while maintaining competitive performance.",
        "published": "2024-10-31 04:00:00",
        "id": "dbe51877-923f-4239-a04e-c2c8935082f2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出在早期退出的大型语言模型（LLMs）中进行动态词汇修剪，以提高置信度估计效率并保持竞争力，改善LLM推理效率。"
        },
        "tokens": 752
    },
    {
        "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
        "link": "https://arxiv.org/abs/2410.18975",
        "description": "arXiv:2410.18975v2 Announce Type: replace \nAbstract: We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.",
        "published": "2024-10-31 04:00:00",
        "id": "9f357238-3ebc-4e5b-85fe-91933cf3361d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一款利用生成式模型的无限游戏Unbounded，阐述了开发它在LLM和视觉生成领域的技术创新，并通过定性和定量分析评估该系统。"
        },
        "tokens": 859
    },
    {
        "title": "Learning to Adopt Generative AI",
        "link": "https://arxiv.org/abs/2410.19806",
        "description": "arXiv:2410.19806v2 Announce Type: replace \nAbstract: Recent advancements in generative AI, exemplified by ChatGPT, have dramatically transformed how people access information. Despite its powerful capabilities, the benefits it provides may not be equally distributed among individuals - a phenomenon referred to as the digital divide. Building upon prior literature, we propose two forms of digital divide in the generative AI adoption process: (i) the learning divide, capturing individuals' heterogeneous abilities to update their perceived utility of ChatGPT; and (ii) the utility divide, representing differences in individuals' actual utility derived from per use of ChatGPT. To evaluate these two divides, we develop a Bayesian learning model that incorporates demographic heterogeneities in both the utility and signal functions. Leveraging a six-month clickstream dataset, we estimate the model and find significant learning and utility divides across various demographic attributes. Interestingly, lower-educated and non-white individuals derive higher utility gains from ChatGPT but learn about its utility at a slower rate. Furthermore, males, younger individuals, and those with an IT background not only derive higher utility per use from ChatGPT but also learn about its utility more rapidly. Besides, we document a phenomenon termed the belief trap, wherein users underestimate ChatGPT's utility, opt not to use the tool, and consequently lack new experiences to update their perceptions, leading to continued underutilization. Our simulation further demonstrates that the learning divide can significantly affect the probability of falling into the belief trap, another form of the digital divide in adoption outcomes (i.e., outcome divide); however, offering training programs can alleviate the belief trap and mitigate the divide.",
        "published": "2024-10-31 04:00:00",
        "id": "f6ff25fd-20ea-4bc6-95ac-219984dfc111",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "基于前人文献提出生成式AI采用过程中的两种数字鸿沟形式，开发贝叶斯学习模型并利用六个月点击流数据集进行估计，发现不同人口属性间存在学习和效用鸿沟，还记录了信念陷阱现象，模拟表明学习鸿沟影响陷入信念陷阱的概率，培训项目可缓解该陷阱和鸿沟。"
        },
        "tokens": 936
    },
    {
        "title": "Residual Random Neural Networks",
        "link": "https://arxiv.org/abs/2410.19987",
        "description": "arXiv:2410.19987v2 Announce Type: replace \nAbstract: The single-layer feedforward neural network with random weights is a recurring motif in the neural networks literature. The advantage of these networks is their simplified training, which reduces to solving a ridge-regression problem. However, a general assumption is that these networks require a large number of hidden neurons relative to the dimensionality of the data samples, in order to achieve good classification accuracy. Contrary to this assumption, here we show that one can obtain good classification results even if the number of hidden neurons has the same order of magnitude as the dimensionality of the data samples, if this dimensionality is reasonably high. We also develop an efficient iterative residual training method for such random neural networks, which significantly improves their classification accuracy. Moreover, we also describe an encryption (obfuscation) method which can be used to protect both the data and the neural network model.",
        "published": "2024-10-31 04:00:00",
        "id": "8d0d0343-7920-421d-94e2-d26dbc897d3d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究表明在数据样本维度合理高时，单隐层随机权重前馈神经网络中隐藏神经元数量与数据样本维度同量级也能获得良好分类结果，还开发了迭代残差训练方法提高分类准确率，并描述了一种保护数据和神经网络模型的加密方法。"
        },
        "tokens": 784
    },
    {
        "title": "Gender Bias in LLM-generated Interview Responses",
        "link": "https://arxiv.org/abs/2410.20739",
        "description": "arXiv:2410.20739v2 Announce Type: replace \nAbstract: LLMs have emerged as a promising tool for assisting individuals in diverse text-generation tasks, including job-related texts. However, LLM-generated answers have been increasingly found to exhibit gender bias. This study evaluates three LLMs (GPT-3.5, GPT-4, Claude) to conduct a multifaceted audit of LLM-generated interview responses across models, question types, and jobs, and their alignment with two gender stereotypes. Our findings reveal that gender bias is consistent, and closely aligned with gender stereotypes and the dominance of jobs. Overall, this study contributes to the systematic examination of gender bias in LLM-generated interview responses, highlighting the need for a mindful approach to mitigate such biases in related applications.",
        "published": "2024-10-31 04:00:00",
        "id": "55f4d0bb-19d3-410e-8e62-814ce2449fd9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究评估了GPT-3.5、GPT-4、Claude三个大型语言模型（LLM）生成的面试回复中的性别偏见，发现其与性别刻板印象和工作主导性紧密相关，强调在相关应用中需注意减轻此类偏见。"
        },
        "tokens": 754
    },
    {
        "title": "Enhancing CNN Classification with Lamarckian Memetic Algorithms and Local Search",
        "link": "https://arxiv.org/abs/2410.20234",
        "description": "arXiv:2410.20234v2 Announce Type: replace \nAbstract: Optimization is critical for optimal performance in deep neural networks (DNNs). Traditional gradient-based methods often face challenges like local minima entrapment. This paper explores population-based metaheuristic optimization algorithms for image classification networks. We propose a novel approach integrating a two-stage training technique with population-based optimization algorithms incorporating local search capabilities. Our experiments demonstrate that the proposed method outperforms state-of-the-art gradient-based techniques, such as ADAM, in accuracy and computational efficiency, particularly with high computational complexity and numerous trainable parameters. The results suggest that our approach offers a robust alternative to traditional methods for weight optimization in convolutional neural networks (CNNs). Future work will explore integrating adaptive mechanisms for parameter tuning and applying the proposed method to other types of neural networks and real-time applications.",
        "published": "2024-10-31 04:00:00",
        "id": "22e669af-a8c8-496a-a5d2-3ff1c7089009",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文提出将两阶段训练技术与含局部搜索能力的种群优化算法相结合的新方法用于图像分类网络，实验表明该方法在准确性和计算效率上优于ADAM等传统梯度技术，可作为卷积神经网络权重优化的替代方法。"
        },
        "tokens": 770
    },
    {
        "title": "YourSkatingCoach: A Figure Skating Video Benchmark for Fine-Grained Element Analysis",
        "link": "https://arxiv.org/abs/2410.20427",
        "description": "arXiv:2410.20427v2 Announce Type: replace \nAbstract: Combining sports and machine learning involves leveraging ML algorithms and techniques to extract insight from sports-related data such as player statistics, game footage, and other relevant information. However, datasets related to figure skating in the literature focus primarily on element classification and are currently unavailable or exhibit only limited access, which greatly raise the entry barrier to developing visual sports technology for it. Moreover, when using such data to help athletes improve their skills, we find they are very coarse-grained: they work for learning what an element is, but they are poorly suited to learning whether the element is good or bad. Here we propose air time detection, a novel motion analysis task, the goal of which is to accurately detect the duration of the air time of a jump. We present YourSkatingCoach, a large, novel figure skating dataset which contains 454 videos of jump elements, the detected skater skeletons in each video, along with the gold labels of the start and ending frames of each jump, together as a video benchmark for figure skating. In addition, although this type of task is often viewed as classification, we cast it as a sequential labeling problem and propose a Transformer-based model to calculate the duration. Experimental results show that the proposed model yields a favorable results for a strong baseline. To further verify the generalizability of the fine-grained labels, we apply the same process to other sports as cross-sports tasks but for coarse-grained task action classification. Here we fine-tune the classification to demonstrate that figure skating, as it contains the essential body movements, constitutes a strong foundation for adaptation to other sports.",
        "published": "2024-10-31 04:00:00",
        "id": "b75e17ef-1673-4ef2-8101-0e0b9b8cf126",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出空中时间检测任务，介绍包含454个花样滑冰跳跃元素视频的YourSkatingCoach数据集，将任务视为序列标记问题并用基于Transformer的模型计算时长，还将该过程应用于其他运动以验证细粒度标签的通用性。"
        },
        "tokens": 944
    },
    {
        "title": "Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?",
        "link": "https://arxiv.org/abs/2410.20533",
        "description": "arXiv:2410.20533v2 Announce Type: replace \nAbstract: How can \"weak teacher models\" such as average human annotators or existing AI systems, effectively supervise LLMs to improve performance on hard reasoning tasks, especially those that challenge and requires expertise or daily practice from the teacher models? In this paper, we seek for empirical answers to this question by investigating various data-driven strategies that offer supervision data at different quality levels upon tasks of varying complexity. Two intuitive strategies emerge for teacher models to provide supervision during alignment training: 1) using lower-quality supervision from complete tasks that match the difficulty of the target reasoning tasks, and 2) leveraging higher-quality supervision from easier subtasks that are less challenging. Interestingly, we find that even when the outcome error rate for hard task supervision is high (e.g., 90\\%), training on such data can outperform perfectly correct supervision on easier subtasks on multiple hard math benchmarks. We further identify a more critical factor influencing training performance: step-wise error rates, which indicate the severity of errors in solutions. Specifically, training on hard task supervision with the same outcome error rates but disparate step-wise error rates can lead to a 30\\% accuracy gap on MATH benchmark. Our results also reveal that supplementing hard task supervision with the corresponding subtask supervision can yield notable performance improvements than simply combining rephrased hard full task supervision, suggesting new avenues for data augmentation. Data and code are released at \\url{https://github.com/hexuan21/Weak-to-Strong}.",
        "published": "2024-10-31 04:00:00",
        "id": "24314e83-e96a-469b-abf2-c9dc1623cece",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究通过不同质量监督数据策略探究弱教师模型如何有效监督大型语言模型提高硬推理任务性能，发现硬任务监督虽结果错误率高但可能优于简单子任务的完美监督，步长错误率是影响训练性能的关键因素，补充硬任务监督与子任务监督可提升性能。"
        },
        "tokens": 929
    },
    {
        "title": "Generative linguistics contribution to artificial intelligence: Where this contribution lies?",
        "link": "https://arxiv.org/abs/2410.20221",
        "description": "arXiv:2410.20221v2 Announce Type: replace \nAbstract: This article aims to characterize Generative linguistics (GL) contribution to artificial intelligence (AI), alluding to the debate among linguists and AI scientists on whether linguistics belongs to humanities or science. In this article, I will try not to be biased as a linguist, studying the phenomenon from an independent scientific perspective. The article walks the researcher/reader through the scientific theorems and rationales involved in AI which belong from GL, specifically the Chomsky School. It, thus, provides good evidence from syntax, semantics, language faculty, Universal Grammar, computational system of human language, language acquisition, human brain, programming languages (e.g. Python), Large Language Models, and unbiased AI scientists that this contribution is huge, and that this contribution cannot be denied. It concludes that however the huge GL contribution to AI, there are still points of divergence including the nature and type of language input.",
        "published": "2024-10-31 04:00:00",
        "id": "7f461a25-3549-46e6-9217-137f47e1b843",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章从独立科学视角探讨生成语言学对人工智能的巨大贡献，包括从语法、语义等多方面提供证据，但也指出二者仍存在分歧点。"
        },
        "tokens": 780
    },
    {
        "title": "History-Matching of Imbibition Flow in Multiscale Fractured Porous Media Using Physics-Informed Neural Networks (PINNs)",
        "link": "https://arxiv.org/abs/2410.20801",
        "description": "arXiv:2410.20801v2 Announce Type: replace \nAbstract: We propose a workflow based on physics-informed neural networks (PINNs) to model multiphase fluid flow in fractured porous media. After validating the workflow in forward and inverse modeling of a synthetic problem of flow in fractured porous media, we applied it to a real experimental dataset in which brine is injected at a constant pressure drop into a CO2 saturated naturally fractured shale core plug. The exact spatial positions of natural fractures and the dynamic in-situ distribution of fluids were imaged using a CT-scan setup. To model the targeted system, we followed a domain decomposition approach for matrix and fractures and a multi-network architecture for the separate calculation of water saturation and pressure. The flow equations in the matrix, fractures and interplay between them were solved during training. Prior to fully-coupled simulations, we proposed pre-training the model. This aided in a more efficient and successful training of the coupled system. Both for the synthetic and experimental inverse problems, we determined flow parameters within the matrix and the fractures. Multiple random initializations of network and system parameters were performed to assess the uncertainty and uniqueness of the results. The results confirmed the precision of the inverse calculated parameters in retrieving the main flow characteristics of the system. The consideration of multiscale matrix-fracture impacts is commonly overlooked in existing workflows. Accounting for them led to several orders of magnitude variations in the calculated flow properties compared to not accounting for them. To the best of our knowledge, the proposed PINNs-based workflow is the first to offer a reliable and computationally efficient solution for inverse modeling of multiphase flow in fractured porous media, achieved through history-matching noisy and multi-fidelity experimental measurements.",
        "published": "2024-10-31 04:00:00",
        "id": "c8cf8a00-f44a-4d5b-a613-59b8897fea09",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "提出基于物理信息神经网络（PINNs）的工作流程来模拟裂隙多孔介质中的多相流体流动，验证后应用于真实实验数据集，采用特定方法建模，还提出预训练模型，通过多次随机初始化评估结果不确定性和唯一性，考虑多尺度基质 - 裂缝影响是创新点。"
        },
        "tokens": 971
    },
    {
        "title": "Transformer-Based Tooth Alignment Prediction With Occlusion And Collision Constraints",
        "link": "https://arxiv.org/abs/2410.20806",
        "description": "arXiv:2410.20806v2 Announce Type: replace \nAbstract: The planning of digital orthodontic treatment requires providing tooth alignment, which not only consumes a lot of time and labor to determine manually but also relays clinical experiences heavily. In this work, we proposed a lightweight tooth alignment neural network based on Swin-transformer. We first re-organized 3D point clouds based on virtual arch lines and converted them into order-sorted multi-channel textures, which improves the accuracy and efficiency simultaneously. We then designed two new occlusal loss functions that quantitatively evaluate the occlusal relationship between the upper and lower jaws. They are important clinical constraints, first introduced to the best of our knowledge, and lead to cutting-edge prediction accuracy. To train our network, we collected a large digital orthodontic dataset that has 591 clinical cases, including various complex clinical cases. This dataset will benefit the community after its release since there is no open dataset so far. Furthermore, we also proposed two new orthodontic dataset augmentation methods considering tooth spatial distribution and occlusion. We evaluated our method with this dataset and extensive experiments, including comparisons with STAT methods and ablation studies, and demonstrate the high prediction accuracy of our method.",
        "published": "2024-10-31 04:00:00",
        "id": "c3e117d5-d6b6-490e-93eb-97798ae8f919",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出基于Swin - transformer的轻量级牙齿矫正神经网络，设计新的咬合损失函数，收集大型正畸数据集并提出两种数据集增强方法以提高预测准确性。"
        },
        "tokens": 829
    },
    {
        "title": "BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference",
        "link": "https://arxiv.org/abs/2410.21262",
        "description": "arXiv:2410.21262v2 Announce Type: replace \nAbstract: Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70% and 40%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST.",
        "published": "2024-10-31 04:00:00",
        "id": "480d6b92-c226-4b72-8a25-49bd0e923560",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出BLAST矩阵，可用于深度学习模型线性层权重矩阵，在语言和视觉任务的中型和大型模型中有较好的压缩效果并提升性能或减少性能退化，代码已开源。"
        },
        "tokens": 842
    },
    {
        "title": "Thank You, Stingray: Multilingual Large Language Models Can Not (Yet) Disambiguate Cross-Lingual Word Sense",
        "link": "https://arxiv.org/abs/2410.21573",
        "description": "arXiv:2410.21573v2 Announce Type: replace \nAbstract: Multilingual large language models (LLMs) have gained prominence, but concerns arise regarding their reliability beyond English. This study addresses the gap in cross-lingual semantic evaluation by introducing a novel benchmark for cross-lingual sense disambiguation, StingrayBench. In this paper, we demonstrate using false friends -- words that are orthographically similar but have completely different meanings in two languages -- as a possible approach to pinpoint the limitation of cross-lingual sense disambiguation in LLMs. We collect false friends in four language pairs, namely Indonesian-Malay, Indonesian-Tagalog, Chinese-Japanese, and English-German; and challenge LLMs to distinguish the use of them in context. In our analysis of various models, we observe they tend to be biased toward higher-resource languages. We also propose new metrics for quantifying the cross-lingual sense bias and comprehension based on our benchmark. Our work contributes to developing more diverse and inclusive language modeling, promoting fairer access for the wider multilingual community.",
        "published": "2024-10-31 04:00:00",
        "id": "4792b8b3-122d-4a6e-8b25-b6800927c871",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究通过创建跨语言词义消歧基准StingrayBench，用假朋友词测试多语言大型语言模型（LLMs），发现模型倾向于高资源语言，提出量化跨语言语义偏差和理解的新指标，以促进更公平的多语言社区访问。"
        },
        "tokens": 842
    },
    {
        "title": "A Novel Score-CAM based Denoiser for Spectrographic Signature Extraction without Ground Truth",
        "link": "https://arxiv.org/abs/2410.21557",
        "description": "arXiv:2410.21557v2 Announce Type: replace \nAbstract: Sonar based audio classification techniques are a growing area of research in the field of underwater acoustics. Usually, underwater noise picked up by passive sonar transducers contains all types of signals that travel through the ocean and is transformed into spectrographic images. As a result, the corresponding spectrograms intended to display the temporal-frequency data of a certain object often include the tonal regions of abundant extraneous noise that can effectively interfere with a 'contact'. So, a majority of spectrographic samples extracted from underwater audio signals are rendered unusable due to their clutter and lack the required indistinguishability between different objects. With limited clean true data for supervised training, creating classification models for these audio signals is severely bottlenecked.\n  This paper derives several new techniques to combat this problem by developing a novel Score-CAM based denoiser to extract an object's signature from noisy spectrographic data without being given any ground truth data. In particular, this paper proposes a novel generative adversarial network architecture for learning and producing spectrographic training data in similar distributions to low-feature spectrogram inputs. In addition, this paper also a generalizable class activation mapping based denoiser for different distributions of acoustic data, even real-world data distributions. Utilizing these novel architectures and proposed denoising techniques, these experiments demonstrate state-of-the-art noise reduction accuracy and improved classification accuracy than current audio classification standards. As such, this approach has applications not only to audio data but for countless data distributions used all around the world for machine learning.",
        "published": "2024-10-31 04:00:00",
        "id": "299c2aa2-4946-450e-8ba9-2d42d95554c8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "本文提出一种基于Score - CAM的去噪器新技术，通过生成对抗网络架构学习和生成频谱训练数据，以及通用的基于类激活映射的去噪器，从无地面真值数据的噪声频谱数据中提取对象特征，展示了先进的降噪和分类准确率。"
        },
        "tokens": 938
    },
    {
        "title": "Intelligent Environmental Empathy (IEE): A new power and platform to fostering green obligation for climate peace and justice",
        "link": "https://arxiv.org/abs/2410.21536",
        "description": "arXiv:2410.21536v2 Announce Type: replace \nAbstract: In this paper, we propose Intelligent Environmental Empathy (IEE) as a new driver for climate peace and justice, as an emerging issue in the age of big data. We first show that the authoritarian top-down intergovernmental cooperation, through international organizations (e.g., UNEP) for climate justice, could not overcome environmental issues and crevices so far. We elaborate on four grounds of climate injustice (i.e., teleological origin, axiological origin, formation cause, and social epistemic cause), and explain how the lack of empathy and environmental motivation on a global scale causes the failure of all the authoritarian top-down intergovernmental cooperation. Addressing all these issues requires a new button-up approach to climate peace and justice. Secondly, focusing on the intersection of AI, environmental empathy, and climate justice, we propose a model of Intelligent Environmental Empathy (IEE) for climate peace and justice at the operational level. IEE is empowered by the new power of environmental empathy (as a driver of green obligation for climate justice) and putative decentralized platform of AI (as an operative system against free riders), which Initially, impact citizens and some middle-class decision makers, such as city planners and local administrators, but will eventually affect global decision-makers as well.",
        "published": "2024-10-31 04:00:00",
        "id": "13de6e8f-af39-44ea-84aa-cda8c38f1d8f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出智能环境共情（IEE）作为气候和平与正义的新驱动力，阐述气候不公正的原因，指出自上而下的政府间合作无法克服环境问题，IEE由环境共情的新力量和人工智能去中心化平台驱动。"
        },
        "tokens": 873
    },
    {
        "title": "Super-resolution in disordered media using neural networks",
        "link": "https://arxiv.org/abs/2410.21556",
        "description": "arXiv:2410.21556v2 Announce Type: replace \nAbstract: We propose a methodology that exploits large and diverse data sets to accurately estimate the ambient medium's Green's functions in strongly scattering media. Given these estimates, obtained with and without the use of neural networks, excellent imaging results are achieved, with a resolution that is better than that of a homogeneous medium. This phenomenon, also known as super-resolution, occurs because the ambient scattering medium effectively enhances the physical imaging aperture.",
        "published": "2024-10-31 04:00:00",
        "id": "86b75da3-b785-44ba-b1a1-ea7427bb0260",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "提出一种利用大数据集准确估计强散射介质中环境介质格林函数的方法，利用这些估计（含神经网络与否）可实现超分辨率成像。"
        },
        "tokens": 669
    },
    {
        "title": "Bayesian Collaborative Bandits with Thompson Sampling for Improved Outreach in Maternal Health Program",
        "link": "https://arxiv.org/abs/2410.21405",
        "description": "arXiv:2410.21405v2 Announce Type: replace \nAbstract: Mobile health (mHealth) programs face a critical challenge in optimizing the timing of automated health information calls to beneficiaries. This challenge has been formulated as a collaborative multi-armed bandit problem, requiring online learning of a low-rank reward matrix. Existing solutions often rely on heuristic combinations of offline matrix completion and exploration strategies. In this work, we propose a principled Bayesian approach using Thompson Sampling for this collaborative bandit problem. Our method leverages prior information through efficient Gibbs sampling for posterior inference over the low-rank matrix factors, enabling faster convergence. We demonstrate significant improvements over state-of-the-art baselines on a real-world dataset from the world's largest maternal mHealth program. Our approach achieves a $16\\%$ reduction in the number of calls compared to existing methods and a $47$\\% reduction compared to the deployed random policy. This efficiency gain translates to a potential increase in program capacity by $0.5-1.4$ million beneficiaries, granting them access to vital ante-natal and post-natal care information. Furthermore, we observe a $7\\%$ and $29\\%$ improvement in beneficiary retention (an extremely hard metric to impact) compared to state-of-the-art and deployed baselines, respectively. Synthetic simulations further demonstrate the superiority of our approach, particularly in low-data regimes and in effectively utilizing prior information. We also provide a theoretical analysis of our algorithm in a special setting using Eluder dimension.",
        "published": "2024-10-31 04:00:00",
        "id": "096682c4-af5d-4921-ad9c-dbf380984ca9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对移动健康(mHealth)项目中优化向受益人发送自动健康信息呼叫的时间这一挑战，本文提出一种基于汤普森采样的贝叶斯方法，在真实数据集和合成模拟中展现出优势，减少呼叫次数并提高受益人留存率。"
        },
        "tokens": 914
    },
    {
        "title": "Carbon-Aware Computing for Data Centers with Probabilistic Performance Guarantees",
        "link": "https://arxiv.org/abs/2410.21510",
        "description": "arXiv:2410.21510v2 Announce Type: replace \nAbstract: Data centers are significant contributors to carbon emissions and can strain power systems due to their high electricity consumption. To mitigate this impact and to participate in demand response programs, cloud computing companies strive to balance and optimize operations across their global fleets by making strategic decisions about when and where to place compute jobs for execution. In this paper, we introduce a load shaping scheme which reacts to time-varying grid signals by leveraging both temporal and spatial flexibility of compute jobs to provide risk-aware management guidelines and job placement with provable performance guarantees based on distributionally robust optimization. Our approach divides the problem into two key components: (i) day-ahead planning, which generates an optimal scheduling strategy based on historical load data, and (ii) real-time job placement and (time) scheduling, which dynamically tracks the optimal strategy generated in (i). We validate our method in simulation using normalized load profiles from randomly selected Google clusters, incorporating time-varying grid signals. We can demonstrate significant reductions in carbon cost and peak power with our approach compared to myopic greedy policies, while maintaining computational efficiency and abiding to system constraints.",
        "published": "2024-10-31 04:00:00",
        "id": "4c05c5ff-749b-4112-844f-789a16c5625d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": true,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "论文提出一种负载整形方案应对时变电网信号，通过将问题分解为日前规划和实时作业放置调度两部分，在使用谷歌集群负载概况模拟验证中，可在满足约束条件下有效减少碳成本和峰值功率。"
        },
        "tokens": 841
    },
    {
        "title": "On filter design in deep convolutional neural network",
        "link": "https://arxiv.org/abs/2410.21644",
        "description": "arXiv:2410.21644v2 Announce Type: replace \nAbstract: The deep convolutional neural network (DCNN) in computer vision has given promising results. It is widely applied in many areas, from medicine, agriculture, self-driving car, biometric system, and almost all computer vision-based applications. Filters or weights are the critical elements responsible for learning in DCNN. Backpropagation has been the primary learning algorithm for DCNN and provides promising results, but the size and numbers of the filters remain hyper-parameters. Various studies have been done in the last decade on semi-supervised, self-supervised, and unsupervised methods and their properties. The effects of filter initialization, size-shape selection, and the number of filters on learning and optimization have not been investigated in a separate publication to collate all the options. Such attributes are often treated as hyper-parameters and lack mathematical understanding. Computer vision algorithms have many limitations in real-life applications, and understanding the learning process is essential to have some significant improvement. To the best of our knowledge, no separate investigation has been published discussing the filters; this is our primary motivation. This study focuses on arguments for choosing specific physical parameters of filters, initialization, and learning technic over scattered methods. The promising unsupervised approaches have been evaluated. Additionally, the limitations, current challenges, and future scope have been discussed in this paper.",
        "published": "2024-10-31 04:00:00",
        "id": "94be69a3-1ccc-47a9-b5fd-6ea834b58c44",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章聚焦于深度卷积神经网络（DCNN）中滤波器的物理参数选择、初始化和学习技术，评估了无监督方法，还讨论了其局限性、挑战和未来展望。"
        },
        "tokens": 857
    },
    {
        "title": "Impact of Code Transformation on Detection of Smart Contract Vulnerabilities",
        "link": "https://arxiv.org/abs/2410.21685",
        "description": "arXiv:2410.21685v2 Announce Type: replace \nAbstract: While smart contracts are foundational elements of blockchain applications, their inherent susceptibility to security vulnerabilities poses a significant challenge. Existing training datasets employed for vulnerability detection tools may be limited, potentially compromising their efficacy. This paper presents a method for improving the quantity and quality of smart contract vulnerability datasets and evaluates current detection methods. The approach centers around semantic-preserving code transformation, a technique that modifies the source code structure without altering its semantic meaning. The transformed code snippets are inserted into all potential locations within benign smart contract code, creating new vulnerable contract versions. This method aims to generate a wider variety of vulnerable codes, including those that can bypass detection by current analysis tools. The paper experiments evaluate the method's effectiveness using tools like Slither, Mythril, and CrossFuzz, focusing on metrics like the number of generated vulnerable samples and the false negative rate in detecting these vulnerabilities. The improved results show that many newly created vulnerabilities can bypass tools and the false reporting rate goes up to 100% and increases dataset size minimum by 2.5X.",
        "published": "2024-10-31 04:00:00",
        "id": "b8f42f13-a236-43e9-b16c-4f8443f02ba0",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "互联网",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出通过语义保留代码转换改进智能合约漏洞数据集的数量和质量并评估检测方法，经实验发现新创建的漏洞可绕过工具且误报率达100%、数据集大小至少增加2.5倍。"
        },
        "tokens": 822
    },
    {
        "title": "Enhancing Safety and Robustness of Vision-Based Controllers via Reachability Analysis",
        "link": "https://arxiv.org/abs/2410.21736",
        "description": "arXiv:2410.21736v2 Announce Type: replace \nAbstract: Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade into catastrophic system failures and compromise system safety. In this work, we compute Neural Reachable Tubes, which act as parameterized approximations of Backward Reachable Tubes to stress-test the vision-based controllers and mine their failure modes. The identified failures are then used to enhance the system safety through both offline and online methods. The online approach involves training a classifier as a run-time failure monitor to detect closed-loop, system-level failures, subsequently triggering a fallback controller that robustly handles these detected failures to preserve system safety. For the offline approach, we improve the original controller via incremental training using a carefully augmented failure dataset, resulting in a more robust controller that is resistant to the known failure modes. In either approach, the system is safeguarded against shortcomings that transcend the vision-based controller and pertain to the closed-loop safety of the overall system. We validate the proposed approaches on an autonomous aircraft taxiing task that involves using a vision-based controller to guide the aircraft towards the centerline of the runway. Our results show the efficacy of the proposed algorithms in identifying and handling system-level failures, outperforming methods that rely on controller prediction error or uncertainty quantification for identifying system failures.",
        "published": "2024-10-31 04:00:00",
        "id": "bcb4f3bc-c040-42ce-92df-4fe5ff3b9dc6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对视觉输入和机器学习决策控制的自主系统存在因输入异常而导致的安全风险，文章通过计算神经可达管挖掘故障模式，采用离线和在线方法提高系统安全性，并在飞机滑行任务中验证了有效性。"
        },
        "tokens": 905
    },
    {
        "title": "IntLoRA: Integral Low-rank Adaptation of Quantized Diffusion Models",
        "link": "https://arxiv.org/abs/2410.21759",
        "description": "arXiv:2410.21759v2 Announce Type: replace \nAbstract: Fine-tuning large-scale text-to-image diffusion models for various downstream tasks has yielded impressive results. However, the heavy computational burdens of tuning large models prevent personal customization. Recent advances have attempted to employ parameter-efficient fine-tuning (PEFT) techniques to adapt the floating-point (FP) or quantized pre-trained weights. Nonetheless, the adaptation parameters in existing works are still restricted to FP arithmetic, hindering hardware-friendly acceleration. In this work, we propose IntLoRA, to further push the efficiency limits by using integer type (INT) low-rank parameters to adapt the quantized diffusion models. By working in the integer arithmetic, our IntLoRA offers three key advantages: (i) for fine-tuning, the pre-trained weights are quantized, reducing memory usage; (ii) for storage, both pre-trained and low-rank weights are in INT which consumes less disk space; (iii) for inference, IntLoRA weights can be naturally merged into quantized pre-trained weights through efficient integer multiplication or bit-shifting, eliminating additional post-training quantization. Extensive experiments demonstrate that IntLoRA can achieve performance on par with or even superior to the vanilla LoRA, accompanied by significant efficiency improvements. Code is available at \\url{https://github.com/csguoh/IntLoRA}.",
        "published": "2024-10-31 04:00:00",
        "id": "9a7b7c8d-0d63-49f5-98c0-f3cfa313c21a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出IntLoRA，使用整数类型低秩参数调整量化扩散模型，具有减少内存使用、节省磁盘空间、无需额外后训练量化等优点，实验证明其性能可与原始LoRA相当甚至更优。"
        },
        "tokens": 879
    },
    {
        "title": "Text-Guided Attention is All You Need for Zero-Shot Robustness in Vision-Language Models",
        "link": "https://arxiv.org/abs/2410.21802",
        "description": "arXiv:2410.21802v2 Announce Type: replace \nAbstract: Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR). This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model's robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. Our code is available at https://github.com/zhyblue424/TGA-ZSR.",
        "published": "2024-10-31 04:00:00",
        "id": "7edcc777-e597-4bef-a1ec-89d5110cd41a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对预训练视觉 - 语言模型（如CLIP）易受对抗样本影响的问题，提出Text - Guided Attention for Zero - Shot Robustness (TGA - ZSR)框架，包含注意力细化和基于注意力的模型约束两个模块，经实验验证该方法在16个数据集上使零射击鲁棒准确率比现有技术提高9.58%，代码已开源。"
        },
        "tokens": 917
    },
    {
        "title": "Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective",
        "link": "https://arxiv.org/abs/2410.22217",
        "description": "arXiv:2410.22217v2 Announce Type: replace \nAbstract: Autoregression in large language models (LLMs) has shown impressive scalability by unifying all language tasks into the next token prediction paradigm. Recently, there is a growing interest in extending this success to vision foundation models. In this survey, we review the recent advances and discuss future directions for autoregressive vision foundation models. First, we present the trend for next generation of vision foundation models, i.e., unifying both understanding and generation in vision tasks. We then analyze the limitations of existing vision foundation models, and present a formal definition of autoregression with its advantages. Later, we categorize autoregressive vision foundation models from their vision tokenizers and autoregression backbones. Finally, we discuss several promising research challenges and directions. To the best of our knowledge, this is the first survey to comprehensively summarize autoregressive vision foundation models under the trend of unifying understanding and generation. A collection of related resources is available at https://github.com/EmmaSRH/ARVFM.",
        "published": "2024-10-31 04:00:00",
        "id": "6bf19c0f-5624-443a-b5b6-0c73d08bd77c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "该文是一篇综述，回顾了自动回归视觉基础模型的最新进展并讨论未来方向，指出将理解和生成统一到视觉任务中的趋势，分析现有模型局限性并给出自动回归的定义及其优势，对自动回归视觉基础模型进行分类并讨论研究挑战和方向。"
        },
        "tokens": 832
    },
    {
        "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online Feedback",
        "link": "https://arxiv.org/abs/2410.22159",
        "description": "arXiv:2410.22159v2 Announce Type: replace \nAbstract: The advent of large language models (LLMs), such as GPT-4, has enabled significant advancements in generating code across various domains. However, these models face unique challenges when generating IEC 61131-3 Structured Text (ST) code due to limited data in public training datasets and the complexity of ST language syntax. This paper proposes a novel approach to training LLMs that emphasizes improving the quality of learning data through an online process involving compiler feedback and evaluation from a secondary LLM. In this framework, the primary LLM generates new training samples, which are subsequently evaluated by a compiler for syntactical correctness and by a specialized LLM that excels at assessing semantic accuracy, though it is not optimized for code generation itself. Through iterative refinement of the training data, this approach results in marked improvements for the trained LLM, leading to higher compilation success rates and better semantic precision. As a result, the framework proves highly suitable for industrial automation applications and outperforms state-of-the-art models.",
        "published": "2024-10-31 04:00:00",
        "id": "b6cdb4e3-6789-4d37-8324-994fde9a5c55",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种训练大型语言模型（LLMs）生成IEC 61131-3结构化文本（ST）代码的新方法，通过编译器反馈和二级LLM评估的在线过程改进学习数据质量，提高编译成功率和语义精度，适用于工业自动化应用且性能优于现有模型。"
        },
        "tokens": 843
    },
    {
        "title": "Feature distribution Adaptation Network for Speech Emotion Recognition",
        "link": "https://arxiv.org/abs/2410.22023",
        "description": "arXiv:2410.22023v2 Announce Type: replace \nAbstract: In this paper, we propose a novel deep inductive transfer learning framework, named feature distribution adaptation network, to tackle the challenging multi-modal speech emotion recognition problem. Our method aims to use deep transfer learning strategies to align visual and audio feature distributions to obtain consistent representation of emotion, thereby improving the performance of speech emotion recognition. In our model, the pre-trained ResNet-34 is utilized for feature extraction for facial expression images and acoustic Mel spectrograms, respectively. Then, the cross-attention mechanism is introduced to model the intrinsic similarity relationships of multi-modal features. Finally, the multi-modal feature distribution adaptation is performed efficiently with feed-forward network, which is extended using the local maximum mean discrepancy loss. Experiments are carried out on two benchmark datasets, and the results demonstrate that our model can achieve excellent performance compared with existing ones.",
        "published": "2024-10-31 04:00:00",
        "id": "7df119a5-87f9-458c-981a-3f9e2a83386d",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为特征分布自适应网络的深度归纳迁移学习框架用于解决多模态语音情感识别问题，利用预训练的ResNet - 34进行特征提取，引入交叉注意力机制并使用前馈网络进行多模态特征分布自适应，在两个基准数据集上实验效果良好。"
        },
        "tokens": 787
    },
    {
        "title": "PC-Gym: Benchmark Environments For Process Control Problems",
        "link": "https://arxiv.org/abs/2410.22093",
        "description": "arXiv:2410.22093v2 Announce Type: replace \nAbstract: PC-Gym is an open-source tool designed to facilitate the development and evaluation of reinforcement learning (RL) algorithms for chemical process control problems. It provides a suite of environments that model a range of chemical processes, incorporating nonlinear dynamics, process disturbances, and constraints. Key features include flexible constraint handling mechanisms, customizable disturbance generation, and modular reward function design. The framework enables benchmarking state-of-the-art RL algorithms against a nonlinear Model Predictive Control (NMPC) oracle across various process control scenarios. Case studies demonstrate PC-Gym's effectiveness in evaluating RL approaches for the control of various chemical engineering systems such as a continuously stirred tank reactor, multistage extraction process, and crystallization reactor. The framework's ability to incorporate realistic disturbances and constraints allows for robust testing of control strategies. Results highlight the performance gaps between RL algorithms and NMPC oracles, demonstrating the utility of PC-Gym for algorithm benchmarking and suggesting areas for improvement in RL-based process control. By offering a standardized platform for developing and assessing RL-based control strategies, PC-Gym aims to accelerate research at the intersection of machine learning and process systems engineering. It bridges the gap between theoretical advancements in RL and practical applications in industrial process control, providing researchers and practitioners with a valuable tool for exploring data-driven control solutions for complex chemical processes.",
        "published": "2024-10-31 04:00:00",
        "id": "7ffca7b1-99fd-42ed-a692-0e674cd7a847",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "PC - Gym是一个开源工具，用于促进化工过程控制问题的强化学习算法开发和评估，其具有多种特性，通过案例研究证明其有效性，可用于算法基准测试并加速机器学习与过程系统工程交叉领域的研究。"
        },
        "tokens": 871
    },
    {
        "title": "Large Language Models Based JSON Parser Fuzzing for Bug Discovery and Behavioral Analysis",
        "link": "https://arxiv.org/abs/2410.21806",
        "description": "arXiv:2410.21806v2 Announce Type: replace \nAbstract: Fuzzing has been incredibly successful in uncovering bugs and vulnerabilities across diverse software systems. JSON parsers play a vital role in modern software development, and ensuring their reliability is of great importance. This research project focuses on leveraging Large Language Models (LLMs) to enhance JSON parser testing. The primary objectives are to generate test cases and mutants using LLMs for the discovery of potential bugs in open-source JSON parsers and the identification of behavioral diversities among them. We aim to uncover underlying bugs, plus discovering (and overcoming) behavioral diversities.",
        "published": "2024-10-31 04:00:00",
        "id": "a8bb86a1-0525-4c75-898b-2514fd6f8d4f",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "利用大型语言模型增强JSON解析器测试，包括生成测试用例和突变体以发现开源JSON解析器中的潜在错误并识别其行为差异。"
        },
        "tokens": 709
    },
    {
        "title": "A Fresh Look at Generalized Category Discovery through Non-negative Matrix Factorization",
        "link": "https://arxiv.org/abs/2410.21807",
        "description": "arXiv:2410.21807v2 Announce Type: replace \nAbstract: Generalized Category Discovery (GCD) aims to classify both base and novel images using labeled base data. However, current approaches inadequately address the intrinsic optimization of the co-occurrence matrix $\\bar{A}$ based on cosine similarity, failing to achieve zero base-novel regions and adequate sparsity in base and novel domains. To address these deficiencies, we propose a Non-Negative Generalized Category Discovery (NN-GCD) framework. It employs Symmetric Non-negative Matrix Factorization (SNMF) as a mathematical medium to prove the equivalence of optimal K-means with optimal SNMF, and the equivalence of SNMF solver with non-negative contrastive learning (NCL) optimization. Utilizing these theoretical equivalences, it reframes the optimization of $\\bar{A}$ and K-means clustering as an NCL optimization problem. Moreover, to satisfy the non-negative constraints and make a GCD model converge to a near-optimal region, we propose a GELU activation function and an NMF NCE loss. To transition $\\bar{A}$ from a suboptimal state to the desired $\\bar{A}^*$, we introduce a hybrid sparse regularization approach to impose sparsity constraints. Experimental results show NN-GCD outperforms state-of-the-art methods on GCD benchmarks, achieving an average accuracy of 66.1\\% on the Semantic Shift Benchmark, surpassing prior counterparts by 4.7\\%.",
        "published": "2024-10-31 04:00:00",
        "id": "6cedeecb-0165-468a-9f42-b207cd8c9d87",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "为解决广义类别发现(GCD)中存在的问题，提出非负广义类别发现(NN - GCD)框架，利用对称非负矩阵分解等理论将优化与聚类转化为非负对比学习优化问题，还提出相关激活函数、损失函数和稀疏正则化方法，实验结果显示NN - GCD在GCD基准测试中优于现有方法。"
        },
        "tokens": 934
    },
    {
        "title": "GPT-4o reads the mind in the eyes",
        "link": "https://arxiv.org/abs/2410.22309",
        "description": "arXiv:2410.22309v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are capable of reproducing human-like inferences, including inferences about emotions and mental states, from text. Whether this capability extends beyond text to other modalities remains unclear. Humans possess a sophisticated ability to read the mind in the eyes of other people. Here we tested whether this ability is also present in GPT-4o, a multimodal LLM. Using two versions of a widely used theory of mind test, the Reading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes Test, we found that GPT-4o outperformed humans in interpreting mental states from upright faces but underperformed humans when faces were inverted. While humans in our sample showed no difference between White and Non-white faces, GPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's errors were not random but revealed a highly consistent, yet incorrect, processing of mental-state information across trials, with an orientation-dependent error structure that qualitatively differed from that of humans for inverted faces but not for upright faces. These findings highlight how advanced mental state inference abilities and human-like face processing signatures, such as inversion effects, coexist in GPT-4o alongside substantial differences in information processing compared to humans.",
        "published": "2024-10-31 04:00:00",
        "id": "dd55479a-2d16-4e50-9e2b-59b12a0e5ec2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 4
            },
            "keyFacts": "研究测试GPT - 4o是否像人类一样能从眼睛解读心理状态，结果显示GPT - 4o解读正脸时表现优于人类，解读侧脸时不如人类，且处理白人与非白人面孔的准确率有差异，错误呈现特定结构且信息处理与人类有很大不同。"
        },
        "tokens": 881
    },
    {
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets",
        "link": "https://arxiv.org/abs/2410.22325",
        "description": "arXiv:2410.22325v2 Announce Type: replace \nAbstract: The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the \"manipulation centricity\" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project website: https://robots-pretrain-robots.github.io/.",
        "published": "2024-10-31 04:00:00",
        "id": "5ae0d053-4e8f-445c-a2f2-16d3d60baefa",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究发现机器人操作中心性对下游任务成功率有重要意义，提出操纵中心表示（MCR）框架，利用DROID机器人数据集预训练视觉编码器，引入新的对比损失和类似行为克隆的演员损失等，实验表明MCR在模拟和真实任务中表现优于基线方法。"
        },
        "tokens": 958
    },
    {
        "title": "Robustifying automatic speech recognition by extracting slowly varying features",
        "link": "https://arxiv.org/abs/2112.07400",
        "description": "arXiv:2112.07400v2 Announce Type: replace-cross \nAbstract: In the past few years, it has been shown that deep learning systems are highly vulnerable under attacks with adversarial examples. Neural-network-based automatic speech recognition (ASR) systems are no exception. Targeted and untargeted attacks can modify an audio input signal in such a way that humans still recognise the same words, while ASR systems are steered to predict a different transcription. In this paper, we propose a defense mechanism against targeted adversarial attacks consisting in removing fast-changing features from the audio signals, either by applying slow feature analysis, a low-pass filter, or both, before feeding the input to the ASR system. We perform an empirical analysis of hybrid ASR models trained on data pre-processed in such a way. While the resulting models perform quite well on benign data, they are significantly more robust against targeted adversarial attacks: Our final, proposed model shows a performance on clean data similar to the baseline model, while being more than four times more robust.",
        "published": "2024-10-31 04:00:00",
        "id": "622bf45c-ae33-4011-b37f-d08e845e85e9",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文提出通过提取缓慢变化特征来抵御有针对性的对抗攻击，对经过预处理的数据训练的混合ASR模型进行实证分析，结果模型在良性数据上表现良好且更抗攻击。"
        },
        "tokens": 796
    },
    {
        "title": "Limits of structures and Total NP Search Problems",
        "link": "https://arxiv.org/abs/2301.13603",
        "description": "arXiv:2301.13603v3 Announce Type: replace-cross \nAbstract: For an infinite class of finite graphs of unbounded size, we define a limit object, to be called a $\\textit{wide limit}$, relative to some computationally restricted class of functions. The limit object is a first order Boolean-valued structure. The first order properties of the wide limit then reflect how a computationally restricted viewer \"sees\" a generic member of the class. The construction uses arithmetic forcing with random variables [Kraj\\'i\\v{c}ek, Forcing with random variables and proof complexity 2011]. We give sufficient conditions for universal and existential sentences to be valid in the limit, provide several examples, and prove that such a limit object can then be expanded to a model of weak arithmetic.\n  To illustrate the concept we give an example in which the wide limit relates to total NP search problems. In particular, we take the wide limit of all maps from $\\{0,\\dots,k-1\\}$ to $\\{0,\\dots,\\lfloor k/2\\rfloor-1\\}$ to obtain a model of $\\forall \\text{PV}_1(f)$ where the problem $\\textbf{RetractionWeakPigeon}$ is total but $\\textbf{WeakPigeon}$, the complete problem for $\\textbf{PWPP}$, is not. Thus, we obtain a new proof of this unprovability and show it implies that $\\textbf{WeakPigeon}$ is not many-one reducible to $\\textbf{RetractionWeakPigeon}$ in the oracle setting.",
        "published": "2024-10-31 04:00:00",
        "id": "19a68dba-2b59-4c08-ae1b-3c2d8e188795",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "定义了相对于计算受限函数类的有限图无限类的极限对象（宽极限），给出其通用和存在句子在极限中有效的充分条件、示例并证明可扩展为弱算术模型，还用宽极限与NP完全搜索问题的关系示例说明概念。"
        },
        "tokens": 937
    },
    {
        "title": "Online Control with Adversarial Disturbance for Continuous-time Linear Systems",
        "link": "https://arxiv.org/abs/2306.01952",
        "description": "arXiv:2306.01952v4 Announce Type: replace-cross \nAbstract: We study online control for continuous-time linear systems with finite sampling rates, where the objective is to design an online procedure that learns under non-stochastic noise and performs comparably to a fixed optimal linear controller. We present a novel two-level online algorithm, by integrating a higher-level learning strategy and a lower-level feedback control strategy. This method offers a practical and robust solution for online control, which achieves sublinear regret. Our work provides the first nonasymptotic results for controlling continuous-time linear systems with finite number of interactions with the system. Moreover, we examine how to train an agent in domain randomization environments from a non-stochastic control perspective. By applying our method to the SAC (Soft Actor-Critic) algorithm, we achieved improved results in multiple reinforcement learning tasks within domain randomization environments. Our work provides new insights into non-asymptotic analyses of controlling continuous-time systems. Furthermore, our work brings practical intuition into controller learning under non-stochastic environments.",
        "published": "2024-10-31 04:00:00",
        "id": "14169c56-b1a7-4df9-9932-318599c51fc7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究有限采样率下连续时间线性系统的在线控制，提出一种新的两级在线算法，提供非渐近结果，还从非随机控制角度研究在域随机化环境中训练代理，将该方法应用于SAC算法在多个强化学习任务中取得改进结果。"
        },
        "tokens": 819
    },
    {
        "title": "Fair Wasserstein Coresets",
        "link": "https://arxiv.org/abs/2311.05436",
        "description": "arXiv:2311.05436v4 Announce Type: replace-cross \nAbstract: Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that FWC: (i) achieves a competitive fairness-utility tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).",
        "published": "2024-10-31 04:00:00",
        "id": "242d770e-75f8-4101-a7d5-c4d9aeeac770",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出公平Wasserstein核心集（FWC）这种新的核心集方法，可生成公平的合成代表性样本和样本级权重用于下游学习任务，其在下游模型中能实现有竞争力的公平 - 效用权衡、改善下游公平性并可用于减少大型语言模型预测中的偏差。"
        },
        "tokens": 887
    },
    {
        "title": "PAC-Bayes-Chernoff bounds for unbounded losses",
        "link": "https://arxiv.org/abs/2401.01148",
        "description": "arXiv:2401.01148v4 Announce Type: replace-cross \nAbstract: We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cram\\'er-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cram\\'er transform of the loss. Our approach naturally leverages properties of Cram\\'er-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new \\textit{model-dependent} assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques.",
        "published": "2024-10-31 04:00:00",
        "id": "023f8392-9298-4ff0-bae2-0eb61cb61041",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "文章引入新的PAC - Bayes oracle界用于无界损失，扩展了Cramér - Chernoff界到PAC - Bayesian设置，介绍了证明技术、主要定理的应用、在新假设下的通用界等内容。"
        },
        "tokens": 810
    },
    {
        "title": "Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions",
        "link": "https://arxiv.org/abs/2402.00077",
        "description": "arXiv:2402.00077v2 Announce Type: replace-cross \nAbstract: Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by the American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging such multi-institutional sequencing data presents significant challenges. Variations in gene panels result in loss of information when the analysis is conducted on common gene sets. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It uses a quantile-matched latent variable approach to derive integrated features to preserve information beyond common genes and maximize the utilization of all available data while leveraging information sharing to enhance both learning efficiency and the model's capacity to generalize. By extracting harmonized and noise-reduced lower-dimensional latent variables, the true mutation pattern unique to each individual is captured. We assess the model's performance and parameter estimation through extensive simulation studies. The extracted latent features from the Bridge model consistently excel in predicting patient survival across six cancer types in GENIE BPC data.",
        "published": "2024-10-31 04:00:00",
        "id": "451a7fbe-02b3-4508-9316-29d9d178f535",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": true,
                "entityRel": true,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "针对多机构基因组数据整合面临的挑战，提出Bridge模型，通过模拟研究评估其性能，在GENIE BPC数据六种癌症类型中表现良好。"
        },
        "tokens": 880
    },
    {
        "title": "Parameter uncertainties for imperfect surrogate models in the low-noise regime",
        "link": "https://arxiv.org/abs/2402.01810",
        "description": "arXiv:2402.01810v4 Announce Type: replace-cross \nAbstract: Bayesian regression determines model parameters by minimizing the expected loss, an upper bound to the true generalization error. However, the loss ignores misspecification, where models are imperfect. Parameter uncertainties from Bayesian regression are thus significantly underestimated and vanish in the large data limit. This is particularly problematic when building models of low-noise, or near-deterministic, calculations, as the main source of uncertainty is neglected. We analyze the generalization error of misspecified, near-deterministic surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and design an ansatz that respects this constraint, which for linear models incurs minimal overhead. This is demonstrated on model problems before application to thousand dimensional datasets in atomistic machine learning. Our efficient misspecification-aware scheme gives accurate prediction and bounding of test errors where existing schemes fail, allowing this important source of uncertainty to be incorporated in computational workflows.",
        "published": "2024-10-31 04:00:00",
        "id": "6b3bc4a7-fa04-4ea5-9b02-9bd39870d24c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "分析错定、近确定性代理模型的泛化误差，设计符合约束的方案，在原子机器学习的高维数据集上展示该方案在预测和界定测试误差方面的优势。"
        },
        "tokens": 795
    },
    {
        "title": "Beyond Strong labels: Weakly-supervised Learning Based on Gaussian Pseudo Labels for The Segmentation of Ellipse-like Vascular Structures in Non-contrast CTs",
        "link": "https://arxiv.org/abs/2402.03492",
        "description": "arXiv:2402.03492v3 Announce Type: replace-cross \nAbstract: Deep-learning-based automated segmentation of vascular structures in preoperative CT scans contributes to computer-assisted diagnosis and intervention procedure in vascular diseases. While CT angiography (CTA) is the common standard, non-contrast CT imaging is significant as a contrast-risk-free alternative, avoiding complications associated with contrast agents. However, the challenges of labor-intensive labeling and high labeling variability due to the ambiguity of vascular boundaries hinder conventional strong-label-based, fully-supervised learning in non-contrast CTs. This paper introduces a weakly-supervised framework using ellipses' topology in slices, including 1) an efficient annotation process based on predefined standards, 2) ellipse-fitting processing, 3) the generation of 2D Gaussian heatmaps serving as pseudo labels, 4) a training process through a combination of voxel reconstruction loss and distribution loss with the pseudo labels. We assess the effectiveness of the proposed method on one local and two public datasets comprising non-contrast CT scans, particularly focusing on the abdominal aorta. On the local dataset, our weakly-supervised learning approach based on pseudo labels outperforms strong-label-based fully-supervised learning (1.54\\% of Dice score on average), reducing labeling time by around 82.0\\%. The efficiency in generating pseudo labels allows the inclusion of label-agnostic external data in the training set, leading to an additional improvement in performance (2.74\\% of Dice score on average) with a reduction of 66.3\\% labeling time, where the labeling time remains considerably less than that of strong labels. On the public dataset, the pseudo labels achieve an overall improvement of 1.95\\% in Dice score for 2D models while a reduction of 11.65 voxel spacing in Hausdorff distance for 3D model.",
        "published": "2024-10-31 04:00:00",
        "id": "0ac4995b-9668-4f0a-b094-cdd435046058",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍一种弱监督框架，通过切片中的椭圆拓扑结构处理非对比CT中椭圆状血管结构的分割，在多个数据集上验证了该方法在减少标注时间的同时提高了性能。"
        },
        "tokens": 996
    },
    {
        "title": "Entrywise error bounds for low-rank approximations of kernel matrices",
        "link": "https://arxiv.org/abs/2405.14494",
        "description": "arXiv:2405.14494v2 Announce Type: replace-cross \nAbstract: In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.",
        "published": "2024-10-31 04:00:00",
        "id": "f6840ebf-3a9a-4a55-857b-c9920db50e58",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "本文推导了使用截断特征分解（或奇异值分解）得到的核矩阵低秩近似的逐元误差界，创新点是针对小特征值的核矩阵特征向量的离域结果，最后通过实验验证了理论。"
        },
        "tokens": 736
    },
    {
        "title": "Full Event Particle-Level Unfolding with Variable-Length Latent Variational Diffusion",
        "link": "https://arxiv.org/abs/2404.14332",
        "description": "arXiv:2404.14332v2 Announce Type: replace-cross \nAbstract: The measurements performed by particle physics experiments must account for the imperfect response of the detectors used to observe the interactions. One approach, unfolding, statistically adjusts the experimental data for detector effects. Recently, generative machine learning models have shown promise for performing unbinned unfolding in a high number of dimensions. However, all current generative approaches are limited to unfolding a fixed set of observables, making them unable to perform full-event unfolding in the variable dimensional environment of collider data. A novel modification to the variational latent diffusion model (VLD) approach to generative unfolding is presented, which allows for unfolding of high- and variable-dimensional feature spaces. The performance of this method is evaluated in the context of semi-leptonic top quark pair production at the Large Hadron Collider.",
        "published": "2024-10-31 04:00:00",
        "id": "de407297-5dd6-4421-a8ff-34db5500de92",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种对变分潜在扩散模型（VLD）生成展开方法的新修改，可用于展开高维和可变维特征空间，并在大型强子对撞机的半轻子顶夸克对产生的背景下评估了该方法的性能，此研究属于粒子物理实验中的展开工作，使用了人工智能相关技术。"
        },
        "tokens": 791
    },
    {
        "title": "Real-time multichannel deep speech enhancement in hearing aids: Comparing monaural and binaural processing in complex acoustic scenarios",
        "link": "https://arxiv.org/abs/2405.01967",
        "description": "arXiv:2405.01967v3 Announce Type: replace-cross \nAbstract: Deep learning has the potential to enhance speech signals and increase their intelligibility for users of hearing aids. Deep models suited for real-world application should feature a low computational complexity and low processing delay of only a few milliseconds. In this paper, we explore deep speech enhancement that matches these requirements and contrast monaural and binaural processing algorithms in two complex acoustic scenes. Both algorithms are evaluated with objective metrics and in experiments with hearing-impaired listeners performing a speech-in-noise test. Results are compared to two traditional enhancement strategies, i.e., adaptive differential microphone processing and binaural beamforming. While in diffuse noise, all algorithms perform similarly, the binaural deep learning approach performs best in the presence of spatial interferers. Through a post-analysis, this can be attributed to improvements at low SNRs and to precise spatial filtering.",
        "published": "2024-10-31 04:00:00",
        "id": "a1002eb9-624d-4e94-aab8-a762537fd998",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探索符合低计算复杂度和低处理延迟要求的深度语音增强，对比单耳和双耳处理算法在复杂声学场景中的表现，并与传统增强策略比较，结果表明在空间干扰存在时双耳深度学习方法效果最佳。"
        },
        "tokens": 790
    },
    {
        "title": "Is Transductive Learning Equivalent to PAC Learning?",
        "link": "https://arxiv.org/abs/2405.05190",
        "description": "arXiv:2405.05190v2 Announce Type: replace-cross \nAbstract: Much of learning theory is concerned with the design and analysis of probably approximately correct (PAC) learners. The closely related transductive model of learning has recently seen more scrutiny, with its learners often used as precursors to PAC learners. Our goal in this work is to understand and quantify the exact relationship between these two models. First, we observe that modest extensions of existing results show the models to be essentially equivalent for realizable learning for most natural loss functions, up to low order terms in the error and sample complexity. The situation for agnostic learning appears less straightforward, with sample complexities potentially separated by a $\\frac{1}{\\epsilon}$ factor. This is therefore where our main contributions lie. Our results are two-fold:\n  1. For agnostic learning with bounded losses (including, for example, multiclass classification), we show that PAC learning reduces to transductive learning at the cost of low-order terms in the error and sample complexity via an adaptation of the reduction of arXiv:2304.09167 to the agnostic setting.\n  2. For agnostic binary classification, we show the converse: transductive learning is essentially no more difficult than PAC learning. Together with our first result this implies that the PAC and transductive models are essentially equivalent for agnostic binary classification. This is our most technical result, and involves two steps: A symmetrization argument on the agnostic one-inclusion graph (OIG) of arXiv:2309.13692 to derive the worst-case agnostic transductive instance, and expressing the error of the agnostic OIG algorithm for this instance in terms of the empirical Rademacher complexity of the class.\n  We leave as an intriguing open question whether our second result can be extended beyond binary classification to show the transductive and PAC models equivalent more broadly.",
        "published": "2024-10-31 04:00:00",
        "id": "b6a7f56e-8202-4d3e-aa3a-cb0f07796af6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "探讨转导学习与PAC学习的关系，在可实现学习方面二者对多数自然损失函数基本等价，不可知学习情况较复杂，在有界损失的不可知学习中PAC学习可归约为转导学习，在不可知二元分类中二者基本等价，后者是技术难点且其能否扩展到多元分类仍是未解决问题。"
        },
        "tokens": 1023
    },
    {
        "title": "Genetic-guided GFlowNets for Sample Efficient Molecular Optimization",
        "link": "https://arxiv.org/abs/2402.05961",
        "description": "arXiv:2402.05961v3 Announce Type: replace-cross \nAbstract: The challenge of discovering new molecules with desired properties is crucial in domains like drug discovery and material design. Recent advances in deep learning-based generative methods have shown promise but face the issue of sample efficiency due to the computational expense of evaluating the reward function. This paper proposes a novel algorithm for sample-efficient molecular optimization by distilling a powerful genetic algorithm into deep generative policy using GFlowNets training, the off-policy method for amortized inference. This approach enables the deep generative policy to learn from domain knowledge, which has been explicitly integrated into the genetic algorithm. Our method achieves state-of-the-art performance in the official molecular optimization benchmark, significantly outperforming previous methods. It also demonstrates effectiveness in designing inhibitors against SARS-CoV-2 with substantially fewer reward calls.",
        "published": "2024-10-31 04:00:00",
        "id": "53e2e2d6-c9d4-4768-bfba-2b3ab1cbb59a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种新算法，通过将遗传算法融入GFlowNets训练来提高分子优化的样本效率，在分子优化基准测试中达到领先性能且在设计新冠病毒抑制剂上也有效。"
        },
        "tokens": 757
    },
    {
        "title": "Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics",
        "link": "https://arxiv.org/abs/2405.14806",
        "description": "arXiv:2405.14806v3 Announce Type: replace-cross \nAbstract: Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.",
        "published": "2024-10-31 04:00:00",
        "id": "62f48ed4-be2f-4029-84a0-785992f1898a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种用于高能物理的多用途架构L - GATr，是基于几何代数的洛伦兹等变Transformer，在回归、分类任务上得到演示，还构建了首个洛伦兹等变生成模型，性能与特定领域强基线相当或更优。"
        },
        "tokens": 803
    },
    {
        "title": "Conformal Classification with Equalized Coverage for Adaptively Selected Groups",
        "link": "https://arxiv.org/abs/2405.15106",
        "description": "arXiv:2405.15106v2 Announce Type: replace-cross \nAbstract: This paper introduces a conformal inference method to evaluate uncertainty in classification by generating prediction sets with valid coverage conditional on adaptively chosen features. These features are carefully selected to reflect potential model limitations or biases. This can be useful to find a practical compromise between efficiency -- by providing informative predictions -- and algorithmic fairness -- by ensuring equalized coverage for the most sensitive groups. We demonstrate the validity and effectiveness of this method on simulated and real data sets.",
        "published": "2024-10-31 04:00:00",
        "id": "96d3c43b-28bb-4a2a-bd66-66f7849c662b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文介绍一种共形推理方法，通过生成具有有效覆盖范围的预测集来评估分类中的不确定性，其特征经精心选择以反映潜在模型局限或偏差，该方法在模拟和真实数据集上被证明有效。"
        },
        "tokens": 700
    },
    {
        "title": "Structured Learning of Compositional Sequential Interventions",
        "link": "https://arxiv.org/abs/2406.05745",
        "description": "arXiv:2406.05745v2 Announce Type: replace-cross \nAbstract: We consider sequential treatment regimes where each unit is exposed to combinations of interventions over time. When interventions are described by qualitative labels, such as \"close schools for a month due to a pandemic\" or \"promote this podcast to this user during this week\", it is unclear which appropriate structural assumptions allow us to generalize behavioral predictions to previously unseen combinations of interventions. Standard black-box approaches mapping sequences of categorical variables to outputs are applicable, but they rely on poorly understood assumptions on how reliable generalization can be obtained, and may underperform under sparse sequences, temporal variability, and large action spaces. To approach that, we pose an explicit model for composition, that is, how the effect of sequential interventions can be isolated into modules, clarifying which data conditions allow for the identification of their combined effect at different units and time steps. We show the identification properties of our compositional model, inspired by advances in causal matrix factorization methods. Our focus is on predictive models for novel compositions of interventions instead of matrix completion tasks and causal effect estimation. We compare our approach to flexible but generic black-box models to illustrate how structure aids prediction in sparse data conditions.",
        "published": "2024-10-31 04:00:00",
        "id": "36a38a7b-8aa8-4fa3-8c60-beae4c48feb2",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章考虑顺序治疗方案中的干预组合，提出组合模型并展示其识别特性，与通用黑箱模型对比以说明结构对稀疏数据预测的帮助。"
        },
        "tokens": 819
    },
    {
        "title": "Particle Semi-Implicit Variational Inference",
        "link": "https://arxiv.org/abs/2407.00649",
        "description": "arXiv:2407.00649v2 Announce Type: replace-cross \nAbstract: Semi-implicit variational inference (SIVI) enriches the expressiveness of variational families by utilizing a kernel and a mixing distribution to hierarchically define the variational distribution. Existing SIVI methods parameterize the mixing distribution using implicit distributions, leading to intractable variational densities. As a result, directly maximizing the evidence lower bound (ELBO) is not possible, so they resort to one of the following: optimizing bounds on the ELBO, employing costly inner-loop Markov chain Monte Carlo runs, or solving minimax objectives. In this paper, we propose a novel method for SIVI called Particle Variational Inference (PVI) which employs empirical measures to approximate the optimal mixing distributions characterized as the minimizer of a free energy functional. PVI arises naturally as a particle approximation of a Euclidean--Wasserstein gradient flow and, unlike prior works, it directly optimizes the ELBO whilst making no parametric assumption about the mixing distribution. Our empirical results demonstrate that PVI performs favourably compared to other SIVI methods across various tasks. Moreover, we provide a theoretical analysis of the behaviour of the gradient flow of a related free energy functional: establishing the existence and uniqueness of solutions as well as propagation of chaos results.",
        "published": "2024-10-31 04:00:00",
        "id": "0886f5ff-5129-4902-8cf6-97b090e210b7",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种名为粒子变分推理（PVI）的半隐式变分推理（SIVI）新方法，该方法用经验测度近似最优混合分布，可直接优化证据下限（ELBO），经验结果表明其在多种任务中表现优于其他SIVI方法，还提供了相关自由能泛函梯度流行为的理论分析。"
        },
        "tokens": 888
    },
    {
        "title": "WildDESED: An LLM-Powered Dataset for Wild Domestic Environment Sound Event Detection System",
        "link": "https://arxiv.org/abs/2407.03656",
        "description": "arXiv:2407.03656v3 Announce Type: replace-cross \nAbstract: This work aims to advance sound event detection (SED) research by presenting a new large language model (LLM)-powered dataset namely wild domestic environment sound event detection (WildDESED). It is crafted as an extension to the original DESED dataset to reflect diverse acoustic variability and complex noises in home settings. We leveraged LLMs to generate eight different domestic scenarios based on target sound categories of the DESED dataset. Then we enriched the scenarios with a carefully tailored mixture of noises selected from AudioSet and ensured no overlap with target sound. We consider widely popular convolutional neural recurrent network to study WildDESED dataset, which depicts its challenging nature. We then apply curriculum learning by gradually increasing noise complexity to enhance the model's generalization capabilities across various noise levels. Our results with this approach show improvements within the noisy environment, validating the effectiveness on the WildDESED dataset promoting noise-robust SED advancements.",
        "published": "2024-10-31 04:00:00",
        "id": "6fc9c86c-e580-429a-9cd3-86502d53d793",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "WildDESED数据集由大语言模型驱动，是DESED数据集的扩展，研究中利用卷积神经网络研究该数据集，并应用课程学习提升模型在不同噪声水平下的泛化能力以推动声音事件检测发展。"
        },
        "tokens": 798
    },
    {
        "title": "BUSClean: Open-source software for breast ultrasound image pre-processing and knowledge extraction for medical AI",
        "link": "https://arxiv.org/abs/2407.11316",
        "description": "arXiv:2407.11316v3 Announce Type: replace-cross \nAbstract: Development of artificial intelligence (AI) for medical imaging demands curation and cleaning of large-scale clinical datasets comprising hundreds of thousands of images. Some modalities, such as mammography, contain highly standardized imaging. In contrast, breast ultrasound imaging (BUS) can contain many irregularities not indicated by scan metadata, such as enhanced scan modes, sonographer annotations, or additional views. We present an open-source software solution for automatically processing clinical BUS datasets. The algorithm performs BUS scan filtering (flagging of invalid and non-B-mode scans), cleaning (dual-view scan detection, scan area cropping, and caliper detection), and knowledge extraction (BI-RADS Labeling and Measurement fields) from sonographer annotations. Its modular design enables users to adapt it to new settings. Experiments on an internal testing dataset of 430 clinical BUS images achieve >95% sensitivity and >98% specificity in detecting every type of text annotation, >98% sensitivity and specificity in detecting scans with blood flow highlighting, alternative scan modes, or invalid scans. A case study on a completely external, public dataset of BUS scans found that BUSClean identified text annotations and scans with blood flow highlighting with 88.6% and 90.9% sensitivity and 98.3% and 99.9% specificity, respectively. Adaptation of the lesion caliper detection method to account for a type of caliper specific to the case study demonstrates the intended use of BUSClean in new data distributions and improved performance in lesion caliper detection from 43.3% and 93.3% out-of-the-box to 92.1% and 92.3% sensitivity and specificity, respectively. Source code, example notebooks, and sample data are available at https://github.com/hawaii-ai/bus-cleaning.",
        "published": "2024-10-31 04:00:00",
        "id": "6bec2632-6615-42ea-885a-87bf6689b892",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "介绍了一款名为BUSClean的开源软件，用于自动处理临床乳腺超声（BUS）数据集，包括扫描过滤、清理和知识提取等功能，给出了在内部测试数据集和外部公开数据集上的实验结果，并提供了源代码、示例笔记本和样本数据的获取地址。"
        },
        "tokens": 1001
    },
    {
        "title": "Noise-augmented Chaotic Ising Machines for Combinatorial Optimization and Sampling",
        "link": "https://arxiv.org/abs/2408.04744",
        "description": "arXiv:2408.04744v2 Announce Type: replace-cross \nAbstract: Ising machines, hardware accelerators for combinatorial optimization and probabilistic sampling problems, have gained significant interest recently. A key element is stochasticity, which enables a wide exploration of configurations, thereby helping avoid local minima. Here, we refine the previously proposed concept of coupled chaotic bits (c-bits) that operate without explicit stochasticity. We show that augmenting chaotic bits with stochasticity enhances performance in combinatorial optimization, achieving algorithmic scaling comparable to probabilistic bits (p-bits). We first demonstrate that c-bits follow the quantum Boltzmann law in a 1D transverse field Ising model. We then show that c-bits exhibit critical dynamics similar to stochastic p-bits in 2D Ising and 3D spin glass models, with promising potential to solve challenging optimization problems. Finally, we propose a noise-augmented version of coupled c-bits via the adaptive parallel tempering algorithm (APT). Our noise-augmented c-bit algorithm outperforms fully deterministic c-bits running versions of the simulated annealing algorithm. Other analog Ising machines with coupled oscillators could draw inspiration from the proposed algorithm. Running replicas at constant temperature eliminates the need for global modulation of coupling strengths. Mixing stochasticity with deterministic c-bits creates a powerful hybrid computing scheme that can bring benefits in scaled, asynchronous, and massively parallel hardware implementations.",
        "published": "2024-10-31 04:00:00",
        "id": "58393bdb-eed3-4603-b066-dfe6276a20f4",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章提出通过自适应并行回火算法（APT）对耦合混沌比特（c - bits）进行加噪，其在组合优化中的性能优于完全确定性的c - bits运行版本的模拟退火算法，这种混合计算方案可应用于大规模、异步和大规模并行硬件实现。"
        },
        "tokens": 895
    },
    {
        "title": "Vertex-critical graphs in co-gem-free graphs",
        "link": "https://arxiv.org/abs/2408.05027",
        "description": "arXiv:2408.05027v2 Announce Type: replace-cross \nAbstract: A graph $G$ is $k$-vertex-critical if $\\chi(G)=k$ but $\\chi(G-v)<k$ for all $v\\in V(G)$ and $(G,H)$-free if it contains no induced subgraph isomorphic to $G$ or $H$. We show that there are only finitely many $k$-vertex-critical (co-gem, $H$)-free graphs for all $k$ when $H$ is any graph of order $4$ by showing finiteness in the three remaining open cases, those are the cases when $H$ is $2P_2$, $K_3+P_1$, and $K_4$. For the first two cases we actually prove the stronger results:\n  $\\bullet$ There are only finitely many $k$-vertex-critical (co-gem, paw$+P_1$)-free graphs for all $k$ and that only finitely many $k$-vertex-critical (co-gem, paw$+P_1$)-free graphs for all $k\\ge 1$.\n  $\\bullet$ There are only finitely many $k$-vertex-critical (co-gem, $P_5$, $P_3+cP_2$)-free graphs for all $k\\ge 1$ and $c\\ge 0$.\n  To prove the latter result, we employ a novel application of Sperner's Theorem on the number of antichains in a partially ordered set. Our result for $K_4$ uses exhaustive computer search and is proved by showing the stronger result that every $(\\text{co-gem, }K_4)$-free graph is $4$-colourable. Our results imply the existence of simple polynomial-time certifying algorithms to decide the $k$-colourability of (co-gem, $H$)-free graphs for all $k$ and all $H$ of order $4$ by searching the vertex-critical graphs as induced subgraphs.",
        "published": "2024-10-31 04:00:00",
        "id": "24a4a5d0-a476-4d43-9770-e0fa3e14d995",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "文章证明了对于所有的k，当H为任意4阶图时，k -顶点临界（共宝石，H） - 无图只有有限个，阐述了部分更强的结果，并指出这些结果暗示存在简单多项式时间证明算法来判定（共宝石，H） - 无图的k -可着色性。"
        },
        "tokens": 1061
    },
    {
        "title": "CT-AGRG: Automated Abnormality-Guided Report Generation from 3D Chest CT Volumes",
        "link": "https://arxiv.org/abs/2408.11965",
        "description": "arXiv:2408.11965v4 Announce Type: replace-cross \nAbstract: The rapid increase of computed tomography (CT) scans and their time-consuming manual analysis have created an urgent need for robust automated analysis techniques in clinical settings. These aim to assist radiologists and help them managing their growing workload. Existing methods typically generate entire reports directly from 3D CT images, without explicitly focusing on observed abnormalities. This unguided approach often results in repetitive content or incomplete reports, failing to prioritize anomaly-specific descriptions. We propose a new anomaly-guided report generation model, which first predicts abnormalities and then generates targeted descriptions for each. Evaluation on a public dataset demonstrates significant improvements in report quality and clinical relevance. We extend our work by conducting an ablation study to demonstrate its effectiveness.",
        "published": "2024-10-31 04:00:00",
        "id": "14207f3a-5136-4990-881f-b3653ba4bcf5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种异常引导报告生成模型，先预测异常再生成针对性描述，在公共数据集评估中提升报告质量和临床相关性，并通过消融研究证明有效性。"
        },
        "tokens": 743
    },
    {
        "title": "A nonlinear elasticity model in computer vision",
        "link": "https://arxiv.org/abs/2408.17237",
        "description": "arXiv:2408.17237v2 Announce Type: replace-cross \nAbstract: The purpose of this paper is to analyze a nonlinear elasticity model previously introduced by the authors for comparing two images, regarded as bounded open subsets of $\\R^n$ together with associated vector-valued intensity maps. Optimal transformations between the images are sought as minimisers of an integral functional among orientation-preserving homeomorphisms. The existence of minimisers is proved under natural coercivity and polyconvexity conditions, assuming only that the intensity functions are bounded measurable. Variants of the existence theorem are also proved, first under the constraint that finite sets of landmark points in the two images are mapped one to the other, and second when one image is to be compared to an unknown part of another.\n  The question is studied as to whether for images related by a linear mapping the unique minimizer is given by that linear mapping. For a natural class of functional integrands an example is given guaranteeing that this property holds for pairs of images in which the second is a scaling of the first by a constant factor. However for the property to hold for arbitrary pairs of linearly related images it is shown that the integrand has to depend on the gradient of the transformation as a convex function of its determinant alone. This suggests a new model in which the integrand depends also on second derivatives of the transformation, and an example is given for which both existence of minimizers is assured and the above property holds for all pairs of linearly related images.",
        "published": "2024-10-31 04:00:00",
        "id": "b7b76f19-cba3-46e2-82ff-c803558c2796",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "论文分析非线性弹性模型用于比较图像，证明了极小值存在性的相关定理，研究线性映射相关图像的唯一极小值问题并提出新模型。"
        },
        "tokens": 879
    },
    {
        "title": "Reassessing Noise Augmentation Methods in the Context of Adversarial Speech",
        "link": "https://arxiv.org/abs/2409.01813",
        "description": "arXiv:2409.01813v2 Announce Type: replace-cross \nAbstract: In this study, we investigate if noise-augmented training can concurrently improve adversarial robustness in automatic speech recognition (ASR) systems. We conduct a comparative analysis of the adversarial robustness of four different state-of-the-art ASR architectures, where each of the ASR architectures is trained under three different augmentation conditions: one subject to background noise, speed variations, and reverberations, another subject to speed variations only, and a third without any form of data augmentation. The results demonstrate that noise augmentation not only improves model performance on noisy speech but also the model's robustness to adversarial attacks.",
        "published": "2024-10-31 04:00:00",
        "id": "689c5e00-7c29-4fa9-9f50-6a784f69e771",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "研究调查噪声增强训练能否提高自动语音识别系统的对抗鲁棒性，对比分析四种ASR架构在三种不同增强条件下的对抗鲁棒性，结果表明噪声增强可提升模型在含噪语音上的性能和对抗攻击的鲁棒性。"
        },
        "tokens": 742
    },
    {
        "title": "WaveMixSR-V2: Enhancing Super-resolution with Higher Efficiency",
        "link": "https://arxiv.org/abs/2409.10582",
        "description": "arXiv:2409.10582v3 Announce Type: replace-cross \nAbstract: Recent advancements in single image super-resolution have been predominantly driven by token mixers and transformer architectures. WaveMixSR utilized the WaveMix architecture, employing a two-dimensional discrete wavelet transform for spatial token mixing, achieving superior performance in super-resolution tasks with remarkable resource efficiency. In this work, we present an enhanced version of the WaveMixSR architecture by (1) replacing the traditional transpose convolution layer with a pixel shuffle operation and (2) implementing a multistage design for higher resolution tasks ($4\\times$). Our experiments demonstrate that our enhanced model -- WaveMixSR-V2 -- outperforms other architectures in multiple super-resolution tasks, achieving state-of-the-art for the BSD100 dataset, while also consuming fewer resources, exhibits higher parameter efficiency, lower latency and higher throughput. Our code is available at https://github.com/pranavphoenix/WaveMixSR.",
        "published": "2024-10-31 04:00:00",
        "id": "9f2d23fc-a3a1-48e6-b6a3-26083d852641",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "WaveMixSR - V2为WaveMixSR架构的增强版，在超分辨率任务中有更好性能，资源消耗少且参数效率高、延迟低、吞吐量高，代码已开源。"
        },
        "tokens": 784
    },
    {
        "title": "GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks",
        "link": "https://arxiv.org/abs/2409.13832",
        "description": "arXiv:2409.13832v4 Announce Type: replace-cross \nAbstract: The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability. To tackle these problems, we present GTSinger, a large global, multi-technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks. Particularly, (1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset; (2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles; (3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control; (4) GTSinger offers realistic music scores, assisting real-world musical composition; (5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks. Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion. The corpus and demos can be found at http://gtsinger.github.io. We provide the dataset and the code for processing data and conducting benchmarks at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger.",
        "published": "2024-10-31 04:00:00",
        "id": "e18d1808-25f4-4ff4-8491-d77f757eb75c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "GTSinger是一个大型全球性、多技术、免费使用、带有真实乐谱的高质量歌唱语料库，为所有歌唱任务而设计，还进行了四个基准实验，相关数据和代码可在特定网址获取。"
        },
        "tokens": 942
    },
    {
        "title": "AMARO: All Heavy-Atom Transferable Neural Network Potentials of Protein Thermodynamics",
        "link": "https://arxiv.org/abs/2409.17852",
        "description": "arXiv:2409.17852v2 Announce Type: replace-cross \nAbstract: All-atom molecular simulations offer detailed insights into macromolecular phenomena, but their substantial computational cost hinders the exploration of complex biological processes. We introduce Advanced Machine-learning Atomic Representation Omni-force-field (AMARO), a new neural network potential (NNP) that combines an O(3)-equivariant message-passing neural network architecture, TensorNet, with a coarse-graining map that excludes hydrogen atoms. AMARO demonstrates the feasibility of training coarser NNP, without prior energy terms, to run stable protein dynamics with scalability and generalization capabilities.",
        "published": "2024-10-31 04:00:00",
        "id": "f129709e-2e01-4f1f-9912-8ad35b54d12a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 2
            },
            "keyFacts": "AMARO是一种新的神经网络势，它结合特定神经网络架构和粗粒化映射，展示了训练更粗的无先验能量项的NNP以运行稳定蛋白质动力学的可行性。"
        },
        "tokens": 727
    },
    {
        "title": "Response Estimation and System Identification of Dynamical Systems via Physics-Informed Neural Networks",
        "link": "https://arxiv.org/abs/2410.01340",
        "description": "arXiv:2410.01340v2 Announce Type: replace-cross \nAbstract: The accurate modelling of structural dynamics is crucial across numerous engineering applications, such as Structural Health Monitoring (SHM), seismic analysis, and vibration control. Often, these models originate from physics-based principles and can be derived from corresponding governing equations, often of differential equation form. However, complex system characteristics, such as nonlinearities and energy dissipation mechanisms, often imply that such models are approximative and often imprecise. This challenge is further compounded in SHM, where sensor data is often sparse, making it difficult to fully observe the system's states. To address these issues, this paper explores the use of Physics-Informed Neural Networks (PINNs), a class of physics-enhanced machine learning (PEML) techniques, for the identification and estimation of dynamical systems. PINNs offer a unique advantage by embedding known physical laws directly into the neural network's loss function, allowing for simple embedding of complex phenomena, even in the presence of uncertainties. This study specifically investigates three key applications of PINNs: state estimation in systems with sparse sensing, joint state-parameter estimation, when both system response and parameters are unknown, and parameter estimation within a Bayesian framework to quantify uncertainties. The results demonstrate that PINNs deliver an efficient tool across all aforementioned tasks, even in presence of modelling errors. However, these errors tend to have a more significant impact on parameter estimation, as the optimization process must reconcile discrepancies between the prescribed model and the true system behavior. Despite these challenges, PINNs show promise in dynamical system modeling, offering a robust approach to handling uncertainties.",
        "published": "2024-10-31 04:00:00",
        "id": "9045ad73-ae22-4554-ba89-24748136804b",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文探讨物理信息神经网络（PINNs）在动力系统识别和估计中的应用，包括稀疏传感系统的状态估计、联合状态 - 参数估计、贝叶斯框架内的参数估计以量化不确定性，结果表明PINNs是有效的工具，但建模误差对参数估计影响较大。"
        },
        "tokens": 936
    },
    {
        "title": "EG-SpikeFormer: Eye-Gaze Guided Transformer on Spiking Neural Networks for Medical Image Analysis",
        "link": "https://arxiv.org/abs/2410.09674",
        "description": "arXiv:2410.09674v2 Announce Type: replace-cross \nAbstract: Neuromorphic computing has emerged as a promising energy-efficient alternative to traditional artificial intelligence, predominantly utilizing spiking neural networks (SNNs) implemented on neuromorphic hardware. Significant advancements have been made in SNN-based convolutional neural networks (CNNs) and Transformer architectures. However, neuromorphic computing for the medical imaging domain remains underexplored. In this study, we introduce EG-SpikeFormer, an SNN architecture tailored for clinical tasks that incorporates eye-gaze data to guide the model's attention to the diagnostically relevant regions in medical images. Our developed approach effectively addresses shortcut learning issues commonly observed in conventional models, especially in scenarios with limited clinical data and high demands for model reliability, generalizability, and transparency. Our EG-SpikeFormer not only demonstrates superior energy efficiency and performance in medical image prediction tasks but also enhances clinical relevance through multi-modal information alignment. By incorporating eye-gaze data, the model improves interpretability and generalization, opening new directions for applying neuromorphic computing in healthcare.",
        "published": "2024-10-31 04:00:00",
        "id": "ed88a677-5dbf-4121-9688-3c6767610ba6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "EG - SpikeFormer是一种针对临床任务的SNN架构，它引入眼动数据引导模型关注医学图像中的诊断相关区域，解决传统模型的捷径学习问题，提高医学图像预测任务的能效和性能，增强临床相关性、可解释性和泛化能力。"
        },
        "tokens": 831
    },
    {
        "title": "Predicting Molecular Ground-State Conformation via Conformation Optimization",
        "link": "https://arxiv.org/abs/2410.09795",
        "description": "arXiv:2410.09795v2 Announce Type: replace-cross \nAbstract: Predicting ground-state conformation from the corresponding molecular graph is crucial for many chemical applications, such as molecular modeling, molecular docking, and molecular property prediction. Recently, many learning-based methods have been proposed to replace time-consuming simulations for this task. However, these methods are often inefficient and sub-optimal as they merely rely on molecular graph information to make predictions from scratch. In this work, considering that molecular low-quality conformations are readily available, we propose a novel framework called ConfOpt to predict molecular ground-state conformation from the perspective of conformation optimization. Specifically, ConfOpt takes the molecular graph and corresponding low-quality 3D conformation as inputs, and then derives the ground-state conformation by iteratively optimizing the low-quality conformation under the guidance of the molecular graph. During training, ConfOpt concurrently optimizes the predicted atomic 3D coordinates and the corresponding interatomic distances, resulting in a strong predictive model. Extensive experiments demonstrate that ConfOpt significantly outperforms existing methods, thus providing a new paradigm for efficiently and accurately predicting molecular ground-state conformation.",
        "published": "2024-10-31 04:00:00",
        "id": "191f1bb0-fb96-46aa-b1d9-e29ab3d7589c",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为ConfOpt的新框架从构象优化角度预测分子基态构象，该框架以分子图和低质量3D构象为输入，通过迭代优化低质量构象得出基态构象，实验表明其性能优于现有方法。"
        },
        "tokens": 823
    },
    {
        "title": "Bayesian Optimisation with Unknown Hyperparameters: Regret Bounds Logarithmically Closer to Optimal",
        "link": "https://arxiv.org/abs/2410.10384",
        "description": "arXiv:2410.10384v2 Announce Type: replace-cross \nAbstract: Bayesian Optimization (BO) is widely used for optimising black-box functions but requires us to specify the length scale hyperparameter, which defines the smoothness of the functions the optimizer will consider. Most current BO algorithms choose this hyperparameter by maximizing the marginal likelihood of the observed data, albeit risking misspecification if the objective function is less smooth in regions we have not yet explored. The only prior solution addressing this problem with theoretical guarantees was A-GP-UCB, proposed by Berkenkamp et al. (2019). This algorithm progressively decreases the length scale, expanding the class of functions considered by the optimizer. However, A-GP-UCB lacks a stopping mechanism, leading to over-exploration and slow convergence. To overcome this, we introduce Length scale Balancing (LB) - a novel approach, aggregating multiple base surrogate models with varying length scales. LB intermittently adds smaller length scale candidate values while retaining longer scales, balancing exploration and exploitation. We formally derive a cumulative regret bound of LB and compare it with the regret of an oracle BO algorithm using the optimal length scale. Denoting the factor by which the regret bound of A-GP-UCB was away from oracle as $g(T)$, we show that LB is only $\\log g(T)$ away from oracle regret. We also empirically evaluate our algorithm on synthetic and real-world benchmarks and show it outperforms A-GP-UCB, maximum likelihood estimation and MCMC.",
        "published": "2024-10-31 04:00:00",
        "id": "d91c61bb-fbb1-4509-bad4-bac1effa1bcf",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "文章介绍一种新的贝叶斯优化方法Length scale Balancing（LB），推导其累积遗憾界并与最优算法比较，经实证评估在合成和真实基准测试中优于其他算法。"
        },
        "tokens": 903
    },
    {
        "title": "Quantum Boltzmann machine learning of ground-state energies",
        "link": "https://arxiv.org/abs/2410.12935",
        "description": "arXiv:2410.12935v2 Announce Type: replace-cross \nAbstract: Estimating the ground-state energy of Hamiltonians is a fundamental task for which it is believed that quantum computers can be helpful. Several approaches have been proposed toward this goal, including algorithms based on quantum phase estimation and hybrid quantum-classical optimizers involving parameterized quantum circuits, the latter falling under the umbrella of the variational quantum eigensolver. Here, we analyze the performance of quantum Boltzmann machines for this task, which is a less explored ansatz based on parameterized thermal states and which is not known to suffer from the barren-plateau problem. We delineate a hybrid quantum-classical algorithm for this task and rigorously prove that it converges to an $\\varepsilon$-approximate stationary point of the energy function optimized over parameter space, while using a number of parameterized-thermal-state samples that is polynomial in $\\varepsilon^{-1}$, the number of parameters, and the norm of the Hamiltonian being optimized. Our algorithm estimates the gradient of the energy function efficiently by means of a novel quantum circuit construction that combines classical sampling, Hamiltonian simulation, and the Hadamard test, thus overcoming a key obstacle to quantum Boltzmann machine learning that has been left open since [Amin et al., Phys. Rev. X 8, 021050 (2018)]. Additionally supporting our main claims are calculations of the gradient and Hessian of the energy function, as well as an upper bound on the matrix elements of the latter that is used in the convergence analysis.",
        "published": "2024-10-31 04:00:00",
        "id": "08a37d15-a886-4802-8322-0be8b7f4abdc",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "分析量子玻尔兹曼机用于估计哈密顿量基态能量任务的性能，阐述相关混合量子 - 经典算法并证明其收敛性，还提及能量函数的梯度和黑塞计算等支持性内容。"
        },
        "tokens": 910
    },
    {
        "title": "Cryogenic Control and Readout Integrated Circuits for Solid-State Quantum Computing",
        "link": "https://arxiv.org/abs/2410.15895",
        "description": "arXiv:2410.15895v2 Announce Type: replace-cross \nAbstract: In the pursuit of quantum computing, solid-state quantum systems, particularly superconducting ones, have made remarkable advancements over the past two decades. However, achieving fault-tolerant quantum computing for next-generation applications necessitates the integration of several million qubits, which presents significant challenges in terms of interconnection complexity and latency that are currently unsolvable with state-of-the-art room-temperature control and readout electronics. Recently, cryogenic integrated circuits (ICs), including CMOS radio-frequency ICs and rapid-single-flux-quantum-logic ICs, have emerged as potential alternatives to room-temperature electronics. Unlike their room-temperature counterparts, these ICs are deployed within cryostats to enhance scalability by reducing the number and length of transmission lines. Additionally, operating at cryogenic temperatures can suppress electronic noise and improve qubit control fidelity. However, for CMOS ICs specifically, circuit design uncertainties arise due to a lack of reliable models for cryogenic field effect transistors as well as issues related to severe fickle noises and power dissipation at cryogenic temperatures. This paper provides a comprehensive review of recent research on both types of cryogenic control and readout ICs but primarily focuses on the more mature CMOS technology. The discussion encompasses principles underlying control and readout techniques employed in cryogenic CMOS ICs along with their architectural designs; characterization and modeling approaches for field effect transistors under cryogenic conditions; as well as fundamental concepts pertaining to rapid single flux quantum circuits.",
        "published": "2024-10-31 04:00:00",
        "id": "72be3399-9906-47f4-bcf8-3b2179c3a1d5",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "硬件设施",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "综述了低温集成电路（包括CMOS射频IC和快速单磁通量子逻辑IC）在固态量子计算中的控制和读出研究，重点关注CMOS技术，讨论了其技术原理、架构设计、晶体管表征建模以及快速单磁通量子电路概念等内容。"
        },
        "tokens": 907
    },
    {
        "title": "Spectra and pseudospectra in the evaluation of material stability",
        "link": "https://arxiv.org/abs/2410.20570",
        "description": "arXiv:2410.20570v2 Announce Type: replace-cross \nAbstract: We consider the dynamics of bodies with \"active\" microstructure described by vector valued phase fields. For waves with time-varying amplitude, the associated evolution equation involves a matrix that can be non-normal, depending on the constitutive choices adopted for the microstructural actions associated with the phase field. In the presence of non-normality, the spectral analysis of such a matrix when (unknown or uncertain or time-dependent) constitutive parameters vary is generically not decisive to evaluate the material stability. We thus need to look at the pseudospectrum, that is, the set of all possible eigenvalues of matrices in an {\\epsilon}-neighborhood of the matrix of interest. As a case study, we look at quasicrystals and evaluate through spectra and pseudospectra the influence of a microstructural self-action on the material stability. This approach allows us to reliably determine thresholds where stable-unstable transitions in the material behavior occur.",
        "published": "2024-10-31 04:00:00",
        "id": "ad192f7f-81da-4f0a-bf39-a07accd780a8",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "其他",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "研究考虑由向量值相场描述的具有“活性”微结构的物体动力学，在矩阵非正规情况下需通过谱和伪谱评估材料稳定性，以准晶体为例研究微结构自作用对材料稳定性的影响。"
        },
        "tokens": 798
    },
    {
        "title": "PACER: Physics Informed Uncertainty Aware Climate Emulator",
        "link": "https://arxiv.org/abs/2410.21657",
        "description": "arXiv:2410.21657v2 Announce Type: replace-cross \nAbstract: Climate models serve as critical tools for evaluating the effects of climate change and projecting future climate scenarios. However, the reliance on numerical simulations of physical equations renders them computationally intensive and inefficient. While deep learning methodologies have made significant progress in weather forecasting, they are still unstable for climate emulation tasks. Here, we propose PACER, a lightweight 684K parameter Physics Informed Uncertainty Aware Climate Emulator. PACER emulates temperature and precipitation stably for 86 years while only being trained on greenhouse gas emissions data. We incorporate a fundamental physical law of advection-diffusion in PACER accounting for boundary conditions and empirically estimating the diffusion co-efficient and flow velocities from emissions data. PACER has been trained on 15 climate models provided by ClimateSet outperforming baselines across most of the climate models and advancing a new state of the art in a climate diagnostic task.",
        "published": "2024-10-31 04:00:00",
        "id": "94c62046-c8ff-45f6-939d-58b04e03a32a",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": true,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出名为PACER的气候模拟器，仅用温室气体排放数据训练就能稳定模拟86年的温度和降水，结合平流 - 扩散物理定律，基于ClimateSet的15个气候模型训练且性能超越基线，在气候诊断任务上达到新高度。"
        },
        "tokens": 807
    },
    {
        "title": "Analyzing Noise Models and Advanced Filtering Algorithms for Image Enhancement",
        "link": "https://arxiv.org/abs/2410.21946",
        "description": "arXiv:2410.21946v2 Announce Type: replace-cross \nAbstract: Noise, an unwanted component in an image, can be the reason for the degradation of Image at the time of transmission or capturing. Noise reduction from images is still a challenging task. Digital Image Processing is a component of Digital signal processing. A wide variety of algorithms can be used in image processing to apply to an image or an input dataset and obtain important outcomes. In image processing research, removing noise from images before further analysis is essential. Post-noise removal of images improves clarity, enabling better interpretation and analysis across medical imaging, satellite imagery, and radar applications. While numerous algorithms exist, each comes with its own assumptions, strengths, and limitations. The paper aims to evaluate the effectiveness of different filtering techniques on images with eight types of noise. It evaluates methodologies like Wiener, Median, Gaussian, Mean, Low pass, High pass, Laplacian and bilateral filtering, using the performance metric Peak signal to noise ratio. It shows us the impact of different filters on noise models by applying a variety of filters to various kinds of noise. Additionally, it also assists us in determining which filtering strategy is most appropriate for a certain noise model based on the circumstances.",
        "published": "2024-10-31 04:00:00",
        "id": "2bd23fbd-a7ac-4006-a1f5-b837697f9e31",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "论文旨在评估不同滤波技术对含八种噪声图像的有效性，使用峰值信噪比评估多种滤波方法，展示不同滤波器对噪声模型的影响并确定合适的滤波策略。"
        },
        "tokens": 827
    },
    {
        "title": "Identifiability Analysis of Linear ODE Systems with Hidden Confounders",
        "link": "https://arxiv.org/abs/2410.21917",
        "description": "arXiv:2410.21917v2 Announce Type: replace-cross \nAbstract: The identifiability analysis of linear Ordinary Differential Equation (ODE) systems is a necessary prerequisite for making reliable causal inferences about these systems. While identifiability has been well studied in scenarios where the system is fully observable, the conditions for identifiability remain unexplored when latent variables interact with the system. This paper aims to address this gap by presenting a systematic analysis of identifiability in linear ODE systems incorporating hidden confounders. Specifically, we investigate two cases of such systems. In the first case, latent confounders exhibit no causal relationships, yet their evolution adheres to specific functional forms, such as polynomial functions of time $t$. Subsequently, we extend this analysis to encompass scenarios where hidden confounders exhibit causal dependencies, with the causal structure of latent variables described by a Directed Acyclic Graph (DAG). The second case represents a more intricate variation of the first case, prompting a more comprehensive identifiability analysis. Accordingly, we conduct detailed identifiability analyses of the second system under various observation conditions, including both continuous and discrete observations from single or multiple trajectories. To validate our theoretical results, we perform a series of simulations, which support and substantiate our findings.",
        "published": "2024-10-31 04:00:00",
        "id": "f860f76e-ddc2-4a6f-b92d-e6811724030e",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "数据服务",
                "titleRel": false,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 1
            },
            "keyFacts": "本文对包含隐藏混杂因素的线性常微分方程（ODE）系统进行可识别性分析，分两种情况探讨，并在不同观测条件下对第二种系统详细分析，通过模拟验证理论结果。"
        },
        "tokens": 850
    },
    {
        "title": "Adaptive Transfer Clustering: A Unified Framework",
        "link": "https://arxiv.org/abs/2410.21263",
        "description": "arXiv:2410.21263v2 Announce Type: replace-cross \nAbstract: We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method's effectiveness in various scenarios.",
        "published": "2024-10-31 04:00:00",
        "id": "3dc3f7aa-7342-44f0-8bab-f90e4168c3d6",
        "source": "arxiv",
        "section": "computerScience",
        "sentiment": {
            "relevance": {
                "industry": "人工智能",
                "titleRel": true,
                "entityRel": false,
                "prodRel": false,
                "techRel": true,
                "PolicyRel": false,
                "activityRel": false,
                "relRank": 3
            },
            "keyFacts": "提出一种通用的迁移学习聚类框架（自适应迁移聚类算法ATC），理论分析证明其在高斯混合模型下的最优性，模拟和实验证实其有效性。"
        },
        "tokens": 729
    }
]